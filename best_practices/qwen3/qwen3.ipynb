{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Qwen3\n",
    "\n",
    "## æ¨¡å‹æ¨ç†\n",
    "\n",
    "### ä½¿ç”¨transformers\n",
    "åœ¨ transformers ä¸­ä½¿ç”¨ Qwen3-8B ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# å·²å®‰è£…ï¼ŒVersion: 4.52.4\n",
    "!pip install transformers -U"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æŒ‡å®šGPU\n",
    "import os\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"5\"  # 0-basedç¼–å·ï¼ŒGPU7æ˜¯ç¬¬8å—å¡\n",
    "\n",
    "# å±è”½è­¦å‘Šä¿¡æ¯\n",
    "import warnings\n",
    "warnings.filterwarnings(\n",
    "    \"ignore\",\n",
    "    message=r\"You are using `torch\\.load` with `weights_only=False`.*\",\n",
    "    category=FutureWarning,\n",
    "    module=r\"torch\\.serialization\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä½¿ç”¨ä¸‹è½½å¥½çš„æ¨¡å‹ï¼š/data/joey.wang/llm/models/pretrained/llm/Qwen3-8B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.10/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:02<00:00,  1.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "Okay, the user wants a short introduction to large language models. Let me start by defining what they are. They're AI systems trained on vast amounts of text data, right? So I should mention their training data size and the purpose, like understanding and generating human-like text.\n",
      "\n",
      "Wait, maybe I should explain the key features. They can handle multiple languages, answer questions, write stories, etc. Oh, and they're called \"large\" because of their scaleâ€”parameters, data, and computational resources. That's important to highlight.\n",
      "\n",
      "I should also touch on their applications. Like, they're used in chatbots, customer service, content creation. Maybe mention some examples like GPT or BERT? But the user might not need specific names, just a general idea.\n",
      "\n",
      "Also, the user might be looking for a concise overview, so I need to keep it brief. Avoid jargon but still be accurate. Make sure to mention the training process, maybe something about neural networks. But keep it simple.\n",
      "\n",
      "Wait, should I include the fact that they're transformer-based? That's a key part of their architecture. But maybe that's too technical for a short intro. Hmm. Maybe just say they use advanced neural networks instead of going into details.\n",
      "\n",
      "Also, mention that they can adapt to different tasks without retraining. Transfer learning? That's a good point. But again, keep it simple. The user probably wants a high-level understanding.\n",
      "\n",
      "Let me structure it: start with definition, mention training on large data, capabilities, applications, and maybe a note on their impact. Keep each part brief. Let me check the length. The user said \"short,\" so maybe around 100-150 words.\n",
      "\n",
      "Avoid being too technical. Use examples like answering questions, writing stories, coding. Maybe mention that they're used in various industries. Alright, that should cover it. Let me put it all together now.\n",
      "</think>\n",
      "content: Large language models (LLMs) are advanced AI systems trained on vast amounts of text data to understand and generate human-like language. They leverage complex neural networks and massive computational resources to process and learn patterns from diverse sources, enabling them to perform tasks like answering questions, writing stories, coding, and translating languages. Their \"large\" designation refers to their scaleâ€”enormous datasets, billions of parameters, and sophisticated architectures that allow them to adapt to a wide range of tasks without explicit retraining. LLMs are revolutionizing fields like customer service, content creation, and research by providing efficient, context-aware solutions. However, they also raise ethical considerations, such as bias and data privacy, highlighting the need for responsible development and use.\n"
     ]
    }
   ],
   "source": [
    "from modelscope import AutoModelForCausalLM, AutoTokenizer\n",
    "\n",
    "model_name = \"/data/joey.wang/llm/models/pretrained/llm/Qwen3-8B\"\n",
    "# load the tokenizer and the model\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_name,\n",
    "    torch_dtype=\"auto\",\n",
    "    device_map=\"auto\"\n",
    ")\n",
    "\n",
    "# prepare the model input\n",
    "prompt = \"Give me a short introduction to large language model.\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True  # Switch between thinking and non-thinking modes. Default is True.\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist()\n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T22:40:56.254706Z",
     "iopub.status.busy": "2025-04-29T22:40:56.254440Z",
     "iopub.status.idle": "2025-04-29T22:40:56.257758Z",
     "shell.execute_reply": "2025-04-29T22:40:56.257239Z",
     "shell.execute_reply.started": "2025-04-29T22:40:56.254688Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ç¦ç”¨æ€è€ƒæ¨¡å¼ï¼Œåªéœ€å¯¹å‚æ•° enable_thinking è¿›è¡Œå¦‚ä¸‹ä¿®æ”¹ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T22:41:11.693003Z",
     "iopub.status.busy": "2025-04-29T22:41:11.692709Z",
     "iopub.status.idle": "2025-04-29T22:41:16.236261Z",
     "shell.execute_reply": "2025-04-29T22:41:16.235839Z",
     "shell.execute_reply.started": "2025-04-29T22:41:11.692986Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: \n",
      "content: A large language model (LLM) is an advanced type of artificial intelligence that is trained on vast amounts of text data to understand and generate human-like text. These models can perform a wide range of tasks, such as answering questions, writing stories, coding, and translating languages. LLMs are characterized by their massive sizeâ€”often containing billions of parametersâ€”which allows them to capture complex patterns in language and provide more accurate and contextually relevant responses. They are widely used in various applications, including chatbots, virtual assistants, content creation, and data analysis.\n"
     ]
    }
   ],
   "source": [
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=False\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "\n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æŸ¥çœ‹ enable_thinking è®¾ç½®ä¸ºFalse åšäº†ä»€ä¹ˆï¼Ÿ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-29T22:41:23.525779Z",
     "iopub.status.busy": "2025-04-29T22:41:23.525270Z",
     "iopub.status.idle": "2025-04-29T22:41:23.528896Z",
     "shell.execute_reply": "2025-04-29T22:41:23.528431Z",
     "shell.execute_reply.started": "2025-04-29T22:41:23.525752Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>user\n",
      "Give me a short introduction to large language model.<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print(text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨ /think å’Œ /no_think æ§åˆ¶æ¨¡å‹çš„æ€è€ƒæ¨¡å¼ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-30T05:59:52.441898Z",
     "iopub.status.busy": "2025-04-30T05:59:52.441605Z",
     "iopub.status.idle": "2025-04-30T05:59:57.078858Z",
     "shell.execute_reply": "2025-04-30T05:59:57.078344Z",
     "shell.execute_reply.started": "2025-04-30T05:59:52.441882Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "thinking content: <think>\n",
      "\n",
      "</think>\n",
      "content: A large language model (LLM) is an advanced type of artificial intelligence that is trained on vast amounts of text data to understand and generate human-like text. These models can perform a wide range of tasks, such as answering questions, writing stories, coding, translating languages, and more. LLMs are built using deep learning techniques, particularly transformer architectures, which allow them to process and generate text efficiently. Their ability to comprehend context and produce coherent responses makes them powerful tools in various applications, from customer service to content creation.\n"
     ]
    }
   ],
   "source": [
    "# prepare the model input\n",
    "prompt = \"Give me a short introduction to large language model. /no_think\"\n",
    "messages = [\n",
    "    {\"role\": \"user\", \"content\": prompt}\n",
    "]\n",
    "text = tokenizer.apply_chat_template(\n",
    "    messages,\n",
    "    tokenize=False,\n",
    "    add_generation_prompt=True,\n",
    "    enable_thinking=True\n",
    ")\n",
    "model_inputs = tokenizer([text], return_tensors=\"pt\").to(model.device)\n",
    "# conduct text completion\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=32768\n",
    ")\n",
    "output_ids = generated_ids[0][len(model_inputs.input_ids[0]):].tolist() \n",
    "# parsing thinking content\n",
    "try:\n",
    "    # rindex finding 151668 (</think>)\n",
    "    index = len(output_ids) - output_ids[::-1].index(151668)\n",
    "except ValueError:\n",
    "    index = 0\n",
    "thinking_content = tokenizer.decode(output_ids[:index], skip_special_tokens=True).strip(\"\\n\")\n",
    "content = tokenizer.decode(output_ids[index:], skip_special_tokens=True).strip(\"\\n\")\n",
    "print(\"thinking content:\", thinking_content)\n",
    "print(\"content:\", content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨é­”æ­ API-Inference\n",
    "ä½¿ç”¨é­”æ­å…è´¹æä¾›çš„ Qwen3 API-Inferenceæ¨ç†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "å—¯ï¼Œç”¨æˆ·é—®çš„æ˜¯9.9å’Œ9.11å“ªä¸ªæ›´å¤§ã€‚é¦–å…ˆï¼Œæˆ‘éœ€è¦ç¡®è®¤è¿™ä¸¤ä¸ªæ•°å­—çš„ç»“æ„ã€‚çœ‹èµ·æ¥éƒ½æ˜¯å°æ•°ï¼Œä½†å¯èƒ½ç”¨æˆ·å¯¹å°æ•°ç‚¹åçš„ä½æ•°æœ‰ç–‘é—®ã€‚æ¯”å¦‚ï¼Œ9.9å¯èƒ½è¢«çœ‹ä½œæ˜¯9.90ï¼Œè€Œ9.11åˆ™æ˜¯ä¸¤ä½å°æ•°ã€‚è¿™æ—¶å€™æ¯”è¾ƒçš„è¯ï¼Œåº”è¯¥å…ˆç»Ÿä¸€å°æ•°ä½æ•°ï¼Œå†é€ä½æ¯”è¾ƒã€‚\n",
      "\n",
      "é¦–å…ˆï¼Œæ¯”è¾ƒæ•´æ•°éƒ¨åˆ†ï¼Œä¸¤è€…éƒ½æ˜¯9ï¼Œæ‰€ä»¥ç›¸åŒã€‚æ¥ä¸‹æ¥æ¯”è¾ƒå°æ•°éƒ¨åˆ†ã€‚9.9çš„å°æ•°éƒ¨åˆ†æ˜¯0.9ï¼Œè€Œ9.11çš„å°æ•°éƒ¨åˆ†æ˜¯0.11ã€‚è¿™æ—¶å€™å¯èƒ½éœ€è¦å°†å®ƒä»¬è½¬æ¢æˆç›¸åŒçš„å°æ•°ä½æ•°æ¥æ¯”è¾ƒã€‚æ¯”å¦‚ï¼ŒæŠŠ9.9å†™æˆ9.90ï¼Œè¿™æ ·å°æ•°éƒ¨åˆ†å°±æ˜¯90å’Œ11ã€‚æ˜¾ç„¶ï¼Œ90æ¯”11å¤§ï¼Œæ‰€ä»¥9.90å¤§äº9.11ã€‚å› æ­¤ï¼Œ9.9æ¯”9.11å¤§ã€‚\n",
      "\n",
      "ä¸è¿‡ï¼Œä¹Ÿæœ‰å¯èƒ½ç”¨æˆ·å¯¹å°æ•°çš„ç†è§£æœ‰è¯¯ï¼Œæ¯”å¦‚è®¤ä¸º9.11æ˜¯ä¸¤ä½å°æ•°ï¼Œè€Œ9.9æ˜¯ä¸€ä½å°æ•°ï¼Œå¯èƒ½è¯¯ä»¥ä¸º9.11æ›´å¤§ã€‚è¿™æ—¶å€™éœ€è¦è§£é‡Šå°æ•°æ¯”è¾ƒçš„æ–¹æ³•ï¼Œå³å…ˆçœ‹æ•´æ•°éƒ¨åˆ†ï¼Œå†é€ä½æ¯”è¾ƒå°æ•°éƒ¨åˆ†ï¼Œä»å·¦åˆ°å³ï¼Œç›´åˆ°åˆ†å‡ºå¤§å°ä¸ºæ­¢ã€‚\n",
      "\n",
      "å¦å¤–ï¼Œå¯èƒ½ç”¨æˆ·æ˜¯åˆšå¼€å§‹å­¦ä¹ å°æ•°æ¯”è¾ƒï¼Œå®¹æ˜“æ··æ·†ä½æ•°å’Œæ•°å€¼çš„å¤§å°ã€‚è¿™æ—¶å€™éœ€è¦ä¸¾ä¾‹å­è¯´æ˜ï¼Œæ¯”å¦‚0.9å’Œ0.11ï¼Œå…¶å®0.9æ›´å¤§ï¼Œå› ä¸ºç¬¬ä¸€ä½å°æ•°9æ¯”1å¤§ï¼Œåé¢çš„æ•°å­—ä¸éœ€è¦å†æ¯”è¾ƒäº†ã€‚åŒæ ·çš„é“ç†ï¼Œ9.9å’Œ9.11æ¯”è¾ƒï¼Œå°æ•°éƒ¨åˆ†ç¬¬ä¸€ä½9å’Œ1ï¼Œ9æ›´å¤§ï¼Œæ‰€ä»¥æ•´ä½“9.9æ›´å¤§ã€‚\n",
      "\n",
      "å†æ£€æŸ¥ä¸€ä¸‹æ˜¯å¦æœ‰å…¶ä»–å¯èƒ½æ€§ï¼Œæ¯”å¦‚æ˜¯å¦ç”¨æˆ·è¾“å…¥æœ‰è¯¯ï¼Œæˆ–è€…æœ‰æ²¡æœ‰è€ƒè™‘å•ä½çš„é—®é¢˜ã€‚ä½†é¢˜ç›®ä¸­æ²¡æœ‰æåˆ°å•ä½ï¼Œåº”è¯¥åªæ˜¯çº¯æ•°å­—æ¯”è¾ƒã€‚æ‰€ä»¥ç»“è®ºåº”è¯¥æ˜¯9.9å¤§äº9.11ã€‚\n",
      "\n",
      " === Final Answer ===\n",
      "\n",
      "9.9 å’Œ 9.11 çš„æ¯”è¾ƒå¯ä»¥é€šè¿‡ä»¥ä¸‹æ­¥éª¤è¿›è¡Œï¼š\n",
      "\n",
      "1. **ç»Ÿä¸€å°æ•°ä½æ•°**ï¼š  \n",
      "   å°† 9.9 è¡¥é›¶ä¸º **9.90**ï¼Œä»¥ä¾¿ä¸ 9.11 å¯¹é½å°æ•°ä½æ•°ã€‚\n",
      "\n",
      "2. **é€ä½æ¯”è¾ƒ**ï¼š  \n",
      "   - **æ•´æ•°éƒ¨åˆ†**ï¼šä¸¤è€…å‡ä¸º 9ï¼Œç›¸ç­‰ã€‚  \n",
      "   - **å°æ•°éƒ¨åˆ†**ï¼š  \n",
      "     - ç¬¬ä¸€ä½å°æ•°ï¼š9.90 çš„ **9** ä¸ 9.11 çš„ **1** æ¯”è¾ƒï¼Œ**9 > 1**ã€‚  \n",
      "     - åç»­ä½æ•°æ— éœ€å†æ¯”è¾ƒï¼Œç›´æ¥å¾—å‡ºç»“è®ºã€‚\n",
      "\n",
      "**ç»“è®º**ï¼š  \n",
      "**9.9 > 9.11**ã€‚  \n",
      "ï¼ˆå› ä¸ºå°æ•°éƒ¨åˆ† 0.90 æ˜æ˜¾å¤§äº 0.11ï¼‰"
     ]
    }
   ],
   "source": [
    "# MODELSCOPE_SDK_TOKEN = os.environ.get('MODELSCOPE_SDK_TOKEN')\n",
    "MODELSCOPE_SDK_TOKEN='ms-c5c215e6-c2c6-444b-8305-469cc21e243b'\n",
    "from openai import OpenAI\n",
    "client = OpenAI(\n",
    "    base_url='https://api-inference.modelscope.cn/v1/',\n",
    "    api_key=MODELSCOPE_SDK_TOKEN, # ModelScope Token\n",
    ")\n",
    "\n",
    "# set extra_body for thinking control\n",
    "extra_body = {\n",
    "    # enable thinking, set to False to disable\n",
    "    \"enable_thinking\": True,\n",
    "    # use thinking_budget to contorl num of tokens used for thinking\n",
    "    # \"thinking_budget\": 4096\n",
    "}\n",
    "response = client.chat.completions.create(\n",
    "    model='Qwen/Qwen3-32B',  # ModelScope Model-Id\n",
    "    messages=[\n",
    "        {\n",
    "          'role': 'user',\n",
    "          'content': '9.9å’Œ9.11è°å¤§'\n",
    "        }\n",
    "    ],\n",
    "    stream=True,\n",
    "    extra_body=extra_body\n",
    ")\n",
    "done_thinking = False\n",
    "for chunk in response:\n",
    "    thinking_chunk = chunk.choices[0].delta.reasoning_content\n",
    "    answer_chunk = chunk.choices[0].delta.content\n",
    "    if thinking_chunk != '':\n",
    "        print(thinking_chunk, end='', flush=True)\n",
    "    elif answer_chunk != '':\n",
    "        if not done_thinking:\n",
    "            print('\\n\\n === Final Answer ===\\n')\n",
    "            done_thinking = True\n",
    "        print(answer_chunk, end='', flush=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ä½¿ç”¨ms-swift"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install vllm -U\n",
    "!pip install git+https://github.com/modelscope/ms-swift.git"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ€è€ƒæ¨¡å¼æ¨ç†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid decimal literal (562069236.py, line 3)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[9], line 3\u001b[0;36m\u001b[0m\n\u001b[0;31m    --model /data/joey.wang/llm/models/pretrained/llm/Qwen3-8B \\\u001b[0m\n\u001b[0m                                                            ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid decimal literal\n"
     ]
    }
   ],
   "source": [
    "nproc_per_node=1\n",
    "CUDA_VISIBLE_DEVICES=5 NPROC_PER_NODE=$nproc_per_node swift infer \\\n",
    "    --model /data/joey.wang/llm/models/pretrained/llm/Qwen3-8B \\\n",
    "    --infer_backend vllm \\\n",
    "    --max_model_len 2048 \\\n",
    "    --max_new_tokens 1024 \\\n",
    "    --stream true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "éæ€è€ƒæ¨¡å¼ç•Œé¢æ¨ç†ï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 \\\n",
    "swift app \\\n",
    "    --model Qwen/Qwen3-8B \\\n",
    "    --infer_backend vllm \\\n",
    "    --stream true \\\n",
    "    --max_new_tokens 1024 \\\n",
    "    --max_model_len 2048 \\\n",
    "    --response_prefix '<think>\\n\\n</think>\\n\\n'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨¡å‹å¾®è°ƒ\n",
    "\n",
    "éå¸¸é«˜å…´å¬åˆ°Qwen3å’ŒQwen3-MoEçš„å¼€æºï¼Œ [ms-swift](https://github.com/modelscope/ms-swift)å¤§æ¨¡å‹è®­ç»ƒæ¡†æ¶é¦–å‘æ”¯æŒäº†Qwen3/Qwen3-MoEçš„**CPT/SFT/DPO/GRPO**ï¼ŒåŒæ—¶æ”¯æŒäº†Qwen3/Qwen3-MoEçš„**Megatron**è®­ç»ƒ(CPT/SFT)å®ç°ï¼Œåœ¨MoEæ¨¡å‹ä¸Šç›¸æ¯”transformerså®ç°çš„è®­ç»ƒé€Ÿåº¦**å¿«10å€**ã€‚\n",
    "\n",
    "æˆ‘ä»¬å°†å±•ç¤ºå¯è¿è¡Œçš„å¾®è°ƒdemoï¼Œå¹¶ç»™å‡ºè‡ªå®šä¹‰æ•°æ®é›†çš„æ ¼å¼ã€‚\n",
    "\n",
    "åœ¨å¼€å§‹å¾®è°ƒä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨çš„ç¯å¢ƒå·²å‡†å¤‡å¦¥å½“ã€‚"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!pip install git+https://github.com/modelscope/ms-swift.git\n",
    "\n",
    "!pip install liger-kernel transformers -U"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qwen3-8B SFT\n",
    "\n",
    "å¯¹Qwen3-8Bè¿›è¡Œè®­ç»ƒçš„è„šæœ¬å¦‚ä¸‹ï¼Œå¯åœ¨é­”æ­æä¾›çš„**å…è´¹ç®—åŠ›A10**ä¸­è¿è¡Œï¼šhttps://modelscope.cn/my/mynotebook\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# è®­ç»ƒæ˜¾å­˜ï¼š22GB\n",
    "# ä½ å¯ä»¥æŒ‡å®š`--dataset AI-ModelScope/alpaca-gpt4-data-zh`æ¥è·‘é€šå®éªŒ\n",
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "swift sft \\\n",
    "    --model Qwen/Qwen3-8B \\\n",
    "    --train_type lora \\\n",
    "    --dataset '<dataset-path>' \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 32 \\\n",
    "    --target_modules all-linear \\\n",
    "    --gradient_accumulation_steps 4 \\\n",
    "    --eval_steps 50 \\\n",
    "    --save_steps 50 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --logging_steps 5 \\\n",
    "    --max_length 2048 \\\n",
    "    --output_dir output \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --packing true \\\n",
    "    --use_liger_kernel true \\\n",
    "    --attn_impl flash_attn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**è‡ªå®šä¹‰æ•°æ®é›†**æ ¼å¼å¦‚ä¸‹ï¼ˆsystemå­—æ®µå¯é€‰ï¼‰ï¼ŒæŒ‡å®š`--dataset <dataset_path>`å³å¯ï¼š\n",
    "\n",
    "å‚è€ƒè‡ªå®šä¹‰æ•°æ®é›†æ–‡æ¡£ï¼šhttps://swift.readthedocs.io/zh-cn/latest/Customization/%E8%87%AA%E5%AE%9A%E4%B9%89%E6%95%B0%E6%8D%AE%E9%9B%86.html\n",
    "\n",
    "```jsonl\n",
    "{\"messages\": [{\"role\": \"user\", \"content\": \"æµ™æ±Ÿçš„çœä¼šåœ¨å“ªï¼Ÿ\"}, {\"role\": \"assistant\", \"content\": \"<think>\\nxxx\\n</think>\\n\\næµ™æ±Ÿçš„çœä¼šåœ¨æ­å·ã€‚\"}]}\n",
    "{\"messages\": [{\"role\": \"user\", \"content\": \"æµ™æ±Ÿçš„çœä¼šåœ¨å“ªï¼Ÿ /no_think\"}, {\"role\": \"assistant\", \"content\": \"<think>\\n\\n</think>\\n\\næµ™æ±Ÿçš„çœä¼šåœ¨æ­å·ã€‚\"}]}\n",
    "```\n",
    "\n",
    "**10åˆ†é’Ÿå¿«é€Ÿè‡ªæˆ‘è®¤çŸ¥å¾®è°ƒDemo**ï¼ˆæ˜¾å­˜å ç”¨ï¼š22GBï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-04-29T22:59:45.714129Z",
     "iopub.status.busy": "2025-04-29T22:59:45.713823Z",
     "iopub.status.idle": "2025-04-29T23:37:39.747497Z",
     "shell.execute_reply": "2025-04-29T23:37:39.746927Z",
     "shell.execute_reply.started": "2025-04-29T22:59:45.714106Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run sh: `/usr/local/bin/python /usr/local/lib/python3.11/site-packages/swift/cli/sft.py --model Qwen/Qwen3-8B --train_type lora --dataset swift/Qwen3-SFT-Mixin#2000 swift/self-cognition:qwen3#600 --torch_dtype bfloat16 --num_train_epochs 1 --per_device_train_batch_size 1 --per_device_eval_batch_size 1 --learning_rate 1e-4 --lora_rank 8 --lora_alpha 32 --target_modules all-linear --gradient_accumulation_steps 16 --eval_steps 50 --save_steps 50 --save_total_limit 2 --logging_steps 5 --max_length 2048 --output_dir output --warmup_ratio 0.05 --dataloader_num_workers 4 --use_liger_kernel true --model_author swift --model_name swift-robot`\n",
      "[2025-04-30 06:59:52,362] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n",
      "[INFO:swift] Successfully registered `/usr/local/lib/python3.11/site-packages/swift/llm/dataset/data/dataset_info.json`.\n",
      "[INFO:swift] rank: -1, local_rank: -1, world_size: 1, local_world_size: 1\n",
      "[INFO:swift.hub.hub] Downloading the model from ModelScope Hub, model_id: Qwen/Qwen3-8B\n",
      "Downloading Model from https://www.modelscope.cn to directory: /mnt/workspace/.cache/modelscope/models/Qwen/Qwen3-8B\n",
      "[WARNING:modelscope] Using branch: master as version is unstable, use with caution\n",
      "[INFO:swift] Loading the model using model_dir: /mnt/workspace/.cache/modelscope/models/Qwen/Qwen3-8B\n",
      "[INFO:swift] Setting args.lazy_tokenize: False\n",
      "[INFO:swift] output_dir: /mnt/workspace/qwen3/output/v0-20250430-065954\n",
      "[INFO:swift] Global seed set to 42\n",
      "[INFO:swift] args: TrainArguments(\n",
      "_n_gpu=-1,\n",
      "acc_steps=1,\n",
      "acc_strategy=token,\n",
      "accelerator_config={'dispatch_batches': False},\n",
      "adafactor=False,\n",
      "adalora_beta1=0.85,\n",
      "adalora_beta2=0.85,\n",
      "adalora_deltaT=1,\n",
      "adalora_init_r=12,\n",
      "adalora_orth_reg_weight=0.5,\n",
      "adalora_target_r=8,\n",
      "adalora_tfinal=0,\n",
      "adalora_tinit=0,\n",
      "adam_beta1=0.9,\n",
      "adam_beta2=0.95,\n",
      "adam_epsilon=1e-08,\n",
      "adapter_act=gelu,\n",
      "adapter_length=128,\n",
      "adapters=[],\n",
      "add_version=True,\n",
      "agent_template=None,\n",
      "attn_impl=None,\n",
      "auto_find_batch_size=False,\n",
      "average_tokens_across_devices=False,\n",
      "batch_eval_metrics=False,\n",
      "bf16=True,\n",
      "bf16_full_eval=False,\n",
      "bnb_4bit_compute_dtype=torch.bfloat16,\n",
      "bnb_4bit_quant_storage=None,\n",
      "bnb_4bit_quant_type=nf4,\n",
      "bnb_4bit_use_double_quant=True,\n",
      "boft_block_num=0,\n",
      "boft_block_size=4,\n",
      "boft_dropout=0.0,\n",
      "boft_n_butterfly_factor=1,\n",
      "check_model=True,\n",
      "ckpt_dir=None,\n",
      "columns={},\n",
      "create_checkpoint_symlink=False,\n",
      "custom_dataset_info=[],\n",
      "custom_register_path=[],\n",
      "data_seed=42,\n",
      "dataloader_drop_last=False,\n",
      "dataloader_num_workers=4,\n",
      "dataloader_persistent_workers=False,\n",
      "dataloader_pin_memory=True,\n",
      "dataloader_prefetch_factor=None,\n",
      "dataset=['swift/Qwen3-SFT-Mixin#2000', 'swift/self-cognition:qwen3#600'],\n",
      "dataset_num_proc=1,\n",
      "dataset_shuffle=True,\n",
      "ddp_backend=None,\n",
      "ddp_broadcast_buffers=None,\n",
      "ddp_bucket_cap_mb=None,\n",
      "ddp_find_unused_parameters=None,\n",
      "ddp_timeout=1800,\n",
      "debug=None,\n",
      "deepspeed=None,\n",
      "device_map=None,\n",
      "disable_tqdm=None,\n",
      "do_eval=False,\n",
      "do_predict=False,\n",
      "do_train=False,\n",
      "download_mode=reuse_dataset_if_exists,\n",
      "enable_cache=False,\n",
      "eval_accumulation_steps=None,\n",
      "eval_datasets=[],\n",
      "eval_datasets_args=None,\n",
      "eval_delay=0,\n",
      "eval_do_concat_batches=True,\n",
      "eval_generation_config=None,\n",
      "eval_limit=None,\n",
      "eval_on_start=False,\n",
      "eval_steps=50.0,\n",
      "eval_strategy=steps,\n",
      "eval_use_evalscope=False,\n",
      "eval_use_gather_object=False,\n",
      "external_plugins=[],\n",
      "fourier_n_frequency=2000,\n",
      "fourier_scaling=300.0,\n",
      "fp16=False,\n",
      "fp16_backend=auto,\n",
      "fp16_full_eval=False,\n",
      "fp16_opt_level=O1,\n",
      "freeze_aligner=True,\n",
      "freeze_llm=False,\n",
      "freeze_parameters=[],\n",
      "freeze_parameters_ratio=0.0,\n",
      "freeze_vit=True,\n",
      "fsdp=,\n",
      "fsdp_config=None,\n",
      "fsdp_min_num_params=0,\n",
      "fsdp_num=1,\n",
      "fsdp_transformer_layer_cls_to_wrap=None,\n",
      "full_determinism=False,\n",
      "galore_cos_threshold=0.4,\n",
      "galore_gamma_proj=2,\n",
      "galore_optim_per_parameter=False,\n",
      "galore_proj_bits=4,\n",
      "galore_proj_group_size=256,\n",
      "galore_proj_quant=False,\n",
      "galore_proj_type=std,\n",
      "galore_quantization=False,\n",
      "galore_queue_size=5,\n",
      "galore_rank=128,\n",
      "galore_scale=1.0,\n",
      "galore_target_modules=None,\n",
      "galore_update_proj_gap=50,\n",
      "galore_with_embedding=False,\n",
      "generation_config=None,\n",
      "generation_max_length=None,\n",
      "generation_num_beams=None,\n",
      "gradient_accumulation_steps=16,\n",
      "gradient_checkpointing=True,\n",
      "gradient_checkpointing_kwargs=None,\n",
      "greater_is_better=False,\n",
      "group_by_length=False,\n",
      "half_precision_backend=auto,\n",
      "hqq_axis=None,\n",
      "hub_always_push=False,\n",
      "hub_model_id=None,\n",
      "hub_private_repo=None,\n",
      "hub_strategy=every_save,\n",
      "hub_token=<HUB_TOKEN>,\n",
      "ignore_args_error=False,\n",
      "ignore_data_skip=False,\n",
      "include_for_metrics=[],\n",
      "include_inputs_for_metrics=False,\n",
      "include_num_input_tokens_seen=False,\n",
      "include_tokens_per_second=False,\n",
      "init_weights=True,\n",
      "interleave_prob=None,\n",
      "jit_mode_eval=False,\n",
      "label_names=None,\n",
      "label_smoothing_factor=0.0,\n",
      "lazy_tokenize=False,\n",
      "learning_rate=0.0001,\n",
      "length_column_name=length,\n",
      "lisa_activated_layers=0,\n",
      "lisa_step_interval=20,\n",
      "llamapro_num_groups=None,\n",
      "llamapro_num_new_blocks=4,\n",
      "load_args=False,\n",
      "load_best_model_at_end=False,\n",
      "load_data_args=False,\n",
      "load_dataset_config=None,\n",
      "local_rank=-1,\n",
      "local_repo_path=None,\n",
      "log_level=passive,\n",
      "log_level_replica=warning,\n",
      "log_on_each_node=True,\n",
      "logging_dir=/mnt/workspace/qwen3/output/v0-20250430-065954/runs,\n",
      "logging_first_step=True,\n",
      "logging_nan_inf_filter=True,\n",
      "logging_steps=5,\n",
      "logging_strategy=steps,\n",
      "logprobs=False,\n",
      "lora_alpha=32,\n",
      "lora_bias=none,\n",
      "lora_dropout=0.05,\n",
      "lora_dtype=None,\n",
      "lora_ga_batch_size=2,\n",
      "lora_ga_direction=ArB2r,\n",
      "lora_ga_iters=2,\n",
      "lora_ga_max_length=1024,\n",
      "lora_ga_scale=stable,\n",
      "lora_ga_stable_gamma=16,\n",
      "lora_modules=[],\n",
      "lora_rank=8,\n",
      "lorap_lr_ratio=None,\n",
      "loss_scale=default,\n",
      "loss_type=None,\n",
      "lr_scheduler_kwargs=None,\n",
      "lr_scheduler_type=cosine,\n",
      "max_grad_norm=1.0,\n",
      "max_length=2048,\n",
      "max_memory={},\n",
      "max_new_tokens=64,\n",
      "max_pixels=None,\n",
      "max_steps=-1,\n",
      "metric=None,\n",
      "metric_for_best_model=loss,\n",
      "metric_warmup_step=0,\n",
      "model=Qwen/Qwen3-8B,\n",
      "model_author=['swift'],\n",
      "model_kwargs={},\n",
      "model_name=['swift-robot'],\n",
      "model_revision=None,\n",
      "model_type=qwen3,\n",
      "modules_to_save=[],\n",
      "mp_parameters=,\n",
      "neftune_noise_alpha=None,\n",
      "no_cuda=False,\n",
      "norm_bbox=None,\n",
      "num_beams=1,\n",
      "num_labels=None,\n",
      "num_train_epochs=1.0,\n",
      "optim=adamw_torch,\n",
      "optim_args=None,\n",
      "optim_target_modules=None,\n",
      "optimizer=None,\n",
      "output_dir=/mnt/workspace/qwen3/output/v0-20250430-065954,\n",
      "overwrite_output_dir=False,\n",
      "packing=False,\n",
      "padding_side=right,\n",
      "past_index=-1,\n",
      "per_device_eval_batch_size=1,\n",
      "per_device_train_batch_size=1,\n",
      "predict_with_generate=False,\n",
      "prediction_loss_only=False,\n",
      "problem_type=None,\n",
      "push_to_hub=False,\n",
      "push_to_hub_model_id=None,\n",
      "push_to_hub_organization=None,\n",
      "push_to_hub_token=<PUSH_TO_HUB_TOKEN>,\n",
      "quant_bits=None,\n",
      "quant_method=None,\n",
      "ray_scope=last,\n",
      "reft_args=None,\n",
      "reft_intervention_type=LoreftIntervention,\n",
      "reft_layer_key=None,\n",
      "reft_layers=None,\n",
      "reft_rank=4,\n",
      "remove_unused_columns=True,\n",
      "repetition_penalty=None,\n",
      "report_to=['tensorboard'],\n",
      "response_prefix=None,\n",
      "restore_callback_states_from_checkpoint=False,\n",
      "resume_from_checkpoint=None,\n",
      "resume_only_model=False,\n",
      "rope_scaling=None,\n",
      "run_name=None,\n",
      "save_on_each_node=False,\n",
      "save_only_model=False,\n",
      "save_safetensors=True,\n",
      "save_steps=50.0,\n",
      "save_strategy=steps,\n",
      "save_total_limit=2,\n",
      "seed=42,\n",
      "sequence_parallel_size=1,\n",
      "shuffle_buffer_size=1000,\n",
      "skip_memory_metrics=True,\n",
      "sortish_sampler=False,\n",
      "split_dataset_ratio=0.01,\n",
      "stop_words=[],\n",
      "stopping_strategy=first_exhausted,\n",
      "stream=False,\n",
      "streaming=False,\n",
      "strict=False,\n",
      "swanlab_exp_name=None,\n",
      "swanlab_mode=cloud,\n",
      "swanlab_project=None,\n",
      "swanlab_token=<SWANLAB_TOKEN>,\n",
      "swanlab_workspace=None,\n",
      "system=None,\n",
      "target_modules=['all-linear'],\n",
      "target_regex=None,\n",
      "task_type=causal_lm,\n",
      "temperature=0.0,\n",
      "template=qwen3,\n",
      "template_backend=swift,\n",
      "tf32=None,\n",
      "top_k=None,\n",
      "top_logprobs=None,\n",
      "top_p=None,\n",
      "torch_compile=False,\n",
      "torch_compile_backend=None,\n",
      "torch_compile_mode=None,\n",
      "torch_dtype=torch.bfloat16,\n",
      "torch_empty_cache_steps=None,\n",
      "torchdynamo=None,\n",
      "tp_size=0,\n",
      "tpu_metrics_debug=False,\n",
      "tpu_num_cores=None,\n",
      "train_dataloader_shuffle=True,\n",
      "train_type=lora,\n",
      "trainable_parameters=[],\n",
      "truncation_strategy=delete,\n",
      "tuner_backend=peft,\n",
      "use_chat_template=True,\n",
      "use_cpu=False,\n",
      "use_dora=False,\n",
      "use_galore=False,\n",
      "use_hf=False,\n",
      "use_ipex=False,\n",
      "use_legacy_prediction_loop=False,\n",
      "use_liger_kernel=True,\n",
      "use_mps_device=False,\n",
      "use_rslora=False,\n",
      "use_swift_lora=False,\n",
      "val_dataset=[],\n",
      "val_dataset_shuffle=False,\n",
      "vera_d_initial=0.1,\n",
      "vera_dropout=0.0,\n",
      "vera_projection_prng_key=0,\n",
      "vera_rank=256,\n",
      "warmup_ratio=0.05,\n",
      "warmup_steps=0,\n",
      "weight_decay=0.1,\n",
      "zero_hpz_partition_size=None,\n",
      ")\n",
      "[INFO:swift.hub.hub] Downloading the model from ModelScope Hub, model_id: Qwen/Qwen3-8B\n",
      "Downloading Model from https://www.modelscope.cn to directory: /mnt/workspace/.cache/modelscope/models/Qwen/Qwen3-8B\n",
      "[WARNING:modelscope] Using branch: master as version is unstable, use with caution\n",
      "[INFO:swift] Loading the model using model_dir: /mnt/workspace/.cache/modelscope/models/Qwen/Qwen3-8B\n",
      "[INFO:swift] model_kwargs: {'device_map': 'cuda:0'}\n",
      "Loading checkpoint shards: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 5/5 [00:03<00:00,  1.33it/s]\n",
      "[INFO:swift] model.hf_device_map: {'': device(type='cuda', index=0)}\n",
      "[INFO:swift] model_info: ModelInfo(model_type='qwen3', model_dir='/mnt/workspace/.cache/modelscope/models/Qwen/Qwen3-8B', torch_dtype=torch.bfloat16, max_model_len=40960, quant_method=None, quant_bits=None, rope_scaling=None, config=Qwen3Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"Qwen3ForCausalLM\"\n",
      "  ],\n",
      "  \"attention_bias\": false,\n",
      "  \"attention_dropout\": 0.0,\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": 151645,\n",
      "  \"head_dim\": 128,\n",
      "  \"hidden_act\": \"silu\",\n",
      "  \"hidden_size\": 4096,\n",
      "  \"initializer_range\": 0.02,\n",
      "  \"intermediate_size\": 12288,\n",
      "  \"max_position_embeddings\": 40960,\n",
      "  \"max_window_layers\": 36,\n",
      "  \"model_type\": \"qwen3\",\n",
      "  \"num_attention_heads\": 32,\n",
      "  \"num_hidden_layers\": 36,\n",
      "  \"num_key_value_heads\": 8,\n",
      "  \"pad_token_id\": 151643,\n",
      "  \"rms_norm_eps\": 1e-06,\n",
      "  \"rope_scaling\": null,\n",
      "  \"rope_theta\": 1000000,\n",
      "  \"sliding_window\": null,\n",
      "  \"tie_word_embeddings\": false,\n",
      "  \"torch_dtype\": \"bfloat16\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"use_sliding_window\": false,\n",
      "  \"vocab_size\": 151936\n",
      "}\n",
      ", task_type='causal_lm', num_labels=None)\n",
      "[INFO:swift] model.generation_config: GenerationConfig {\n",
      "  \"bos_token_id\": 151643,\n",
      "  \"eos_token_id\": [\n",
      "    151645,\n",
      "    151643\n",
      "  ],\n",
      "  \"max_new_tokens\": 64,\n",
      "  \"pad_token_id\": 151643\n",
      "}\n",
      "\n",
      "[INFO:swift] default_system: None\n",
      "[INFO:swift] response_prefix: ''\n",
      "[INFO:swift] agent_template: hermes\n",
      "[INFO:swift] max_length: 2048\n",
      "[INFO:swift] norm_bbox: norm1000\n",
      "[INFO:swift] Start time of running main: 2025-04-30 06:59:59.892937\n",
      "[INFO:swift] Downloading the dataset from ModelScope, dataset_id: swift/Qwen3-SFT-Mixin\n",
      "[WARNING:modelscope] Use trust_remote_code=True. Will invoke codes from Qwen3-SFT-Mixin. Please make sure that you can trust the external codes.\n",
      "[WARNING:modelscope] Use trust_remote_code=True. Will invoke codes from swift/Qwen3-SFT-Mixin. Please make sure that you can trust the external codes.\n",
      "[WARNING:modelscope] Use trust_remote_code=True. Will invoke codes from swift/Qwen3-SFT-Mixin. Please make sure that you can trust the external codes.\n",
      "[WARNING:modelscope] Use trust_remote_code=True. Will invoke codes from swift/Qwen3-SFT-Mixin. Please make sure that you can trust the external codes.\n",
      "Downloading [README.md]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 190/190 [00:00<00:00, 2.02MB/s]\n",
      "[INFO:modelscope] storing https://www.modelscope.cn/api/v1/datasets/swift/Qwen3-SFT-Mixin/repo?Source=SDK&Revision=master&FilePath=README.md&View=False in cache at /mnt/workspace/.cache/modelscope/datasets/f1864246c164f6c660ab58aa991af9785362c6c02e5286109163da637c3c531f\n",
      "[INFO:modelscope] creating metadata file for /mnt/workspace/.cache/modelscope/datasets/f1864246c164f6c660ab58aa991af9785362c6c02e5286109163da637c3c531f\n",
      "Downloading data: 17.4MB [00:00, 35.0MB/s]\n",
      "[INFO:modelscope] storing https://www.modelscope.cn/api/v1/datasets/swift/Qwen3-SFT-Mixin/repo?Source=SDK&Revision=master&FilePath=qwen3_32b_distill_1k.jsonl in cache at /mnt/workspace/.cache/modelscope/datasets/downloads/5449be2eeecdde4bbdb6ca373bd44bdd93227ffc0efa754b691caabefae332b5\n",
      "[INFO:modelscope] creating metadata file for /mnt/workspace/.cache/modelscope/datasets/downloads/5449be2eeecdde4bbdb6ca373bd44bdd93227ffc0efa754b691caabefae332b5\n",
      "Generating train split: 1000 examples [00:00, 7158.19 examples/s]\n",
      "[INFO:swift] create tmp_dir: /mnt/workspace/.cache/modelscope/tmp/hf_datasets-7rsyrcak\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1000/1000 [00:00<00:00, 2921.65 examples/s]\n",
      "[WARNING:swift] dataset_sample:1990 is greater than len(dataset):990, repeated sampling will be performed.\n",
      "[INFO:swift] Downloading the dataset from ModelScope, dataset_id: swift/self-cognition\n",
      "[WARNING:modelscope] Use trust_remote_code=True. Will invoke codes from self-cognition. Please make sure that you can trust the external codes.\n",
      "[WARNING:modelscope] Use trust_remote_code=True. Will invoke codes from swift/self-cognition. Please make sure that you can trust the external codes.\n",
      "[WARNING:modelscope] Use trust_remote_code=True. Will invoke codes from swift/self-cognition. Please make sure that you can trust the external codes.\n",
      "[WARNING:modelscope] Use trust_remote_code=True. Will invoke codes from swift/self-cognition. Please make sure that you can trust the external codes.\n",
      "Downloading [README.md]: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2.54k/2.54k [00:00<00:00, 17.2MB/s]\n",
      "[INFO:modelscope] storing https://www.modelscope.cn/api/v1/datasets/swift/self-cognition/repo?Source=SDK&Revision=master&FilePath=README.md&View=False in cache at /mnt/workspace/.cache/modelscope/datasets/b977b495d3bd0f0d74785b0ed77db79c2eb11dc76de6ac5809568f169f358494\n",
      "[INFO:modelscope] creating metadata file for /mnt/workspace/.cache/modelscope/datasets/b977b495d3bd0f0d74785b0ed77db79c2eb11dc76de6ac5809568f169f358494\n",
      "Downloading data: 23.8kB [00:00, 727kB/s]\n",
      "[INFO:modelscope] storing https://www.modelscope.cn/api/v1/datasets/swift/self-cognition/repo?Source=SDK&Revision=master&FilePath=self_cognition.jsonl in cache at /mnt/workspace/.cache/modelscope/datasets/downloads/cfb4df88d977913540a71cbf5c12f778c7552e1968aab47079439b784a96f0a1\n",
      "[INFO:modelscope] creating metadata file for /mnt/workspace/.cache/modelscope/datasets/downloads/cfb4df88d977913540a71cbf5c12f778c7552e1968aab47079439b784a96f0a1\n",
      "Generating train split: 108 examples [00:00, 7814.53 examples/s]\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 108/108 [00:00<00:00, 2400.76 examples/s]\n",
      "[WARNING:swift] dataset_sample:599 is greater than len(dataset):107, repeated sampling will be performed.\n",
      "[INFO:swift] train_dataset: Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 2589\n",
      "})\n",
      "[INFO:swift] val_dataset: Dataset({\n",
      "    features: ['messages'],\n",
      "    num_rows: 11\n",
      "})\n",
      "[INFO:swift] The split dataset from the training set will be saved at: /mnt/workspace/qwen3/output/v0-20250430-065954/val_dataset.jsonl.\n",
      "Map:   0%|                                      | 0/2589 [00:00<?, ? examples/s][INFO:swift] Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/preprocessor/core.py\", line 169, in batched_preprocess\n",
      "    row = self.preprocess(row)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/utils.py\", line 278, in preprocess\n",
      "    return self.template.encode(row)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/template/base.py\", line 424, in encode\n",
      "    raise MaxLengthError(f'Current length of row({length}) is larger'\n",
      "swift.llm.template.base.MaxLengthError: Current length of row(2132) is larger than the max_length(2048).\n",
      "\n",
      "[WARNING:swift] ğŸ‘†ğŸ‘†ğŸ‘†There are errors in the dataset, the data will be deleted\n",
      "[INFO:swift] Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/preprocessor/core.py\", line 169, in batched_preprocess\n",
      "    row = self.preprocess(row)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/utils.py\", line 278, in preprocess\n",
      "    return self.template.encode(row)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/template/base.py\", line 424, in encode\n",
      "    raise MaxLengthError(f'Current length of row({length}) is larger'\n",
      "swift.llm.template.base.MaxLengthError: Current length of row(2097) is larger than the max_length(2048).\n",
      "\n",
      "[WARNING:swift] ğŸ‘†ğŸ‘†ğŸ‘†There are errors in the dataset, the data will be deleted\n",
      "[INFO:swift] Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/preprocessor/core.py\", line 169, in batched_preprocess\n",
      "    row = self.preprocess(row)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/utils.py\", line 278, in preprocess\n",
      "    return self.template.encode(row)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/template/base.py\", line 424, in encode\n",
      "    raise MaxLengthError(f'Current length of row({length}) is larger'\n",
      "swift.llm.template.base.MaxLengthError: Current length of row(2066) is larger than the max_length(2048).\n",
      "\n",
      "[WARNING:swift] ğŸ‘†ğŸ‘†ğŸ‘†There are errors in the dataset, the data will be deleted\n",
      "[INFO:swift] Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/preprocessor/core.py\", line 169, in batched_preprocess\n",
      "    row = self.preprocess(row)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/utils.py\", line 278, in preprocess\n",
      "    return self.template.encode(row)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/template/base.py\", line 424, in encode\n",
      "    raise MaxLengthError(f'Current length of row({length}) is larger'\n",
      "swift.llm.template.base.MaxLengthError: Current length of row(2127) is larger than the max_length(2048).\n",
      "\n",
      "[WARNING:swift] ğŸ‘†ğŸ‘†ğŸ‘†There are errors in the dataset, the data will be deleted\n",
      "[INFO:swift] Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/preprocessor/core.py\", line 169, in batched_preprocess\n",
      "    row = self.preprocess(row)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/utils.py\", line 278, in preprocess\n",
      "    return self.template.encode(row)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/template/base.py\", line 424, in encode\n",
      "    raise MaxLengthError(f'Current length of row({length}) is larger'\n",
      "swift.llm.template.base.MaxLengthError: Current length of row(2145) is larger than the max_length(2048).\n",
      "\n",
      "[WARNING:swift] ğŸ‘†ğŸ‘†ğŸ‘†There are errors in the dataset, the data will be deleted\n",
      "[INFO:swift] Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/preprocessor/core.py\", line 169, in batched_preprocess\n",
      "    row = self.preprocess(row)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/utils.py\", line 278, in preprocess\n",
      "    return self.template.encode(row)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/template/base.py\", line 424, in encode\n",
      "    raise MaxLengthError(f'Current length of row({length}) is larger'\n",
      "swift.llm.template.base.MaxLengthError: Current length of row(2099) is larger than the max_length(2048).\n",
      "\n",
      "[WARNING:swift] ğŸ‘†ğŸ‘†ğŸ‘†There are errors in the dataset, the data will be deleted\n",
      "[INFO:swift] Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/preprocessor/core.py\", line 169, in batched_preprocess\n",
      "    row = self.preprocess(row)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/utils.py\", line 278, in preprocess\n",
      "    return self.template.encode(row)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/template/base.py\", line 424, in encode\n",
      "    raise MaxLengthError(f'Current length of row({length}) is larger'\n",
      "swift.llm.template.base.MaxLengthError: Current length of row(2133) is larger than the max_length(2048).\n",
      "\n",
      "[WARNING:swift] ğŸ‘†ğŸ‘†ğŸ‘†There are errors in the dataset, the data will be deleted\n",
      "[INFO:swift] Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/preprocessor/core.py\", line 169, in batched_preprocess\n",
      "    row = self.preprocess(row)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/utils.py\", line 278, in preprocess\n",
      "    return self.template.encode(row)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/template/base.py\", line 424, in encode\n",
      "    raise MaxLengthError(f'Current length of row({length}) is larger'\n",
      "swift.llm.template.base.MaxLengthError: Current length of row(2088) is larger than the max_length(2048).\n",
      "\n",
      "[WARNING:swift] ğŸ‘†ğŸ‘†ğŸ‘†There are errors in the dataset, the data will be deleted\n",
      "[INFO:swift] Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/preprocessor/core.py\", line 169, in batched_preprocess\n",
      "    row = self.preprocess(row)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/utils.py\", line 278, in preprocess\n",
      "    return self.template.encode(row)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/template/base.py\", line 424, in encode\n",
      "    raise MaxLengthError(f'Current length of row({length}) is larger'\n",
      "swift.llm.template.base.MaxLengthError: Current length of row(2101) is larger than the max_length(2048).\n",
      "\n",
      "[WARNING:swift] ğŸ‘†ğŸ‘†ğŸ‘†There are errors in the dataset, the data will be deleted\n",
      "[INFO:swift] Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/preprocessor/core.py\", line 169, in batched_preprocess\n",
      "    row = self.preprocess(row)\n",
      "          ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/dataset/utils.py\", line 278, in preprocess\n",
      "    return self.template.encode(row)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/torch/utils/_contextlib.py\", line 116, in decorate_context\n",
      "    return func(*args, **kwargs)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"/usr/local/lib/python3.11/site-packages/swift/llm/template/base.py\", line 424, in encode\n",
      "    raise MaxLengthError(f'Current length of row({length}) is larger'\n",
      "swift.llm.template.base.MaxLengthError: Current length of row(2100) is larger than the max_length(2048).\n",
      "\n",
      "[WARNING:swift] ğŸ‘†ğŸ‘†ğŸ‘†There are errors in the dataset, the data will be deleted\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2589/2589 [00:11<00:00, 230.92 examples/s]\n",
      "[INFO:swift] Dataset filtered, origin length: 2589, filtered dataset length: 1638\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 11/11 [00:00<00:00, 35.87 examples/s]\n",
      "[INFO:swift] Dataset filtered, origin length: 11, filtered dataset length: 7\n",
      "[INFO:swift] [INPUT_IDS] [151644, 872, 198, 108386, 3837, 56568, 18830, 108965, 99245, 100078, 101036, 608, 2152, 5854, 766, 151645, 198, 151644, 77091, 198, 151667, 271, 151668, 271, 108386, 6313, 106249, 15469, 110498, 3837, 109944, 100364, 56568, 102104, 100646, 86119, 3837, 99553, 27369, 3837, 100364, 56568, 100638, 104768, 3837, 71817, 102064, 101069, 3837, 33108, 56568, 71817, 100836, 100281, 104008, 1773, 100783, 112735, 99245, 100364, 3837, 100671, 104927, 99788, 101121, 102595, 3837, 35946, 101938, 110121, 85336, 100364, 56568, 1773, 14880, 106525, 56568, 104139, 100398, 104378, 3837, 105351, 110121, 101929, 1773, 151645]\n",
      "[INFO:swift] [INPUT] <|im_start|>user\n",
      "ä½ å¥½ï¼Œä½ æœ‰å¸®æˆ‘ä»€ä¹ˆå¿™å‘¢ /no_think<|im_end|>\n",
      "<|im_start|>assistant\n",
      "<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "ä½ å¥½ï¼ä½œä¸ºä¸€ä¸ªAIåŠ©æ‰‹ï¼Œæˆ‘å¯ä»¥å¸®åŠ©ä½ å›ç­”å„ç§é—®é¢˜ï¼Œæä¾›ä¿¡æ¯ï¼Œå¸®åŠ©ä½ è§£å†³éš¾é¢˜ï¼Œè¿›è¡Œè¯­è¨€äº¤æµï¼Œå’Œä½ è¿›è¡Œé—²èŠç­‰ç­‰ã€‚æ— è®ºä½ éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Œåªè¦æ˜¯æˆ‘èƒ½åŠ›èŒƒå›´å†…çš„ï¼Œæˆ‘éƒ½ä¼šå°½åŠ›å»å¸®åŠ©ä½ ã€‚è¯·å‘Šè¯‰æˆ‘ä½ æœ‰ä»€ä¹ˆå…·ä½“çš„éœ€æ±‚ï¼Œæˆ‘ä¼šå°½åŠ›æ»¡è¶³ã€‚<|im_end|>\n",
      "[INFO:swift] [LABELS_IDS] [-100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, -100, 151667, 271, 151668, 271, 108386, 6313, 106249, 15469, 110498, 3837, 109944, 100364, 56568, 102104, 100646, 86119, 3837, 99553, 27369, 3837, 100364, 56568, 100638, 104768, 3837, 71817, 102064, 101069, 3837, 33108, 56568, 71817, 100836, 100281, 104008, 1773, 100783, 112735, 99245, 100364, 3837, 100671, 104927, 99788, 101121, 102595, 3837, 35946, 101938, 110121, 85336, 100364, 56568, 1773, 14880, 106525, 56568, 104139, 100398, 104378, 3837, 105351, 110121, 101929, 1773, 151645]\n",
      "[INFO:swift] [LABELS] [-100 * 20]<think>\n",
      "\n",
      "</think>\n",
      "\n",
      "ä½ å¥½ï¼ä½œä¸ºä¸€ä¸ªAIåŠ©æ‰‹ï¼Œæˆ‘å¯ä»¥å¸®åŠ©ä½ å›ç­”å„ç§é—®é¢˜ï¼Œæä¾›ä¿¡æ¯ï¼Œå¸®åŠ©ä½ è§£å†³éš¾é¢˜ï¼Œè¿›è¡Œè¯­è¨€äº¤æµï¼Œå’Œä½ è¿›è¡Œé—²èŠç­‰ç­‰ã€‚æ— è®ºä½ éœ€è¦ä»€ä¹ˆå¸®åŠ©ï¼Œåªè¦æ˜¯æˆ‘èƒ½åŠ›èŒƒå›´å†…çš„ï¼Œæˆ‘éƒ½ä¼šå°½åŠ›å»å¸®åŠ©ä½ ã€‚è¯·å‘Šè¯‰æˆ‘ä½ æœ‰ä»€ä¹ˆå…·ä½“çš„éœ€æ±‚ï¼Œæˆ‘ä¼šå°½åŠ›æ»¡è¶³ã€‚<|im_end|>\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 1638/1638 [00:01<00:00, 1264.17 examples/s]\n",
      "[INFO:swift] Dataset Token Length: 976.672772Â±754.404822, min=26.000000, max=2048.000000, size=1638\n",
      "Map: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:00<00:00, 169.00 examples/s]\n",
      "[INFO:swift] Dataset Token Length: 1230.571429Â±555.204945, min=52.000000, max=1903.000000, size=7\n",
      "[INFO:swift] The TrainArguments will be saved in: /mnt/workspace/qwen3/output/v0-20250430-065954/args.json\n",
      "[INFO:swift] lora_config: LoraConfig(task_type='CAUSAL_LM', peft_type=<PeftType.LORA: 'LORA'>, auto_mapping=None, base_model_name_or_path='/mnt/workspace/.cache/modelscope/models/Qwen/Qwen3-8B', revision=None, inference_mode=False, r=8, target_modules={'o_proj', 'up_proj', 'k_proj', 'q_proj', 'gate_proj', 'down_proj', 'v_proj'}, exclude_modules=None, lora_alpha=32, lora_dropout=0.05, fan_in_fan_out=False, bias='none', use_rslora=False, modules_to_save=[], init_lora_weights=True, layers_to_transform=None, layers_pattern=None, rank_pattern={}, alpha_pattern={}, megatron_config=None, megatron_core='megatron.core', trainable_token_indices=None, loftq_config={}, eva_config=None, corda_config=None, use_dora=False, layer_replication=None, runtime_config=LoraRuntimeConfig(ephemeral_gpu_offload=False), lora_bias=False, lora_dtype=None, lorap_lr_ratio=None, lorap_emb_lr=1e-06)\n",
      "[INFO:swift] model: PeftModelForCausalLM(\n",
      "  (base_model): LoraModel(\n",
      "    (model): Qwen3ForCausalLM(\n",
      "      (model): Qwen3Model(\n",
      "        (embed_tokens): Embedding(151936, 4096)\n",
      "        (layers): ModuleList(\n",
      "          (0-35): 36 x Qwen3DecoderLayer(\n",
      "            (self_attn): Qwen3Attention(\n",
      "              (q_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (k_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (v_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=1024, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=1024, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (o_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (q_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "              (k_norm): Qwen3RMSNorm((128,), eps=1e-06)\n",
      "            )\n",
      "            (mlp): Qwen3MLP(\n",
      "              (gate_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=12288, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (up_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=4096, out_features=12288, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=4096, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=12288, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (down_proj): lora.Linear(\n",
      "                (base_layer): Linear(in_features=12288, out_features=4096, bias=False)\n",
      "                (lora_dropout): ModuleDict(\n",
      "                  (default): Dropout(p=0.05, inplace=False)\n",
      "                )\n",
      "                (lora_A): ModuleDict(\n",
      "                  (default): Linear(in_features=12288, out_features=8, bias=False)\n",
      "                )\n",
      "                (lora_B): ModuleDict(\n",
      "                  (default): Linear(in_features=8, out_features=4096, bias=False)\n",
      "                )\n",
      "                (lora_embedding_A): ParameterDict()\n",
      "                (lora_embedding_B): ParameterDict()\n",
      "                (lora_magnitude_vector): ModuleDict()\n",
      "              )\n",
      "              (act_fn): SiLU()\n",
      "            )\n",
      "            (input_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
      "            (post_attention_layernorm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
      "          )\n",
      "        )\n",
      "        (norm): Qwen3RMSNorm((4096,), eps=1e-06)\n",
      "        (rotary_emb): Qwen3RotaryEmbedding()\n",
      "      )\n",
      "      (lm_head): Linear(in_features=4096, out_features=151936, bias=False)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "[INFO:swift] model_parameter_info: PeftModelForCausalLM: 8212.5588M Params (21.8235M Trainable [0.2657%]), 0.0001M Buffers.\n",
      "/usr/local/lib/python3.11/site-packages/swift/trainers/mixin.py:86: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Seq2SeqTrainer.__init__`. Use `processing_class` instead.\n",
      "  super().__init__(\n",
      "Detected kernel version 4.19.91, which is below the recommended minimum of 5.5.0; this can cause the process to hang. It is recommended to upgrade the kernel to the minimum version or higher.\n",
      "No label_names provided for model class `PeftModelForCausalLM`. Since `PeftModel` hides base models input arguments, if label_names is not given, label_names can't be set automatically within `Trainer`. Note that empty label_names list will be used instead.\n",
      "[INFO:swift] The logging file will be saved in: /mnt/workspace/qwen3/output/v0-20250430-065954/logging.jsonl\n",
      "{'loss': 0.69616938, 'token_acc': 0.78829125, 'grad_norm': 0.26774693, 'learning_rate': 1.667e-05, 'memory(GiB)': 21.16, 'train_speed(iter/s)': 0.036247, 'epoch': 0.01, 'global_step/max_steps': '1/102', 'percentage': '0.98%', 'elapsed_time': '27s', 'remaining_time': '46m 2s'}\n",
      "{'loss': 0.7830407, 'token_acc': 0.79336782, 'grad_norm': 1.01455951, 'learning_rate': 8.333e-05, 'memory(GiB)': 21.16, 'train_speed(iter/s)': 0.044254, 'epoch': 0.05, 'global_step/max_steps': '5/102', 'percentage': '4.90%', 'elapsed_time': '1m 52s', 'remaining_time': '36m 26s'}\n",
      "{'loss': 0.74499516, 'token_acc': 0.78412451, 'grad_norm': 0.26490724, 'learning_rate': 9.957e-05, 'memory(GiB)': 21.16, 'train_speed(iter/s)': 0.047555, 'epoch': 0.1, 'global_step/max_steps': '10/102', 'percentage': '9.80%', 'elapsed_time': '3m 30s', 'remaining_time': '32m 12s'}\n",
      "{'loss': 0.69298482, 'token_acc': 0.78686866, 'grad_norm': 0.25131449, 'learning_rate': 9.785e-05, 'memory(GiB)': 21.16, 'train_speed(iter/s)': 0.047773, 'epoch': 0.15, 'global_step/max_steps': '15/102', 'percentage': '14.71%', 'elapsed_time': '5m 13s', 'remaining_time': '30m 19s'}\n",
      "{'loss': 0.6494771, 'token_acc': 0.79606294, 'grad_norm': 0.19600582, 'learning_rate': 9.484e-05, 'memory(GiB)': 21.16, 'train_speed(iter/s)': 0.049047, 'epoch': 0.2, 'global_step/max_steps': '20/102', 'percentage': '19.61%', 'elapsed_time': '6m 47s', 'remaining_time': '27m 50s'}\n",
      "{'loss': 0.60292683, 'token_acc': 0.80728857, 'grad_norm': 0.21342412, 'learning_rate': 9.064e-05, 'memory(GiB)': 21.16, 'train_speed(iter/s)': 0.048167, 'epoch': 0.24, 'global_step/max_steps': '25/102', 'percentage': '24.51%', 'elapsed_time': '8m 38s', 'remaining_time': '26m 37s'}\n",
      "{'loss': 0.62766142, 'token_acc': 0.79926009, 'grad_norm': 0.17726883, 'learning_rate': 8.536e-05, 'memory(GiB)': 21.16, 'train_speed(iter/s)': 0.047153, 'epoch': 0.29, 'global_step/max_steps': '30/102', 'percentage': '29.41%', 'elapsed_time': '10m 35s', 'remaining_time': '25m 26s'}\n",
      "{'loss': 0.58281717, 'token_acc': 0.81095233, 'grad_norm': 0.18043336, 'learning_rate': 7.912e-05, 'memory(GiB)': 21.16, 'train_speed(iter/s)': 0.046737, 'epoch': 0.34, 'global_step/max_steps': '35/102', 'percentage': '34.31%', 'elapsed_time': '12m 28s', 'remaining_time': '23m 52s'}\n",
      "{'loss': 0.59440455, 'token_acc': 0.80798071, 'grad_norm': 0.21064375, 'learning_rate': 7.211e-05, 'memory(GiB)': 21.16, 'train_speed(iter/s)': 0.047331, 'epoch': 0.39, 'global_step/max_steps': '40/102', 'percentage': '39.22%', 'elapsed_time': '14m 4s', 'remaining_time': '21m 49s'}\n",
      "{'loss': 0.57923422, 'token_acc': 0.81099405, 'grad_norm': 0.16434595, 'learning_rate': 6.451e-05, 'memory(GiB)': 21.16, 'train_speed(iter/s)': 0.047254, 'epoch': 0.44, 'global_step/max_steps': '45/102', 'percentage': '44.12%', 'elapsed_time': '15m 52s', 'remaining_time': '20m 5s'}\n",
      "{'loss': 0.63240795, 'token_acc': 0.79813747, 'grad_norm': 0.27050704, 'learning_rate': 5.653e-05, 'memory(GiB)': 21.16, 'train_speed(iter/s)': 0.047912, 'epoch': 0.49, 'global_step/max_steps': '50/102', 'percentage': '49.02%', 'elapsed_time': '17m 23s', 'remaining_time': '18m 5s'}\n",
      "Train:  49%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–                 | 50/102 [17:23<15:37, 18.03s/it]\n",
      "{'eval_loss': 0.63324869, 'eval_token_acc': 0.81367052, 'eval_runtime': 3.4181, 'eval_samples_per_second': 2.048, 'eval_steps_per_second': 2.048, 'epoch': 0.49, 'global_step/max_steps': '50/102', 'percentage': '49.02%', 'elapsed_time': '17m 26s', 'remaining_time': '18m 8s'}\n",
      "Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:02<00:00,  2.64it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/qwen3/output/v0-20250430-065954/checkpoint-50\n",
      "{'loss': 0.59517112, 'token_acc': 0.80671907, 'grad_norm': 0.22687259, 'learning_rate': 4.836e-05, 'memory(GiB)': 21.89, 'train_speed(iter/s)': 0.047885, 'epoch': 0.54, 'global_step/max_steps': '55/102', 'percentage': '53.92%', 'elapsed_time': '19m 8s', 'remaining_time': '16m 21s'}\n",
      "{'loss': 0.61055079, 'token_acc': 0.8026431, 'grad_norm': 0.22288062, 'learning_rate': 4.025e-05, 'memory(GiB)': 21.89, 'train_speed(iter/s)': 0.047725, 'epoch': 0.59, 'global_step/max_steps': '60/102', 'percentage': '58.82%', 'elapsed_time': '20m 56s', 'remaining_time': '14m 39s'}\n",
      "{'loss': 0.60835032, 'token_acc': 0.80277504, 'grad_norm': 0.18202978, 'learning_rate': 3.239e-05, 'memory(GiB)': 21.89, 'train_speed(iter/s)': 0.047049, 'epoch': 0.63, 'global_step/max_steps': '65/102', 'percentage': '63.73%', 'elapsed_time': '23m 1s', 'remaining_time': '13m 6s'}\n",
      "{'loss': 0.61953635, 'token_acc': 0.7983146, 'grad_norm': 0.18585092, 'learning_rate': 2.5e-05, 'memory(GiB)': 21.89, 'train_speed(iter/s)': 0.047079, 'epoch': 0.68, 'global_step/max_steps': '70/102', 'percentage': '68.63%', 'elapsed_time': '24m 46s', 'remaining_time': '11m 19s'}\n",
      "Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102/102 [36:52<00:00, 21.56s/it]\n",
      "{'eval_loss': 0.6186465, 'eval_token_acc': 0.81343195, 'eval_runtime': 3.3783, 'eval_samples_per_second': 2.072, 'eval_steps_per_second': 2.072, 'epoch': 1.0, 'global_step/max_steps': '102/102', 'percentage': '100.00%', 'elapsed_time': '36m 56s', 'remaining_time': '0s'}\n",
      "Val: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 7/7 [00:02<00:00,  2.66it/s]\n",
      "[INFO:swift] Saving model checkpoint to /mnt/workspace/qwen3/output/v0-20250430-065954/checkpoint-102\n",
      "{'train_runtime': 2217.5289, 'train_samples_per_second': 0.739, 'train_steps_per_second': 0.046, 'train_loss': 0.62729594, 'epoch': 1.0, 'global_step/max_steps': '102/102', 'percentage': '100.00%', 'elapsed_time': '36m 57s', 'remaining_time': '0s'}\n",
      "Train: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 102/102 [36:57<00:00, 21.74s/it]\n",
      "[INFO:swift] last_model_checkpoint: /mnt/workspace/qwen3/output/v0-20250430-065954/checkpoint-102\n",
      "[INFO:swift] best_model_checkpoint: /mnt/workspace/qwen3/output/v0-20250430-065954/checkpoint-102\n",
      "[INFO:swift] images_dir: /mnt/workspace/qwen3/output/v0-20250430-065954/images\n",
      "[INFO:swift] End time of running main: 2025-04-30 07:37:37.926746\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=7 \\\n",
    "swift sft \\\n",
    "    --model Qwen/Qwen3-8B \\\n",
    "    --train_type lora \\\n",
    "    --dataset 'swift/Qwen3-SFT-Mixin#2000' \\\n",
    "              'swift/self-cognition:qwen3#600' \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size 1 \\\n",
    "    --per_device_eval_batch_size 1 \\\n",
    "    --learning_rate 1e-4 \\\n",
    "    --lora_rank 8 \\\n",
    "    --lora_alpha 32 \\\n",
    "    --target_modules all-linear \\\n",
    "    --gradient_accumulation_steps 16 \\\n",
    "    --eval_steps 50 \\\n",
    "    --save_steps 50 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --logging_steps 5 \\\n",
    "    --max_length 2048 \\\n",
    "    --output_dir output \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --use_liger_kernel true \\\n",
    "    --model_author swift \\\n",
    "    --model_name swift-robot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "æ˜¾å­˜å ç”¨ï¼š22GiB\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "æ¨ç†æµ‹è¯•å¾®è°ƒæ•ˆæœï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CUDA_VISIBLE_DEVICES=0 \\\n",
    "swift app \\\n",
    "    --adapters output/vx-xxx/checkpoint-xxx \\\n",
    "    --stream true \\\n",
    "    --temperature 0 \\\n",
    "    --max_new_tokens 2048"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨Pythonçš„æ–¹å¼æ¨ç†ï¼šï¼ˆtransformers or swiftï¼‰"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-04-30T00:13:26.755440Z",
     "iopub.status.busy": "2025-04-30T00:13:26.755108Z",
     "iopub.status.idle": "2025-04-30T00:13:28.994536Z",
     "shell.execute_reply": "2025-04-30T00:13:28.994057Z",
     "shell.execute_reply.started": "2025-04-30T00:13:26.755425Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading Model from https://www.modelscope.cn to directory: /mnt/workspace/.cache/modelscope/models/Qwen/Qwen3-8B\n"
     ]
    }
   ],
   "source": [
    "from modelscope import snapshot_download\n",
    "\n",
    "# model_dir = snapshot_download('Qwen/Qwen3-8B')\n",
    "model_dir = '/data/joey.wang/llm/models/pretrained/llm/Qwen3-8B'\n",
    "adapter_dir = 'output/vx-xxx/checkpoint-xxx'\n",
    "\n",
    "\n",
    "def infer_hf():\n",
    "    from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "    from peft import PeftModel\n",
    "\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        model_dir, torch_dtype='auto', device_map='auto', trust_remote_code=True)\n",
    "    model = PeftModel.from_pretrained(model, adapter_dir)\n",
    "\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_dir, trust_remote_code=True)\n",
    "\n",
    "    messages = [{\n",
    "        'role': 'user',\n",
    "        'content': 'who are you?'\n",
    "    }]\n",
    "    text = tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
    "    model_inputs = tokenizer([text], return_tensors='pt', add_special_tokens=False).to(model.device)\n",
    "\n",
    "    generated_ids = model.generate(**model_inputs, max_new_tokens=512, do_sample=False)\n",
    "    generated_ids = [\n",
    "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "    ]\n",
    "\n",
    "    response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "    print(f'response: {response}')\n",
    "    return response\n",
    "\n",
    "\n",
    "def infer_swift():\n",
    "    from swift.llm import InferRequest, RequestConfig, PtEngine\n",
    "\n",
    "    engine = PtEngine(model_dir, adapters=[adapter_dir])\n",
    "\n",
    "    messages = [{\n",
    "        'role': 'user',\n",
    "        'content': 'who are you?'\n",
    "    }]\n",
    "    request_config = RequestConfig(max_tokens=512, temperature=0)\n",
    "    resp_list = engine.infer([InferRequest(messages=messages)], request_config=request_config)\n",
    "    response = resp_list[0].choices[0].message.content\n",
    "    print(f'response: {response}')\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T00:11:51.029531Z",
     "iopub.status.busy": "2025-04-30T00:11:51.029188Z",
     "iopub.status.idle": "2025-04-30T00:12:22.474325Z",
     "shell.execute_reply": "2025-04-30T00:12:22.473827Z",
     "shell.execute_reply.started": "2025-04-30T00:11:51.029514Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 08:11:56.755620: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-30 08:11:56.795927: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-30 08:11:57.940359: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-30 08:11:59,178] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "60a817ca08cd44ac9799f8f8c72a0b76",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:631: UserWarning: `do_sample` is set to `False`. However, `temperature` is set to `0.6` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `temperature`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:636: UserWarning: `do_sample` is set to `False`. However, `top_p` is set to `0.95` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_p`.\n",
      "  warnings.warn(\n",
      "/usr/local/lib/python3.11/site-packages/transformers/generation/configuration_utils.py:653: UserWarning: `do_sample` is set to `False`. However, `top_k` is set to `20` -- this flag is only used in sample-based generation modes. You should set `do_sample=True` or unset `top_k`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: <think>\n",
      "Okay, the user asked, \"who are you?\" I need to respond appropriately. First, I should introduce myself as an AI assistant developed by swift. I should mention my name, swift-robot, and my purpose to assist with information and tasks. I should keep the response friendly and open-ended, inviting them to ask questions. I should avoid any technical jargon and make sure the tone is approachable. Let me check if there's anything else I need to include. Maybe a brief mention of my capabilities, like answering questions, providing information, and helping with tasks. Also, I should make sure to use proper grammar and punctuation. Alright, that should cover it.\n",
      "</think>\n",
      "\n",
      "Hello! I am swift-robot, an artificial intelligence assistant developed by swift. My purpose is to assist you in answering questions, providing information, and helping with various tasks. If you have any questions or need assistance, feel free to ask me!\n"
     ]
    }
   ],
   "source": [
    "response = infer_hf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-04-30T00:13:29.000432Z",
     "iopub.status.busy": "2025-04-30T00:13:29.000308Z",
     "iopub.status.idle": "2025-04-30T00:13:59.203913Z",
     "shell.execute_reply": "2025-04-30T00:13:59.203413Z",
     "shell.execute_reply.started": "2025-04-30T00:13:29.000419Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-30 08:13:33.244006: I tensorflow/core/util/port.cc:113] oneDNN custom operations are on. You may see slightly different numerical results due to floating-point round-off errors from different computation orders. To turn them off, set the environment variable `TF_ENABLE_ONEDNN_OPTS=0`.\n",
      "2025-04-30 08:13:33.288002: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 AVX512F AVX512_VNNI FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-30 08:13:34.342324: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-04-30 08:13:34,749] [INFO] [real_accelerator.py:239:get_accelerator] Setting ds_accelerator to cuda (auto detect)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Loading the model using model_dir: /mnt/workspace/.cache/modelscope/models/Qwen/Qwen3-8B\n",
      "[INFO:swift] Setting torch_dtype: torch.bfloat16\n",
      "[INFO:swift] model_kwargs: {'device_map': 'cuda:0'}\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b972d341df584cad8185d075497532ab",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/5 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO:swift] Loading the model using model_dir: /mnt/workspace/qwen3/output/v0-20250430-065954/checkpoint-102\n",
      "[INFO:swift] Create the default_template for the infer_engine\n",
      "[INFO:swift] Successfully registered `/usr/local/lib/python3.11/site-packages/swift/llm/dataset/data/dataset_info.json`.\n",
      "[INFO:swift] Successfully loaded /mnt/workspace/qwen3/output/v0-20250430-065954/checkpoint-102/args.json.\n",
      "[INFO:swift] default_system: None\n",
      "[INFO:swift] response_prefix: ''\n",
      "[INFO:swift] agent_template: hermes\n",
      "[INFO:swift] max_length: 40960\n",
      "[INFO:swift] norm_bbox: norm1000\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "response: <think>\n",
      "Okay, the user asked, \"who are you?\" I need to respond appropriately. First, I should introduce myself as an AI assistant developed by swift. I should mention my name, swift-robot, and my purpose to assist with information and tasks. I should keep the response friendly and open-ended, inviting them to ask questions. I should avoid any technical jargon and make sure the tone is approachable. Let me check if there's anything else I need to include. Maybe a brief mention of my capabilities, like answering questions, providing information, and helping with tasks. Also, I should make sure to use proper grammar and punctuation. Alright, that should cover it.\n",
      "</think>\n",
      "\n",
      "Hello! I am swift-robot, an artificial intelligence assistant developed by swift. My purpose is to assist you in answering questions, providing information, and helping with various tasks. If you have any questions or need assistance, feel free to ask me!\n"
     ]
    }
   ],
   "source": [
    "response2 = infer_swift()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ä½¿ç”¨CLIè¿›è¡Œ merge loraï¼š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "huggingface/tokenizers: The current process just got forked, after parallelism has already been used. Disabling parallelism to avoid deadlocks...\n",
      "To disable this warning, you can either:\n",
      "\t- Avoid using `tokenizers` before the fork if possible\n",
      "\t- Explicitly set the environment variable TOKENIZERS_PARALLELISM=(true | false)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "run sh: `/usr/local/bin/python /data/joey.wang/llm/ms-swift/swift/cli/export.py --adapters output/vx-xxx/checkpoint-xxx --merge_lora true`\n",
      "[INFO:swift] Successfully registered `/data/joey.wang/llm/ms-swift/swift/llm/dataset/data/dataset_info.json`.\n",
      "Traceback (most recent call last):\n",
      "  File \"/data/joey.wang/llm/ms-swift/swift/cli/export.py\", line 2, in <module>\n",
      "    from swift.llm import export_main\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/data/joey.wang/llm/ms-swift/swift/utils/import_utils.py\", line 94, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/data/joey.wang/llm/ms-swift/swift/utils/import_utils.py\", line 103, in _get_module\n",
      "    return importlib.import_module('.' + module_name, self.__name__)\n",
      "  File \"/usr/local/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/data/joey.wang/llm/ms-swift/swift/llm/export/__init__.py\", line 2, in <module>\n",
      "    from .export import SwiftExport, export_main\n",
      "  File \"/data/joey.wang/llm/ms-swift/swift/llm/export/export.py\", line 7, in <module>\n",
      "    from .merge_lora import merge_lora\n",
      "  File \"/data/joey.wang/llm/ms-swift/swift/llm/export/merge_lora.py\", line 4, in <module>\n",
      "    from swift.llm import ExportArguments, prepare_model_template, save_checkpoint\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/data/joey.wang/llm/ms-swift/swift/utils/import_utils.py\", line 95, in __getattr__\n",
      "    value = getattr(module, name)\n",
      "  File \"/data/joey.wang/llm/ms-swift/swift/utils/import_utils.py\", line 94, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/data/joey.wang/llm/ms-swift/swift/utils/import_utils.py\", line 103, in _get_module\n",
      "    return importlib.import_module('.' + module_name, self.__name__)\n",
      "  File \"/usr/local/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/data/joey.wang/llm/ms-swift/swift/llm/infer/utils.py\", line 9, in <module>\n",
      "    from swift.tuners import Swift\n",
      "  File \"<frozen importlib._bootstrap>\", line 1075, in _handle_fromlist\n",
      "  File \"/data/joey.wang/llm/ms-swift/swift/utils/import_utils.py\", line 94, in __getattr__\n",
      "    module = self._get_module(self._class_to_module[name])\n",
      "  File \"/data/joey.wang/llm/ms-swift/swift/utils/import_utils.py\", line 103, in _get_module\n",
      "    return importlib.import_module('.' + module_name, self.__name__)\n",
      "  File \"/usr/local/lib/python3.10/importlib/__init__.py\", line 126, in import_module\n",
      "    return _bootstrap._gcd_import(name[level:], package, level)\n",
      "  File \"/data/joey.wang/llm/ms-swift/swift/tuners/base.py\", line 25, in <module>\n",
      "    from .mapping import SwiftTuners\n",
      "  File \"/data/joey.wang/llm/ms-swift/swift/tuners/mapping.py\", line 5, in <module>\n",
      "    from .longlora.longlora import LongLoRA, LongLoRAConfig\n",
      "  File \"/data/joey.wang/llm/ms-swift/swift/tuners/longlora/longlora.py\", line 9, in <module>\n",
      "    from swift.tuners.lora import lora_state_dict, mark_lora_as_trainable\n",
      "  File \"/data/joey.wang/llm/ms-swift/swift/tuners/lora.py\", line 12, in <module>\n",
      "    from .lora_layers import *  # noqa\n",
      "  File \"/data/joey.wang/llm/ms-swift/swift/tuners/lora_layers.py\", line 77, in <module>\n",
      "    import bitsandbytes as bnb\n",
      "  File \"/usr/local/lib/python3.10/site-packages/bitsandbytes/__init__.py\", line 6, in <module>\n",
      "    from . import research, utils\n",
      "  File \"/usr/local/lib/python3.10/site-packages/bitsandbytes/research/__init__.py\", line 2, in <module>\n",
      "    from .autograd._functions import (\n",
      "  File \"/usr/local/lib/python3.10/site-packages/bitsandbytes/research/autograd/_functions.py\", line 8, in <module>\n",
      "    from bitsandbytes.autograd._functions import GlobalOutlierPooler, MatmulLtState\n",
      "  File \"/usr/local/lib/python3.10/site-packages/bitsandbytes/autograd/__init__.py\", line 1, in <module>\n",
      "    from ._functions import get_inverse_transform_indices, undo_layout\n",
      "  File \"/usr/local/lib/python3.10/site-packages/bitsandbytes/autograd/_functions.py\", line 10, in <module>\n",
      "    import bitsandbytes.functional as F\n",
      "  File \"/usr/local/lib/python3.10/site-packages/bitsandbytes/functional.py\", line 17, in <module>\n",
      "    from .cextension import lib\n",
      "  File \"/usr/local/lib/python3.10/site-packages/bitsandbytes/cextension.py\", line 85, in <module>\n",
      "    lib = get_native_library()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/bitsandbytes/cextension.py\", line 64, in get_native_library\n",
      "    cuda_specs = get_cuda_specs()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/bitsandbytes/cuda_specs.py\", line 38, in get_cuda_specs\n",
      "    highest_compute_capability=(get_compute_capabilities()[-1]),\n",
      "  File \"/usr/local/lib/python3.10/site-packages/bitsandbytes/cuda_specs.py\", line 19, in get_compute_capabilities\n",
      "    return sorted(torch.cuda.get_device_capability(torch.cuda.device(i)) for i in range(torch.cuda.device_count()))\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 904, in device_count\n",
      "    nvml_count = _device_count_amdsmi() if torch.version.hip else _device_count_nvml()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 860, in _device_count_nvml\n",
      "    raw_cnt = _raw_device_count_nvml()\n",
      "  File \"/usr/local/lib/python3.10/site-packages/torch/cuda/__init__.py\", line 714, in _raw_device_count_nvml\n",
      "    rc = nvml_h.nvmlInit()\n",
      "KeyboardInterrupt\n",
      "^C\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/subprocess.py\", line 1209, in wait\n",
      "    return self._wait(timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/subprocess.py\", line 1959, in _wait\n",
      "    (pid, sts) = self._try_wait(0)\n",
      "  File \"/usr/local/lib/python3.10/subprocess.py\", line 1917, in _try_wait\n",
      "    (pid, sts) = os.waitpid(self.pid, wait_flags)\n",
      "KeyboardInterrupt\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/bin/swift\", line 33, in <module>\n",
      "    sys.exit(load_entry_point('ms-swift', 'console_scripts', 'swift')())\n",
      "  File \"/data/joey.wang/llm/ms-swift/swift/cli/main.py\", line 70, in cli_main\n",
      "    result = subprocess.run(args)\n",
      "  File \"/usr/local/lib/python3.10/subprocess.py\", line 505, in run\n",
      "    stdout, stderr = process.communicate(input, timeout=timeout)\n",
      "  File \"/usr/local/lib/python3.10/subprocess.py\", line 1146, in communicate\n",
      "    self.wait()\n",
      "  File \"/usr/local/lib/python3.10/subprocess.py\", line 1222, in wait\n",
      "    self._wait(timeout=sigint_timeout)\n",
      "  File \"/usr/local/lib/python3.10/subprocess.py\", line 1953, in _wait\n",
      "    time.sleep(delay)\n",
      "KeyboardInterrupt\n"
     ]
    }
   ],
   "source": [
    "!CUDA_VISIBLE_DEVICES=0 \\\n",
    "swift export \\\n",
    "    --adapters output/vx-xxx/checkpoint-xxx \\\n",
    "    --merge_lora true"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Qwen3-8B GRPO\n",
    "ä»¥Qwen3-8Bä¸ºä¾‹ï¼Œä¸‹é¢ä½¿ç”¨swiftæ¡†æ¶å¯¹è¿›è¡ŒGRPOè®­ç»ƒã€‚æ›´å¤šå…³äºGRPOï¼Œå¯ä»¥å‚è€ƒGRPOæ–‡æ¡£ï¼šhttps://swift.readthedocs.io/zh-cn/latest/Instruction/GRPO.html\n",
    "\n",
    "ä½¿ç”¨AI-MO/NuminaMath-TIRä½œä¸ºæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨accuracyå‡½æ•°è®¡ç®—æ¨¡å‹å›ç­”çš„å‡†ç¡®ç‡å¥–åŠ±, è®¡ç®—å¥–åŠ±éœ€è¦å®‰è£…ä»¥ä¸‹ç¯å¢ƒï¼š\n",
    "\n",
    "```bash\n",
    "pip install math_verify==0.5.2\n",
    "```\n",
    "è‡ªå®šä¹‰æ•°æ®é›†æ ¼å¼ä¸SFTç±»ä¼¼ï¼Œå…¶ä¸­assistantéƒ¨åˆ†ä¸å¿…éœ€ã€‚å¦‚æœä½¿ç”¨accuracyå¥–åŠ±ï¼Œåˆ™éœ€è¦solutionåˆ—æ¥è®¡ç®—å‡†ç¡®ç‡ã€‚\n",
    "\n",
    "```jsonl\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a useful and harmless assistant\"}, {\"role\": \"user\", \"content\": \"Tell me tomorrow's weather\"}]}\n",
    "{\"messages\": [{\"role\": \"system\", \"content\": \"You are a useful and harmless math calculator\"}, {\"role\": \"user\", \"content\": \"What is 1 + 1?\"}, {\"role\": \"assistant\", \"content\": \"It equals 2\"}, {\"role\": \"user\", \"content\": \"What about adding 1?\"}]}\n",
    "{\"messages\": [{\"role\": \"user\", \"content\": \"What is your name?\"}]}\n",
    "```\n",
    "\n",
    "ä¹Ÿå¯ä»¥ä½¿ç”¨è‡ªå®šä¹‰çš„å¥–åŠ±å‡½æ•°/å¥–åŠ±æ¨¡å‹è¿›è¡Œè®­ç»ƒï¼Œæ•°æ®é›†ä¸­çš„åˆ—ä¼šä¼ åˆ°å¥–åŠ±å‡½æ•°çš„`**kwargs`ä¸­ï¼Œè‡ªå®šä¹‰å¥–åŠ±å‡½æ•°çš„ä¾‹å­å‚è€ƒï¼šswift/examples/train/grpo/plugin/plugin.py\n",
    "\n",
    "```bash\n",
    "    --external_plugins examples/train/grpo/plugin/plugin.py \\\n",
    "    --reward_funcs external_math_acc external_math_format \\\n",
    "    --reward_model AI-ModelScope/Skywork-Reward-Llama-3.1-8B-v0.2\n",
    "```\n",
    "åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œæˆ‘ä»¬ä½¿ç”¨vLLMæ¥åŠ é€Ÿé‡‡æ ·è¿‡ç¨‹ã€‚è®¾ç½®num_infer_workers=8ï¼Œæˆ‘ä»¬ä¸ºæ¯ä¸ªdeviceéƒ½éƒ¨ç½²ä¸€ä¸ªvLLM engineæ¥åŠ é€Ÿé‡‡æ ·è¿‡ç¨‹ã€‚\n",
    "\n",
    "è®­ç»ƒè„šæœ¬å¦‚ä¸‹ï¼š\n",
    "\n",
    "```bash\n",
    "# 70G*8\n",
    "CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \\\n",
    "NPROC_PER_NODE=8 \\\n",
    "swift rlhf \\\n",
    "    --rlhf_type grpo \\\n",
    "    --model Qwen/Qwen3-8B \\\n",
    "    --train_type full \\\n",
    "    --dataset AI-MO/NuminaMath-TIR \\\n",
    "    --torch_dtype bfloat16 \\\n",
    "    --num_train_epochs 1 \\\n",
    "    --per_device_train_batch_size 2 \\\n",
    "    --per_device_eval_batch_size 2 \\\n",
    "    --learning_rate 1e-6 \\\n",
    "    --save_total_limit 2 \\\n",
    "    --logging_steps 5 \\\n",
    "    --output_dir output \\\n",
    "    --gradient_accumulation_steps 1 \\\n",
    "    --warmup_ratio 0.05 \\\n",
    "    --dataloader_num_workers 4 \\\n",
    "    --max_completion_length 4096 \\\n",
    "    --vllm_max_model_len 8192 \\\n",
    "    --reward_funcs accuracy \\\n",
    "    --num_generations 16 \\\n",
    "    --use_vllm true \\\n",
    "    --vllm_gpu_memory_utilization 0.4 \\\n",
    "    --sleep_level 1 \\\n",
    "    --offload_model true \\\n",
    "    --offload_optimizer true \\\n",
    "    --gc_collect_after_offload true \\\n",
    "    --deepspeed zero3 \\\n",
    "    --num_infer_workers 8 \\\n",
    "    --tensor_parallel_size 1 \\\n",
    "    --temperature 1.0 \\\n",
    "    --top_p 0.85 \\\n",
    "    --report_to wandb \\\n",
    "    --log_completions true \\\n",
    "    --overlong_filter true \n",
    "```\n",
    "\n",
    "### Qwen3-30B-A3B MoE SFTï¼ˆMegatron-SWIFTï¼‰\n",
    "\n",
    "ms-swiftå¼•å…¥äº†Megatronçš„å¹¶è¡ŒæŠ€æœ¯æ¥åŠ é€Ÿå¤§æ¨¡å‹çš„è®­ç»ƒï¼ŒåŒ…æ‹¬æ•°æ®å¹¶è¡Œã€å¼ é‡å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œã€åºåˆ—å¹¶è¡Œï¼Œä¸Šä¸‹æ–‡å¹¶è¡Œï¼Œä¸“å®¶å¹¶è¡Œã€‚æ”¯æŒ**Qwen3**ã€**Qwen3-MoE**ã€Qwen2.5ã€Llama3ã€Deepseek-R1è’¸é¦ç³»ç­‰æ¨¡å‹çš„é¢„è®­ç»ƒå’Œå¾®è°ƒã€‚\n",
    "\n",
    "å¯¹äºç¯å¢ƒå‡†å¤‡ï¼ˆé•œåƒï¼‰å’ŒHFä¸MCoreæ¨¡å‹æƒé‡çš„è½¬æ¢ï¼Œå¯ä»¥å‚è€ƒMegatron-SWIFTè®­ç»ƒæ–‡æ¡£ï¼Œè¿™é‡Œä¸è¿›è¡Œä»‹ç»ï¼šhttps://swift.readthedocs.io/zh-cn/latest/Instruction/Megatron-SWIFT%E8%AE%AD%E7%BB%83.html\n",
    "\n",
    "æˆ‘ä»¬ä½¿ç”¨DLCå¯åŠ¨è®­ç»ƒå‘½ä»¤ï¼Œè®­ç»ƒç¯å¢ƒæ˜¯2æœº8 * 80GiB A800ï¼š\n",
    "\n",
    "æ›´å¤š**å¤šèŠ‚ç‚¹å¯åŠ¨**æ–¹å¼å‚è€ƒï¼šhttps://github.com/modelscope/ms-swift/tree/main/examples/train/multi-node\n",
    "\n",
    "```bash\n",
    "# https://help.aliyun.com/zh/pai/user-guide/general-environment-variables\n",
    "# è¯·ç¡®ä¿ä¸¤ä¸ªèŠ‚ç‚¹çš„ä¿å­˜æƒé‡è·¯å¾„ç›¸åŒ\n",
    "NNODES=$WORLD_SIZE \\\n",
    "NODE_RANK=$RANK \\\n",
    "megatron sft \\\n",
    "    --load Qwen3-30B-A3B-Base-mcore \\\n",
    "    --dataset 'liucong/Chinese-DeepSeek-R1-Distill-data-110k-SFT' \\\n",
    "    --tensor_model_parallel_size 2 \\\n",
    "    --expert_model_parallel_size 8 \\\n",
    "    --moe_grouped_gemm true \\\n",
    "    --moe_shared_expert_overlap true \\\n",
    "    --moe_aux_loss_coeff 0.01 \\\n",
    "    --micro_batch_size 1 \\\n",
    "    --global_batch_size 16 \\\n",
    "    --packing true \\\n",
    "    --recompute_granularity full \\\n",
    "    --recompute_method uniform \\\n",
    "    --recompute_num_layers 1 \\\n",
    "    --train_iters 2000 \\\n",
    "    --eval_iters 50 \\\n",
    "    --finetune true \\\n",
    "    --cross_entropy_loss_fusion true \\\n",
    "    --lr 1e-5 \\\n",
    "    --lr_warmup_iters 100 \\\n",
    "    --min_lr 1e-6 \\\n",
    "    --save megatron_output/Qwen3-30B-A3B-Base \\\n",
    "    --eval_interval 200 \\\n",
    "    --save_interval 200 \\\n",
    "    --max_length 8192 \\\n",
    "    --num_workers 8 \\\n",
    "    --dataset_num_proc 8 \\\n",
    "    --no_save_optim true \\\n",
    "    --no_save_rng true \\\n",
    "    --sequence_parallel true \\\n",
    "    --use_flash_attn true\n",
    "```\n",
    "è®­ç»ƒlosså›¾ï¼ˆéƒ¨åˆ†ï¼‰ï¼š\n",
    "\n",
    "<img width=\"910\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/9fe393aa-8299-4659-aa2f-be5d44f0730b\" />\n",
    "\n",
    "æ•ˆæœæˆªå›¾ï¼š\n",
    "\n",
    "<img width=\"1066\" alt=\"Image\" src=\"https://github.com/user-attachments/assets/1a924130-1954-43e9-9093-b019aeef5949\" />\n",
    "\n",
    "è‡ªå®šä¹‰æ•°æ®é›†æ ¼å¼ä¸`swift sft`ç›¸åŒï¼Œå¯ä»¥åœ¨æœ¬æ–‡ä¸Šæ–¹æ‰¾åˆ°ï¼ŒæŒ‡å®š`--dataset <dataset_path>`å³å¯ã€‚\n",
    "\n",
    "ä½¿ç”¨`megatron sft`å’Œ`swift sft`è¿›è¡ŒQwen3-30B-A3Bæ¨¡å‹å…¨å‚æ•°è®­ç»ƒ**é€Ÿåº¦/æ˜¾å­˜å ç”¨**å¯¹æ¯”å¦‚ä¸‹ï¼š\n",
    "\n",
    "|          | Megatron-LM | DeepSpeed-ZeRO2 | DeepSpeed-ZeRO3 |\n",
    "| -------- | ----------- | --------------- | --------------- |\n",
    "| è®­ç»ƒé€Ÿåº¦ | 9.6s/it     | -               | 91.2s/it        |\n",
    "| æ˜¾å­˜å ç”¨ | 16 * 60GiB  | OOM             | 16 * 80GiB      |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## æ¨¡å‹é‡åŒ–\n",
    "\n",
    "Qwen3-32B-AWQ: https://modelscope.cn/models/swift/Qwen3-32B-AWQ\n",
    "\n",
    "Qwen3-30B-A3B-AWQ: https://modelscope.cn/models/swift/Qwen3-30B-A3B-AWQ"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## é­”æ­å¤§æ¨¡å‹è®­ç»ƒäº¤æµç¾¤\n",
    "\n",
    "\n",
    "<img width=\"250\" alt=\"Image\" src=\"https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/ms_swift/wechat.png\" />\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
