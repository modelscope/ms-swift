"""
æ¨¡å—åŠŸèƒ½
-------
æœ¬æ¨¡å—é›†ä¸­å®šä¹‰å¤šæ¨¡æ€ï¼ˆè§†è§‰/éŸ³é¢‘/è§†é¢‘ï¼‰åœºæ™¯ä¸‹çš„æ•°æ®é›†é¢„å¤„ç†å™¨ä¸æ•°æ®é›†æ³¨å†Œé€»è¾‘ï¼Œ
æ¶µç›–å›¾åƒæè¿°ï¼ˆcaption/VQAï¼‰ã€è§†é¢‘ç†è§£ã€OCRã€è¯­éŸ³è¯†åˆ«ã€å·¥å…·/ä»£ç†ç±»ä»»åŠ¡ç­‰ã€‚é€šè¿‡ `DatasetMeta` ä¸
å„ç±» `Preprocessor`ï¼ˆç»§æ‰¿è‡ª `MessagesPreprocessor`/`ResponsePreprocessor`/`RowPreprocessor`ï¼‰
å°†åŸå§‹æ•°æ®æ ·æœ¬ç»Ÿä¸€è½¬æ¢ä¸ºè®­ç»ƒæ‰€éœ€çš„æ ‡å‡†å­—æ®µï¼Œå¦‚ `messages/query/response/images/audios/videos/tools`ã€‚

å…¸å‹ç”¨æ³•
-------
1. å¯¼å…¥æœ¬æ¨¡å—å³å®Œæˆå¯¹è‹¥å¹²å¤šæ¨¡æ€æ•°æ®é›†çš„æ³¨å†Œï¼›
2. ä¸Šå±‚æ•°æ®åŠ è½½å™¨æŒ‰ `ms_dataset_id/hf_dataset_id/subsets/split` æŸ¥æ‰¾å¹¶æ„å»º `Dataset`ï¼›
3. é¢„å¤„ç†å™¨åœ¨ `prepare_dataset/preprocess` ä¸­ä¸‹è½½åª’ä½“ã€é‡å†™è·¯å¾„ã€æŠ½æ ·ä¸æ¸…æ´—å­—æ®µã€‚

è¯´æ˜ï¼šæœ¬æ–‡ä»¶ä¸ºæ¯ä¸€è¡Œä»£ç æ·»åŠ äº†ä¸­æ–‡æ³¨é‡Šä¸å¿…è¦çš„æ–‡æ¡£æ³¨é‡Šï¼Œä¾¿äºå¿«é€Ÿç†è§£ä¸ç»´æŠ¤ã€‚
"""

# Copyright (c) Alibaba, Inc. and its affiliates.  # ç‰ˆæƒå£°æ˜
import ast  # æŠ½è±¡è¯­æ³•æ ‘è§£æï¼šå®‰å…¨è§£æå­—ç¬¦ä¸²å­—é¢é‡
import os  # è·¯å¾„ä¸æ–‡ä»¶æ“ä½œ
from typing import Any, Dict, Optional  # ç±»å‹æ³¨è§£ï¼šé€šç”¨ã€å­—å…¸ã€å¯é€‰

import numpy as np  # æ•°å€¼å·¥å…·ï¼Œè¿™é‡Œç”¨äºéšæœºé€‰æ‹©ä¸ç´¢å¼•
from datasets import Dataset as HfDataset  # HuggingFace æ ‡å‡†æ•°æ®é›†ç±»å‹
from datasets import IterableDataset as HfIterableDataset  # å¯è¿­ä»£æ•°æ®é›†ç±»å‹ï¼Œç”¨äºæµå¼å¤„ç†
from tqdm import tqdm  # è¿›åº¦æ¡ï¼Œè¿­ä»£å¤§æ•°æ®é›†æ—¶æ˜¾ç¤ºè¿›åº¦

from swift.utils import get_hf_endpoint, use_hf_hub  # å·¥å…·ï¼šåˆ¤æ–­æ˜¯å¦ä½¿ç”¨ HF Hub ä¸è·å–ç«¯ç‚¹
from ..media import MediaResource  # åª’ä½“èµ„æºä¸‹è½½/ç¼“å­˜å·¥å…·
from ..preprocessor import GroundingMixin, MessagesPreprocessor, ResponsePreprocessor, RowPreprocessor  # é¢„å¤„ç†åŸºç±»ä¸ Grounding æ··å…¥
from ..register import DatasetMeta, SubsetDataset, register_dataset  # æ•°æ®é›†å…ƒä¿¡æ¯ã€å­é›†æè¿°ä¸æ³¨å†Œå…¥å£


class ShareGPT4oPreprocessor(MessagesPreprocessor):
    """
    ç±»è¯´æ˜
    -----
    ShareGPT-4o å¤šæ¨¡æ€æ•°æ®é›†çš„é¢„å¤„ç†å™¨ï¼š
    - åœ¨ `prepare_dataset` é˜¶æ®µä¸‹è½½å¹¶å®šä½å›¾åƒæ ¹ç›®å½•ï¼›
    - åœ¨ `preprocess` é˜¶æ®µæ‹¼æ¥ç›¸å¯¹è·¯å¾„ä¸ºç»å¯¹è·¯å¾„ï¼Œå¹¶ç¡®ä¿æ–‡ä»¶å­˜åœ¨ï¼›
    - å°†å•å¼ å›¾ç‰‡è·¯å¾„å°è£…ä¸ºåˆ—è¡¨å½¢å¼ï¼Œç»Ÿä¸€ä¸‹æ¸¸æ¶ˆè´¹æ¥å£ã€‚
    """

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        å¤„ç†å•æ¡æ ·æœ¬ï¼šæ ‡å‡†åŒ–å¹¶é‡å†™å›¾åƒè·¯å¾„ã€‚

        å‚æ•°
        ----
        - row: åŸå§‹æ ·æœ¬å­—å…¸ï¼Œé¢„æœŸåŒ…å« `images` ç›¸å¯¹è·¯å¾„ã€‚

        è¿”å›
        ----
        - Optional[Dict[str, Any]]: æˆåŠŸåˆ™è¿”å›æ ‡å‡†åŒ–åçš„è®°å½•ï¼Œå¤±è´¥ï¼ˆæ— å›¾åƒæˆ–è·¯å¾„ä¸å­˜åœ¨ï¼‰åˆ™è¿”å› Noneã€‚
        """
        row = super().preprocess(row)  # å…ˆåšé€šç”¨å­—æ®µæ ‡å‡†åŒ–/æ˜ å°„
        image = row['images']  # å–å‡ºå›¾åƒç›¸å¯¹è·¯å¾„
        if not image:  # è‹¥ä¸å­˜åœ¨å›¾åƒ
            return  # ç›´æ¥ä¸¢å¼ƒæ ·æœ¬
        image = os.path.join(self.prefix_path, image)  # æ‹¼æ¥æˆç»å¯¹è·¯å¾„
        if not os.path.exists(image):  # è‹¥è·¯å¾„ä¸å­˜åœ¨
            return  # ä¸¢å¼ƒæ ·æœ¬
        row['images'] = [image]  # è§„èŒƒä¸ºå•å…ƒç´ åˆ—è¡¨
        return row  # è¿”å›å¤„ç†åçš„æ ·æœ¬

    def prepare_dataset(self, dataset):
        """
        ä¸‹è½½ ShareGPT-4o æ‰€éœ€çš„å›¾åƒå‹ç¼©åŒ…å¹¶è®¾ç½®å‰ç¼€è·¯å¾„ã€‚
        """
        if not use_hf_hub():  # æ ¹æ®ç¯å¢ƒé€‰æ‹©ä¸‹è½½æº
            url = ('https://www.modelscope.cn/api/v1/datasets/AI-ModelScope/ShareGPT-4o/repo?'
                   'Revision=master&FilePath=images.zip')  # MS æºåœ°å€
        else:
            url = f'{get_hf_endpoint()}/datasets/OpenGVLab/ShareGPT-4o/blob/main/images.zip'  # HF æºåœ°å€
        local_dir = MediaResource.download(url, 'sharegpt_4o_images')  # ä¸‹è½½å¹¶è¿”å›æœ¬åœ°ç›®å½•
        self.prefix_path = os.path.join(local_dir, 'mnt', 'petrelfs', 'wangwenhai', 'workspace_cef', '4o', 'image')  # ç»„è£…å‰ç¼€
        return super().prepare_dataset(dataset)  # ç»§ç»­çˆ¶ç±»å‡†å¤‡æµç¨‹


register_dataset(
    DatasetMeta(  # æ³¨å†Œ ShareGPT-4o æ•°æ®é›†
        ms_dataset_id='AI-ModelScope/ShareGPT-4o',  # MS æ•°æ®é›† ID
        hf_dataset_id='OpenGVLab/ShareGPT-4o',  # HF æ•°æ®é›† ID
        preprocess_func=ShareGPT4oPreprocessor(),  # ç»‘å®šé¢„å¤„ç†å™¨
        subsets=['image_caption'],  # ä½¿ç”¨ image_caption å­é›†
        split=['images'],  # ä½¿ç”¨ images åˆ‡åˆ†å
        tags=['vqa', 'multi-modal'],  # æ ‡ç­¾ï¼šè§†è§‰é—®ç­”/å¤šæ¨¡æ€
    ))


class GPT4vDataset(ResponsePreprocessor):
    """
    ç±»è¯´æ˜
    -----
    GPT-4V å›¾åƒæè¿°æ•°æ®ï¼šç»Ÿä¸€è®¾ç½®æŸ¥è¯¢ä¸ºâ€œå›¾åƒçš„æ ‡é¢˜æ˜¯ä»€ä¹ˆï¼Ÿâ€ï¼Œç„¶åè°ƒç”¨çˆ¶ç±»è¿›è¡Œæ ‡å‡†åŒ–ã€‚
    """

    def preprocess(self, row: Dict[str, Any]) -> Dict[str, Any]:
        """
        è®¾ç½® query å¹¶äº¤ç”±çˆ¶ç±»å¤„ç†ã€‚

        å‚æ•°
        ----
        - row: åŸå§‹è®°å½•ï¼Œè‡³å°‘åŒ…å« `images/caption` ç­‰å­—æ®µï¼ˆé€šè¿‡ columns æ˜ å°„ï¼‰ã€‚

        è¿”å›
        ----
        - Dict[str, Any]: æ ‡å‡†åŒ–æ ·æœ¬ã€‚
        """
        row['query'] = 'What is the caption of this image?'  # ç»Ÿä¸€å›¾åƒæè¿°é—®é¢˜
        return super().preprocess(row)  # æ ‡å‡†åŒ–


register_dataset(
    DatasetMeta(  # æ³¨å†Œ gpt4v-dataset
        ms_dataset_id='swift/gpt4v-dataset',  # MS ID
        hf_dataset_id='laion/gpt4v-dataset',  # HF ID
        preprocess_func=GPT4vDataset(columns={  # åˆ—æ˜ å°„ï¼šé“¾æ¥->imagesï¼Œcaption->response
            'link': 'images',
            'caption': 'response'
        }),
        subsets=['train'],
        split=['train'],  # ä½¿ç”¨ train åˆ’åˆ†
        tags=['en', 'caption', 'multi-modal', 'quality'],  # è‹±æ–‡/å›¾åƒæè¿°/å¤šæ¨¡æ€/é«˜è´¨é‡
        huge_dataset=True,  # æ•°æ®é›†è¾ƒå¤§
    ))

register_dataset(
    DatasetMeta(  # æ³¨å†Œ RLAIF-V è§†è§‰åå¥½æ•°æ®
        ms_dataset_id='swift/RLAIF-V-Dataset',  # MS ID
        hf_dataset_id='openbmb/RLAIF-V-Dataset',  # HF ID
        preprocess_func=ResponsePreprocessor(columns={  # åˆ—æ˜ å°„ï¼šé—®é¢˜/ä¼˜é€‰/åŠ£é€‰
            'question': 'query',
            'chosen': 'response',
            'rejected': 'rejected_response'
        }),
        tags=['rlhf', 'dpo', 'multi-modal', 'en'],  # æ ‡ç­¾ï¼šRLHF/DPO/å¤šæ¨¡æ€/è‹±æ–‡
    ))


class GarbagePreprocessor(ResponsePreprocessor):
    """
    ç±»è¯´æ˜
    -----
    åƒåœ¾åˆ†ç±»å›¾ç‰‡æ•°æ®ï¼šç»Ÿä¸€è®¾ç½®åˆ†ç±»ä»»åŠ¡è¯´æ˜åè¿›è¡Œæ ‡å‡†åŒ–ã€‚
    """

    def preprocess(self, row: Dict[str, Any]) -> Dict[str, Any]:
        """
        è®¾ç½® query å¹¶å§”æ‰˜çˆ¶ç±»å¤„ç†ã€‚
        """
        row['query'] = 'Task: Classify household waste.'  # åˆ†ç±»ä»»åŠ¡æŒ‡ä»¤
        return super().preprocess(row)  # æ ‡å‡†åŒ–


register_dataset(
    DatasetMeta(  # æ³¨å†Œåƒåœ¾åˆ†ç±»æ•°æ®é›†
        ms_dataset_id='tany0699/garbage265',  # MS ID
        preprocess_func=GarbagePreprocessor(columns={  # åˆ—æ˜ å°„ï¼šç±»åˆ«->labelï¼Œæ–‡ä»¶->images
            'category': 'label',
            'image:FILE': 'images'
        }),
        tags=['cls', 'ğŸ”¥', 'multi-modal'],  # æ ‡ç­¾ï¼šåˆ†ç±»/çƒ­é—¨/å¤šæ¨¡æ€
    ))


class SA1BPairedCaptionPreprocessor(RowPreprocessor):
    """
    ç±»è¯´æ˜
    -----
    å¤„ç†æˆå¯¹çš„å›¾åƒ-å…¨å±€æè¿°ï¼šéšæœºæŒ‘é€‰ä¸€ä¸ªä¸­æ–‡æç¤ºä½œä¸º queryï¼Œ`global_caption` ä½œä¸º responseã€‚
    """

    def preprocess(self, row: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ„é€ ä¸¤æ®µå¼å¯¹è¯æ¶ˆæ¯ã€‚
        """
        prompt = ['å›¾ç‰‡ä¸­å±•ç¤ºäº†ä»€ä¹ˆ', 'è®²è¿°ä¸€ä¸‹å›¾ç‰‡ä¸­å†…å®¹', 'å‘Šè¯‰æˆ‘é‡Œé¢æœ‰ä»€ä¹ˆ', 'å›¾ç‰‡å†…å®¹æ˜¯å•¥']  # å€™é€‰ä¸­æ–‡æç¤º
        response = row['global_caption']  # å…¨å±€æè¿°
        query = np.random.choice(prompt)  # éšæœºé€‰æ‹©æç¤º
        return {
            'messages': [{
                'role': 'user',
                'content': query,
            }, {
                'role': 'assistant',
                'content': response,
            }]
        }


register_dataset(
    DatasetMeta(  # æ³¨å†Œ SA1B æˆå¯¹æè¿°
        ms_dataset_id='Tongyi-DataEngine/SA1B-Paired-Captions-Images',  # MS ID
        preprocess_func=SA1BPairedCaptionPreprocessor(columns={  # åˆ—æ˜ å°„ï¼šå¼€æº URL -> images
            'opensource_url': 'images',
        }),
        tags=['zh', 'multi-modal', 'vqa'],  # æ ‡ç­¾ï¼šä¸­æ–‡/å¤šæ¨¡æ€/VQA
    ))


class SA1BDenseCaptionPreprocessor(RowPreprocessor):
    """
    ç±»è¯´æ˜
    -----
    å¤„ç† SA1B å¯†é›†æè¿°ï¼šä» `cap_seg` ä¸­è§£æ `global_caption`ï¼Œå¹¶éšæœºæŒ‘é€‰ä¸­æ–‡æç¤ºä½œä¸º queryã€‚
    """
    column_mapping = {
        'url': 'images',  # å°† url åˆ—æ˜ å°„ä¸º images
    }

    def preprocess(self, row: Dict[str, Any]) -> Dict[str, Any]:
        """
        è§£æå¯†é›†æè¿°å¹¶æ„é€ ä¸¤æ®µå¼æ¶ˆæ¯ã€‚
        """
        prompt = ['å›¾ç‰‡ä¸­å±•ç¤ºäº†ä»€ä¹ˆ', 'è®²è¿°ä¸€ä¸‹å›¾ç‰‡ä¸­å†…å®¹', 'å‘Šè¯‰æˆ‘é‡Œé¢æœ‰ä»€ä¹ˆ', 'å›¾ç‰‡å†…å®¹æ˜¯å•¥']  # å€™é€‰æç¤º
        response = ast.literal_eval(row['cap_seg'])  # å®‰å…¨è§£æå­—ç¬¦ä¸²ä¸ºå­—å…¸
        response = response.get('global_caption')  # æå–å…¨å±€æè¿°
        query = np.random.choice(prompt)  # éšæœºæç¤º
        return {
            'messages': [{
                'role': 'user',
                'content': query,
            }, {
                'role': 'assistant',
                'content': response,
            }]
        }


register_dataset(
    DatasetMeta(  # æ³¨å†Œ SA1B å¯†é›†æè¿°
        ms_dataset_id='Tongyi-DataEngine/SA1B-Dense-Caption',  # MS ID
        preprocess_func=SA1BDenseCaptionPreprocessor(columns={  # åˆ—æ˜ å°„ï¼šurl -> images
            'url': 'images',
        }),
        tags=['zh', 'multi-modal', 'vqa'],  # æ ‡ç­¾
        huge_dataset=True,  # æ•°æ®é‡è¾ƒå¤§
    ))


class COCO2014Preprocess(ResponsePreprocessor):
    """
    ç±»è¯´æ˜
    -----
    COCO-2014 å›¾åƒæè¿°ï¼š
    - å»é™¤ `caption` ä¸­ `&&` ä¹‹åçš„å™ªå£°éƒ¨åˆ†ï¼›
    - ç»Ÿä¸€æŸ¥è¯¢ä¸ºè‹±æ–‡æè¿°è¯·æ±‚ã€‚
    """

    def preprocess(self, row: Dict[str, Any]) -> Dict[str, Any]:
        """
        æ¸…æ´— caption å¹¶æ ‡å‡†åŒ–æ ·æœ¬ã€‚
        """
        caption = row['caption']  # åŸå§‹æè¿°
        if '&&' in caption:  # è‹¥åŒ…å«åˆ†éš”å™ªå£°
            caption = caption.split('&&')[0]  # ä»…ä¿ç•™å‰åŠæ®µ
        row['query'] = 'please describe the image.'  # ç»Ÿä¸€æŸ¥è¯¢æŒ‡ä»¤
        row['response'] = caption  # è®¾ç½®å“åº”

        return super().preprocess(row)  # æ ‡å‡†åŒ–


register_dataset(
    DatasetMeta(  # æ³¨å†Œ COCO-2014 caption æ•°æ®
        ms_dataset_id='modelscope/coco_2014_caption',
        preprocess_func=COCO2014Preprocess(),  # ç»‘å®šé¢„å¤„ç†å™¨
        subsets=[  # å­é›†æ˜ å°„åˆ°åº•å±‚ split
            SubsetDataset('train', 'coco_2014_caption', ['train']),
            SubsetDataset('validation', 'coco_2014_caption', ['validation']),
        ],
        tags=['chat', 'multi-modal', 'vision', 'ğŸ”¥'],  # æ ‡ç­¾
    ))


class MantisPreprocessor(MessagesPreprocessor):
    """
    ç±»è¯´æ˜
    -----
    Mantis-Instruct å¤šæ¨¡æ€æ•°æ®é¢„å¤„ç†ï¼š
    - prepare é˜¶æ®µæŒ‰å­é›†ä¸‹è½½å›¾ç‰‡å‹ç¼©åŒ…å¹¶ç¼“å­˜ï¼›
    - preprocess é˜¶æ®µå°†ç›¸å¯¹è·¯å¾„è½¬ä¸ºæœ¬åœ°ç»å¯¹è·¯å¾„ï¼Œè¿‡æ»¤ç¼ºå¤±æ–‡ä»¶ã€‚
    """

    def __init__(self, *, subset: str, columns: Optional[Dict[str, str]] = None) -> None:
        """
        åˆå§‹åŒ–æ‰€éœ€å­é›†ä¸åˆ—æ˜ å°„ã€‚
        """
        self.subset = subset  # è®°å½•å½“å‰å­é›†å
        super().__init__(columns=columns)  # åˆå§‹åŒ–åŸºç±»

    def prepare_dataset(self, dataset: HfDataset) -> HfDataset:
        """
        ä¸‹è½½å½“å‰å­é›†çš„å›¾ç‰‡å‹ç¼©åŒ…å¹¶è®¾ç½®æœ¬åœ°ç›®å½•ã€‚
        """
        if not use_hf_hub():  # æ ¹æ®ç¯å¢ƒé€‰æ‹©æ•°æ®æº
            url = (f'https://www.modelscope.cn/api/v1/datasets/swift/Mantis-Instruct/repo?Revision='
                   f'master&FilePath={self.subset}/train_images.zip')  # noqa  # MS æº
        else:
            url = (f'{get_hf_endpoint()}/datasets/TIGER-Lab/Mantis-Instruct/'
                   f'resolve/main/{self.subset}/train_images.zip')  # HF æº
        self.local_dir = MediaResource.download(url, f'mantis_{self.subset}')  # ä¸‹è½½å¹¶ç¼“å­˜
        return super().prepare_dataset(dataset)  # ç»§ç»­çˆ¶ç±»æµç¨‹

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        å°†ç›¸å¯¹è·¯å¾„åˆ—è¡¨è½¬ä¸ºç»å¯¹è·¯å¾„å¹¶è¿‡æ»¤ç¼ºå¤±é¡¹ã€‚
        """
        images = [os.path.join(self.local_dir, p['path']) for p in row['images']]  # æ‹¼æ¥ç»å¯¹è·¯å¾„
        if not all([os.path.exists(d) for d in images]):  # è‹¥å­˜åœ¨ç¼ºå¤±æ–‡ä»¶
            images = []  # æ¸…ç©ºä»¥è§¦å‘ä¸¢å¼ƒ

        if not images:  # æ— æœ‰æ•ˆå›¾ç‰‡
            return  # ä¸¢å¼ƒæ ·æœ¬
        row['images'] = images  # å†™å›å›¾ç‰‡åˆ—è¡¨
        return super().preprocess(row)  # æ ‡å‡†åŒ–


mantis_subsets_name = [  # Mantis å­é›†æšä¸¾
    'birds-to-words', 'chartqa', 'coinstruct', 'contrastive_caption', 'docvqa', 'dreamsim', 'dvqa', 'iconqa',
    'imagecode', 'llava_665k_multi', 'lrv_multi', 'multi_vqa', 'nextqa', 'nlvr2', 'spot-the-diff', 'star',
    'visual_story_telling'
]

_mantis_subsets = []  # æ”¶é›†æ„é€ å¥½çš„å­é›†æè¿°
for subset in mantis_subsets_name:
    _subset = SubsetDataset(subset=subset, split=['train'], preprocess_func=MantisPreprocessor(subset=subset))  # æ„é€ å­é›†
    _mantis_subsets.append(_subset)  # åŠ å…¥åˆ—è¡¨

register_dataset(
    DatasetMeta(  # æ³¨å†Œ Mantis-Instruct
        ms_dataset_id='swift/Mantis-Instruct',  # MS ID
        subsets=_mantis_subsets,  # ä½¿ç”¨ä¸Šé¢æ„é€ çš„å…¨éƒ¨å­é›†
        tags=['chat', 'multi-modal', 'vision'],  # æ ‡ç­¾
    ))


class LLaVADataPreprocessor(MessagesPreprocessor):
    """
    ç±»è¯´æ˜
    -----
    ä¸º LLaVA æ•°æ®ä¿®å¤å›¾ç‰‡ç»å¯¹è·¯å¾„ï¼š
    - prepare_dataset é˜¶æ®µä¸‹è½½å„å›¾ç‰‡èµ„æºæ ¹ç›®å½•ï¼›
    - preprocess é˜¶æ®µæ ¹æ®ç›¸å¯¹è·¯å¾„å½’å¹¶åˆ°æœ¬åœ°ç¼“å­˜è·¯å¾„ï¼Œæ ¡éªŒå­˜åœ¨æ€§ã€‚
    """

    def prepare_dataset(self, dataset):
        """
        ä¸‹è½½æˆ–å®šä½æ‰€éœ€åª’ä½“æ ¹ç›®å½•å¹¶ç¼“å­˜åˆ° `self.all_folders`ã€‚
        """
        self.all_folders = {}  # å­˜æ”¾å„åª’ä½“ç±»å‹çš„æœ¬åœ°æ ¹è·¯å¾„
        for media_type in ['coco', 'gqa', 'ocr_vqa', 'textvqa', 'VG_100K', 'VG_100K_2']:
            self.all_folders[media_type] = MediaResource.download(media_type)  # ä¸‹è½½æˆ–å®šä½ç¼“å­˜
        return super().prepare_dataset(dataset)  # ç»§ç»­çˆ¶ç±»æµç¨‹

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        å°†ç›¸å¯¹è·¯å¾„å½’å¹¶æˆæœ¬åœ°ç»å¯¹è·¯å¾„å¹¶æ ¡éªŒå­˜åœ¨æ€§ã€‚
        """
        if not row['images']:  # æ— å›¾ç‰‡ç›´æ¥è·³è¿‡
            return
        row = super().preprocess(row)  # æ ‡å‡†åŒ–å­—æ®µ
        images = [p['path'] for p in row['images']]  # æå–ç›¸å¯¹è·¯å¾„åˆ—è¡¨
        new_images = []  # å­˜æ”¾ä¿®å¤åçš„ç»å¯¹è·¯å¾„
        for image in images:  # é’ˆå¯¹ä¸åŒå‰ç¼€é€‰æ‹©å¯¹åº”æ ¹ç›®å½•
            if 'coco/' in image:
                image = os.path.join(self.all_folders['coco'], image.replace('coco/', ''))
            elif 'gqa/' in image:
                image = os.path.join(self.all_folders['gqa'], image.replace('gqa/', ''))
            elif 'ocr_vqa/' in image:
                image = os.path.join(self.all_folders['ocr_vqa'], image)
            elif 'textvqa/' in image:
                image = os.path.join(self.all_folders['textvqa'], image.replace('textvqa/', ''))
            elif 'VG_100K/' in image:
                image = os.path.join(self.all_folders['VG_100K'], image.replace('vg/', ''))
            elif 'VG_100K_2/' in image:
                image = os.path.join(self.all_folders['VG_100K_2'], image.replace('vg/', ''))
            new_images.append(image)  # è®°å½•ä¿®å¤è·¯å¾„
        if all(os.path.exists(image) for image in new_images):  # ç¡®ä¿å…¨éƒ¨å­˜åœ¨
            row['images'] = new_images  # å†™å›ä¿®å¤åçš„è·¯å¾„
        else:
            return {'images': None}  # ä»»ä½•ç¼ºå¤±åˆ™æ ‡è®°ä¸ºæ— å›¾åƒï¼Œä¾›ä¸Šæ¸¸è¿‡æ»¤
        return row  # è¿”å›


register_dataset(
    DatasetMeta(
        ms_dataset_id='swift/llava-data',
        hf_dataset_id='TIGER-Lab/llava-data',
        subsets=['llava_instruct'],
        preprocess_func=LLaVADataPreprocessor(),
        tags=['sft', 'multi-modal', 'quality'],
    ))


class PixelProsePreprocessor(RowPreprocessor):

    def preprocess(self, row: Dict[str, Any]) -> Dict[str, Any]:
        caption_prompt = [
            'Give the description of this image.', 'Describe this picture', 'What is the proper title of this image?'
        ]
        vlm_caption = row['vlm_caption']
        if vlm_caption.startswith('This image displays:'):
            vlm_caption = vlm_caption[len('This image displays:'):].strip()
        return {
            'messages': [{
                'role': 'user',
                'content': np.random.choice(caption_prompt)
            }, {
                'role': 'assistant',
                'content': vlm_caption
            }],
            'images': row['url']
        }


register_dataset(
    DatasetMeta(
        ms_dataset_id='swift/pixelprose',
        hf_dataset_id='tomg-group-umd/pixelprose',
        preprocess_func=PixelProsePreprocessor(),
        split=['train', 'cc12m', 'commonpool', 'redcaps'],
        tags=['caption', 'multi-modal', 'vision'],
        huge_dataset=True,
    ))


class AIShell1Preprocessor(ResponsePreprocessor):
    """
    ç±»è¯´æ˜
    -----
    AIShell-1 è¯­éŸ³è¯†åˆ«ï¼ˆASRï¼‰æ•°æ®ï¼šç»Ÿä¸€è®¾ç½® query ä¸ºâ€œè¯­éŸ³è½¬æ–‡æœ¬â€ï¼Œå¹¶å»æ‰æ–‡æœ¬ä¸­çš„ç©ºæ ¼ã€‚
    """

    def preprocess(self, row: Dict[str, Any]) -> Dict[str, Any]:
        """
        è®¾ç½®ç»Ÿä¸€æŸ¥è¯¢å¹¶æ¸…ç†æ ‡ç­¾æ–‡æœ¬ç©ºæ ¼ã€‚
        """
        row['query'] = 'è¯­éŸ³è½¬æ–‡æœ¬'
        row['response'] = row['Text:LABEL'].replace(' ', '')  # å»é™¤ç©ºæ ¼
        return super().preprocess(row)  # æ ‡å‡†åŒ–


register_dataset(
    DatasetMeta(
        ms_dataset_id='speech_asr/speech_asr_aishell1_trainsets',
        subsets=[
            SubsetDataset('train', split=['train']),
            SubsetDataset('validation', split=['validation']),
            SubsetDataset('test', split=['test']),
        ],
        preprocess_func=AIShell1Preprocessor(columns={'Audio:FILE': 'audios'}),
        tags=['chat', 'multi-modal', 'audio'],
    ))


class EmoSchemaPreprocessor(ResponsePreprocessor):
    """
    ç±»è¯´æ˜
    -----
    EgoSchema è§†é¢‘å¤šé€‰é¢˜ï¼š
    - prepare_dataset é˜¶æ®µä¸‹è½½å¤šä¸ªåˆ†ç‰‡å‹ç¼©åŒ…å¹¶è®°å½•å¯ç”¨ mp4 æ–‡ä»¶é›†åˆï¼›
    - preprocess é˜¶æ®µå°†å¤šé€‰é¡¹æ‹¼æ¥è¿› queryï¼Œè½¬æ¢å“åº”ä¸º A-E é€‰é¡¹å­—æ¯ï¼Œå¹¶ç»‘å®šæœ¬åœ°è§†é¢‘è·¯å¾„ã€‚
    """

    def prepare_dataset(self, dataset: HfDataset) -> HfDataset:
        """
        ä¸‹è½½ 5 ä¸ªè§†é¢‘åˆ†ç‰‡å¹¶æ”¶é›†æœ¬åœ° mp4 åç§°é›†åˆã€‚
        """
        for i in range(1, 6):  # 5 ä¸ªåˆ†ç‰‡
            if not use_hf_hub():
                url = f'https://modelscope.cn/datasets/AI-ModelScope/egoschema/resolve/master/videos_chunked_0{i}.zip'
            else:
                url = f'{get_hf_endpoint()}/datasets/lmms-lab/egoschema/resolve/main/videos_chunked_0{i}.zip'
            local_dir = MediaResource.download(url, 'egoschema')  # ä¸‹è½½

        self.local_dir = os.path.join(local_dir, 'videos')  # è§†é¢‘ç›®å½•
        self.mp4_set = [file[:-4] for file in os.listdir(self.local_dir) if file.endswith('mp4')]  # æ”¶é›†å¯ç”¨è§†é¢‘å
        return super().prepare_dataset(dataset)  # çˆ¶ç±»

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        ç»‘å®šæœ¬åœ°è§†é¢‘è·¯å¾„å¹¶å°†é€‰é¡¹æ•°å­—æ˜ å°„ä¸ºå­—æ¯ã€‚
        """
        if row['video_idx'] not in self.mp4_set:  # è§†é¢‘ä¸å­˜åœ¨åˆ™è·³è¿‡
            return None
        transfer_to_option = {  # æ•°å­—åˆ°å­—æ¯é€‰é¡¹æ˜ å°„
            '0': 'A',
            '1': 'B',
            '2': 'C',
            '3': 'D',
            '4': 'E',
        }
        row = {
            'query': row['query'] + '\n' + '\n'.join(row['option']),  # æ‹¼æ¥å¤šé€‰é¡¹åˆ°æŸ¥è¯¢
            'response': transfer_to_option[row['response']],  # å“åº”æ˜ å°„ä¸ºå­—æ¯
            'videos': [os.path.join(self.local_dir, f"{row['video_idx']}.mp4")],  # æœ¬åœ°è§†é¢‘è·¯å¾„
        }
        return super().preprocess(row)  # æ ‡å‡†åŒ–


class EmoSchemaClsPreprocessor(EmoSchemaPreprocessor):
    """
    ç±»è¯´æ˜
    -----
    EgoSchema åˆ†ç±»ç‰ˆæœ¬ï¼šä¸ `EmoSchemaPreprocessor` ç±»ä¼¼ï¼Œä½†è¾“å‡ºæ•°å€¼æ ‡ç­¾ `label`ã€‚
    """

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        ç»‘å®šæœ¬åœ°è§†é¢‘è·¯å¾„å¹¶è¾“å‡ºæ•´å‹æ ‡ç­¾ã€‚
        """
        if row['video_idx'] not in self.mp4_set:
            return None
        row = {
            'query': row['query'] + '\n' + '\n'.join(row['option']),  # æ‹¼æ¥é€‰é¡¹
            'label': int(row['response']),  # è½¬æ¢ä¸ºæ•´å‹æ ‡ç­¾
            'videos': [os.path.join(self.local_dir, f"{row['video_idx']}.mp4")],  # æœ¬åœ°è§†é¢‘è·¯å¾„
        }
        return ResponsePreprocessor.preprocess(self, row)  # æ ‡å‡†åŒ–


register_dataset(
    DatasetMeta(
        ms_dataset_id='AI-ModelScope/egoschema',
        hf_dataset_id='lmms-lab/egoschema',
        subsets=[
            SubsetDataset('default', 'Subset', preprocess_func=EmoSchemaPreprocessor()),
            SubsetDataset('cls', 'Subset', preprocess_func=EmoSchemaClsPreprocessor())
        ],
        split=['test'],
        tags=['chat', 'multi-modal', 'video'],
    ))


def _generate_url_list(_url, _range):
    lst = []
    for i in range(1, (_range + 1)):
        lst.append(_url.replace('{}', str(i)))
    return lst


class LLaVAVideo178KPreprocessor(MessagesPreprocessor):
    """
    ç±»è¯´æ˜
    -----
    LLaVA-Video-178K è§†é¢‘æ•°æ®ï¼š
    - æ ¹æ®å­é›†è‡ªåŠ¨ä¸‹è½½åˆ†ç‰‡å‹ç¼©åŒ…ï¼›
    - åœ¨æ ·æœ¬çº§å°†ç›¸å¯¹è§†é¢‘æ–‡ä»¶åè½¬æ¢ä¸ºæœ¬åœ°ç»å¯¹è·¯å¾„ã€‚
    """

    def __init__(self, *, subset: str, columns: Optional[Dict[str, str]] = None) -> None:
        """
        è®°å½•å­é›†åå¹¶åˆå§‹åŒ–åˆ—æ˜ å°„ã€‚
        """
        self.subset = subset  # å½“å‰å­é›†
        super().__init__(columns=columns)  # åˆå§‹åŒ–åŸºç±»

    url_prefix = 'https://www.modelscope.cn/datasets/lmms-lab/LLaVA-Video-178K/resolve/master/'  # é»˜è®¤ MS ç«¯ç‚¹
    if use_hf_hub():  # è‹¥ä½¿ç”¨ HF Hubï¼Œåˆ™åˆ‡æ¢ç«¯ç‚¹
        url_prefix = f'{get_hf_endpoint()}/datasets/lmms-lab/LLaVA-Video-178K/resolve/main/'

    video_resources = {
        '0_30_s_academic_v0_1':
        _generate_url_list(
            url_prefix + '0_30_s_academic_v0_1/0_30_s_academic_v0_1_videos_{}.tar.gz',
            8,
        ),
        '0_30_s_youtube_v0_1':
        _generate_url_list(
            url_prefix + '0_30_s_youtube_v0_1/0_30_s_youtube_v0_1_videos_{}.tar.gz',
            19,
        ),
        '1_2_m_academic_v0_1':
        _generate_url_list(
            url_prefix + '1_2_m_academic_v0_1/1_2_m_academic_v0_1_videos_{}.tar.gz',
            14,
        ),
        '1_2_m_youtube_v0_1':
        _generate_url_list(
            url_prefix + '1_2_m_youtube_v0_1/1_2_m_youtube_v0_1_videos_{}.tar.gz',
            50,
        ),
        '2_3_m_academic_v0_1':
        _generate_url_list(
            url_prefix + '2_3_m_academic_v0_1/2_3_m_academic_v0_1_videos_{}.tar.gz',
            18,
        ),
        '2_3_m_youtube_v0_1':
        _generate_url_list(
            url_prefix + '2_3_m_youtube_v0_1/2_3_m_youtube_v0_1_videos_{}.tar.gz',
            98,
        ),
        '30_60_s_academic_v0_1':
        _generate_url_list(
            url_prefix + '30_60_s_academic_v0_1/30_60_s_academic_v0_1_videos_{}.tar.gz',
            10,
        ),
        '30_60_s_youtube_v0_1':
        _generate_url_list(
            url_prefix + '30_60_s_youtube_v0_1/30_60_s_youtube_v0_1_videos_{}.tar.gz',
            13,
        ),
    }

    def prepare_dataset(self, dataset: HfDataset) -> HfDataset:
        """
        ä¸‹è½½é€‰å®šå­é›†çš„è§†é¢‘åˆ†ç‰‡å¹¶è®¾ç½®æœ¬åœ°ç›®å½•ã€‚
        """
        urls = self.video_resources[self.subset]  # å–å‡ºè¯¥å­é›†æ‰€æœ‰åˆ†ç‰‡ URL åˆ—è¡¨
        self.local_dir = MediaResource.download(urls, f'llava_video_178k_{self.subset}', file_type='sharded')  # åˆ†ç‰‡ä¸‹è½½
        return super().prepare_dataset(dataset)  # ç»§ç»­çˆ¶ç±»

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        å°†è§†é¢‘æ–‡ä»¶åè½¬æ¢ä¸ºæœ¬åœ°ç»å¯¹è·¯å¾„å¹¶æ ¡éªŒã€‚
        """
        file_path = os.path.join(self.local_dir, f"{row['videos']}")  # æ‹¼æ¥æœ¬åœ°è·¯å¾„
        if not os.path.exists(file_path):  # æ–‡ä»¶ç¼ºå¤±åˆ™ä¸¢å¼ƒ
            return None
        return super().preprocess({'messages': row['messages'], 'videos': file_path})  # æ ‡å‡†åŒ–


llava_video_subsets = []
for subset in [
        '0_30_s_academic_v0_1',
        '0_30_s_youtube_v0_1',
        '1_2_m_academic_v0_1',
        '1_2_m_youtube_v0_1',
        '2_3_m_academic_v0_1',
        '2_3_m_youtube_v0_1',
        '30_60_s_academic_v0_1',
        '30_60_s_youtube_v0_1',
]:
    subset = SubsetDataset(
        subset=subset,
        split=['caption', 'open_ended', 'multi_choice'],
        preprocess_func=LLaVAVideo178KPreprocessor(subset=subset),
    )
    llava_video_subsets.append(subset)

register_dataset(
    DatasetMeta(
        hf_dataset_id='lmms-lab/LLaVA-Video-178K', subsets=llava_video_subsets, tags=['chat', 'multi-modal', 'video']))


class MovieChat1KPreprocessor(ResponsePreprocessor):
    """
    ç±»è¯´æ˜
    -----
    MovieChat-1K æµ‹è¯•æ•°æ®ï¼š
    - prepare_dataset é˜¶æ®µä¸‹è½½æµ‹è¯•é›†è§†é¢‘æ–‡ä»¶é›†åˆï¼›
    - preprocess é˜¶æ®µå°†ç›¸å¯¹è·¯å¾„è½¬ä¸ºæœ¬åœ°è·¯å¾„ï¼Œå¹¶é€‰å–åˆé€‚çš„ `query/response` å­—æ®µã€‚
    """

    def prepare_dataset(self, dataset: HfDataset) -> HfDataset:
        """
        ä¸‹è½½æµ‹è¯•é›†ä¸­æ¶‰åŠçš„ mp4 æ–‡ä»¶é›†åˆã€‚
        """
        mp4_set = [f'{i}.mp4' for i in range(1, 10)] + \
                  [f'{i}.mp4' for i in range(201, 240)] + \
                  [f'AWA-{i}.mp4' for i in range(1, 10)] + \
                  [f'AWB-{i}.mp4' for i in range(1, 16)] + \
                  [f'AWC-{i}.mp4' for i in range(1, 11)] + \
                  [f'AWD-{i}.mp4' for i in range(1, 8)] + \
                  [f'AWE-{i}.mp4' for i in range(1, 7)] + \
                  [f'AWG-{i}.mp4' for i in range(1, 12)] + \
                  [f'AWH-{i}.mp4' for i in range(1, 8)] + \
                  [f'BWA-{i}.mp4' for i in range(1, 7)] + \
                  [f'BWB-{i}.mp4' for i in range(1, 7)] + \
                  [f'BWD-{i}.mp4' for i in range(1, 6)] + \
                  [f'BWE-{i}.mp4' for i in range(1, 6)] + \
                  [f'BWG-{i}.mp4' for i in range(1, 6)] + \
                  [f'BWH-{i}.mp4' for i in range(1, 6)] + \
                  [f'TFS-{i}.mp4' for i in range(1, 13)] + \
                  [f'UWA-{i}.mp4' for i in range(1, 5)] + ['UWA-6.mp4']  # æ„é€ éœ€è¦ä¸‹è½½çš„æ–‡ä»¶åé›†åˆ
        for file in mp4_set:
            if not use_hf_hub():
                url = f'https://modelscope.cn/datasets/AI-ModelScope/MovieChat-1K-test/resolve/master/videos/{file}'
            else:
                url = f'{get_hf_endpoint()}/datasets/Enxin/MovieChat-1K-test/resolve/main/videos/{file}'
            self.local_dir = MediaResource.download(url, 'moviechat_1k_test', file_type='file')  # ä¸‹è½½åˆ°æœ¬åœ°ç¼“å­˜
        return super().prepare_dataset(dataset)

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        å°†ç›¸å¯¹è§†é¢‘è·¯å¾„è½¬æ¢ä¸ºæœ¬åœ°è·¯å¾„å¹¶é€‰æ‹©é—®é¢˜/ç­”æ¡ˆä½œä¸º query/responseã€‚
        """
        file_path = os.path.join(self.local_dir, f"{row['info']['video_path']}")  # æ‹¼æ¥æœ¬åœ°è·¯å¾„
        if not os.path.exists(file_path):  # ç¼ºå¤±åˆ™è·³è¿‡
            return None
        return super().preprocess({
            'query': row['global'][0]['question'],  # ä½¿ç”¨å…¨å±€é—®é¢˜
            'response': row['global'][0]['answer'],  # ä½¿ç”¨å…¨å±€ç­”æ¡ˆ
            'videos': file_path,  # æœ¬åœ°è§†é¢‘è·¯å¾„
        })


register_dataset(
    DatasetMeta(
        ms_dataset_id='AI-ModelScope/MovieChat-1K-test',
        hf_dataset_id='Enxin/MovieChat-1K-test',
        preprocess_func=MovieChat1KPreprocessor(),
        split=['train'],
        tags=['chat', 'multi-modal', 'video']))


class VideoChatGPTPreprocessor(ResponsePreprocessor):
    """
    ç±»è¯´æ˜
    -----
    VideoChatGPT æµ‹è¯•æ•°æ®ï¼š
    - prepare_dataset é˜¶æ®µä¸‹è½½æµ‹è¯•è§†é¢‘ï¼›
    - preprocess é˜¶æ®µç­›é€‰ `.mp4` å¹¶æ ¹æ®å¤šä¸ªå¯èƒ½å­—æ®µé€‰å–æœ‰æ•ˆçš„ queryã€‚
    """

    def prepare_dataset(self, dataset: HfDataset) -> HfDataset:
        """
        ä¸‹è½½ VideoChatGPT æµ‹è¯•è§†é¢‘å¹¶è®¾ç½®æœ¬åœ°ç›®å½•ã€‚
        """
        if not use_hf_hub():  # é€‰æ‹© MS æˆ– HF æº
            url = 'https://modelscope.cn/datasets/swift/VideoChatGPT/resolve/master/videos.zip'
        else:
            url = f'{get_hf_endpoint()}/datasets/lmms-lab/VideoChatGPT/resolve/main/videos.zip'
        local_dir = MediaResource.download(url, 'video_chatgpt')  # ä¸‹è½½è§†é¢‘å‹ç¼©åŒ…
        self.local_dir = os.path.join(local_dir, 'Test_Videos')  # æŒ‡å‘è§£å‹åçš„æµ‹è¯•è§†é¢‘ç›®å½•
        return super().prepare_dataset(dataset)  # çˆ¶ç±»æµç¨‹

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        ä»…ä¿ç•™å­˜åœ¨äºæœ¬åœ°çš„ mp4 è§†é¢‘ï¼Œå¹¶ä»å¤šä¸ªå­—æ®µä¸­é€‰æ‹©æœ‰æ•ˆ queryã€‚
        """
        # only `.mp4`  # ä»…å¤„ç† mp4
        mp4_set = [file[:-4] for file in os.listdir(self.local_dir) if file.endswith('mp4')]  # å¯ç”¨è§†é¢‘é›†åˆ
        if row['video_name'] not in mp4_set:  # ä¸åœ¨é›†åˆå†…åˆ™è·³è¿‡
            return
        row['videos'] = os.path.join(self.local_dir, f"{row['video_name']}.mp4")  # æ‹¼æ¥è§†é¢‘è·¯å¾„
        for key in ['query', 'question_1', 'question_2']:  # ä¾æ¬¡å°è¯•å¤šä¸ªå­—æ®µ
            query = row.get(key)
            if query is None or query == 'None':  # å¿½ç•¥æ— æ•ˆå­—ç¬¦ä¸²
                continue
            row['query'] = query  # ä½¿ç”¨è¯¥å­—æ®µä½œä¸ºæŸ¥è¯¢
            return super().preprocess(row)  # æ ‡å‡†åŒ–å¹¶è¿”å›


register_dataset(
    DatasetMeta(
        ms_dataset_id='swift/VideoChatGPT',
        hf_dataset_id='lmms-lab/VideoChatGPT',
        subsets=['Generic', 'Temporal', 'Consistency'],
        preprocess_func=VideoChatGPTPreprocessor(),
        split=['test'],
        tags=['chat', 'multi-modal', 'video', 'ğŸ”¥'],
    ))


def preprocess_mind2web(dataset, **kwargs):

    def preprocess_row(row: Dict[str, Any]) -> Dict[str, Any]:
        raw_html = row['cleaned_html']
        screenshot = row['screenshot']
        row['screenshot'] = MediaResource.safe_save(screenshot, row['action_uid'] + '.jpg', 'mind2web')
        action = row['target_action_reprs']
        actions = action.split('->')
        row['query'] = f'The snapshot of screen:<image>\nThe html source code:{raw_html}\n'
        action = actions[-1]
        where = actions[0] if len(actions) > 1 else ''
        what = ''
        if ':' in action:
            action, what = action[:action.find(':')], action[action.find(':') + 1:]
        row['response'] = f'Action: {action.strip()}\nAction Input: {where.strip()}{"," + what.strip()}'
        return row

    conversations = []
    tools = [{
        'function': {
            'name': 'CLICK',
            'desc': 'Choose and click an element in the web page',
            'parameter': [{
                'element': 'string, the element in the web page to click'
            }]
        }
    }, {
        'function': {
            'name':
            'TYPE',
            'desc':
            'Input some text into a web element like <input> or <textbox>',
            'parameter': [{
                'element': 'string, the element in the web page to input to',
                'content': 'string, what content to input into the textbox element'
            }]
        }
    }, {
        'function': {
            'name':
            'SELECT',
            'desc':
            'Select an element from a combobox',
            'parameter': [{
                'element': 'string, the combobox or dropdown in the web page on which the select happens',
                'content': 'string, which choices to choose'
            }]
        }
    }]

    def history_to_messages(history):
        messages = []
        for h in history:
            messages.append({'role': 'user', 'content': h[0]})
            messages.append({'role': 'assistant', 'content': h[1]})
        return messages

    if isinstance(dataset, HfIterableDataset):

        def generate_example(dataset):
            history = []
            images = []
            for row in dataset:
                target_action_index = row['target_action_index']
                row = preprocess_row(row)
                query = row['query']
                if target_action_index == '0':
                    if history:
                        yield {'messages': history_to_messages(history), 'images': images, 'tools': tools}
                        images = []
                        history = []
                    query = query + '\n' + row['confirmed_task']
                history.append([query, row['response']])
                images.append(row['screenshot'])

            if history:
                yield {'messages': history_to_messages(history), 'images': images, 'tools': tools}

        return HfIterableDataset.from_generator(generate_example, gen_kwargs={'dataset': dataset})

    history = []
    images = []
    for row in tqdm(dataset):
        target_action_index = row['target_action_index']
        row = preprocess_row(row)
        query = row['query']
        if target_action_index == '0':
            if history:
                conversations.append({'messages': history_to_messages(history), 'images': images, 'tools': tools})
                images = []
                history = []
            query = query + '\n' + row['confirmed_task']
        history.append([query, row['response']])
        images.append(row['screenshot'])

    if history:
        conversations.append({'messages': history_to_messages(history), 'images': images, 'tools': tools})

    return HfDataset.from_list(conversations)


register_dataset(
    DatasetMeta(
        ms_dataset_id='swift/Multimodal-Mind2Web',
        hf_dataset_id='osunlp/Multimodal-Mind2Web',
        preprocess_func=preprocess_mind2web,
        tags=['agent', 'multi-modal']))

register_dataset(
    DatasetMeta(
        ms_dataset_id='AI-ModelScope/M3IT',
        subsets=[
            'coco', 'vqa-v2', 'shapes', 'shapes-rephrased', 'coco-goi-rephrased', 'snli-ve', 'snli-ve-rephrased',
            'okvqa', 'a-okvqa', 'viquae', 'textcap', 'docvqa', 'science-qa', 'imagenet', 'imagenet-open-ended',
            'imagenet-rephrased', 'coco-goi', 'clevr', 'clevr-rephrased', 'nlvr', 'coco-itm', 'coco-itm-rephrased',
            'vsr', 'vsr-rephrased', 'mocheg', 'mocheg-rephrased', 'coco-text', 'fm-iqa', 'activitynet-qa', 'msrvtt',
            'ss', 'coco-cn', 'refcoco', 'refcoco-rephrased', 'multi30k', 'image-paragraph-captioning', 'visual-dialog',
            'visual-dialog-rephrased', 'iqa', 'vcr', 'visual-mrc', 'ivqa', 'msrvtt-qa', 'msvd-qa', 'gqa', 'text-vqa',
            'ocr-vqa', 'st-vqa', 'flickr8k-cn'
        ],
        preprocess_func=ResponsePreprocessor(columns={
            'instruction': 'system',
            'inputs': 'query',
            'image_base64_str': 'images',
            'outputs': 'response'
        }),
        split=['train'],
        huge_dataset=True,
        tags=['chat', 'multi-modal', 'vision']))


class ShareGPT4VPreprocessor(MessagesPreprocessor):
    """
    ç±»è¯´æ˜
    -----
    ShareGPT4V å›¾åƒæ•°æ®ï¼š
    - prepare_dataset é˜¶æ®µæŒ‰é…ç½®åä¸‹è½½æ‰€éœ€åª’ä½“æ ¹ç›®å½•ï¼›
    - preprocess é˜¶æ®µæ ¹æ®è·¯å¾„å‰ç¼€é€‰æ‹©å¯¹åº”æ ¹ç›®å½•å¹¶é‡å†™ä¸ºæœ¬åœ°ç»å¯¹è·¯å¾„ã€‚
    """

    def prepare_dataset(self, dataset):
        """
        æ ¹æ®æ•°æ®é›†é…ç½®åä¸‹è½½å¿…è¦çš„å›¾ç‰‡æºç›®å½•ã€‚
        """
        split = ['ShareGPT4V', 'ShareGPT4V-PT'] if dataset.config_name is None else dataset.config_name  # é€‰æ‹©å­é…ç½®
        IMAGE_DATASET_REQUIREMENTS = {  # å­é…ç½®å¯¹åº”çš„åª’ä½“éœ€æ±‚
            'ShareGPT4V': ['coco', 'sam', 'llava', 'wikiart', 'share_textvqa', 'web-celebrity', 'web-landmark'],
            'ShareGPT4V-PT': ['coco', 'sam', 'llava']
        }

        if isinstance(split, str):  # ç»Ÿä¸€ä¸ºåˆ—è¡¨
            split = [split]
        self.all_folders = {}  # åª’ä½“æ ¹ç›®å½•æ˜ å°„
        for sp in split:  # éå†å­é…ç½®
            for media_type in IMAGE_DATASET_REQUIREMENTS[sp]:  # ä¸‹è½½æ‰€éœ€åª’ä½“
                self.all_folders[media_type] = MediaResource.download(media_type)
        return super().prepare_dataset(dataset)  # çˆ¶ç±»æµç¨‹

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        æ ¹æ®è·¯å¾„å‰ç¼€æ˜ å°„åˆ°æœ¬åœ°æ ¹ç›®å½•ï¼Œè®¾ç½® `images` å­—æ®µã€‚
        """
        image = row['image']  # å–åŸå§‹å›¾ç‰‡ç›¸å¯¹è·¯å¾„
        row.update(super().preprocess(row))  # çˆ¶ç±»æ ‡å‡†åŒ–
        if 'coco/' in image:
            image = os.path.join(self.all_folders['coco'], image.replace('coco/', ''))
        elif 'sam/' in image:
            image = os.path.join(self.all_folders['sam'], image.replace('sam/images/', ''))
        elif 'llava/' in image:
            image = os.path.join(self.all_folders['llava'], image.replace('llava/llava_pretrain/images/', ''))
        elif 'wikiart/' in image:
            image = os.path.join(self.all_folders['wikiart'], image.replace('wikiart/images/', 'data/wikiart/images/'))
        elif 'share_textvqa/' in image:
            image = os.path.join(self.all_folders['share_textvqa'],
                                 image.replace('share_textvqa/images/', 'data/share_textvqa/images/'))
        elif 'web-celebrity/' in image:
            image = os.path.join(self.all_folders['web-celebrity'],
                                 image.replace('web-celebrity/images/', 'data/web-celebrity/images/'))
        elif 'web-landmark/' in image:
            image = os.path.join(self.all_folders['web-landmark'],
                                 image.replace('web-landmark/images/', 'data/web-landmark/images/'))
        if os.path.exists(image):  # æ–‡ä»¶å­˜åœ¨åˆ™è®¾ç½® images
            row['images'] = image
        else:
            return  # ä¸¢å¼ƒç¼ºå¤±æ ·æœ¬
        return row  # è¿”å›å¤„ç†ç»“æœ


register_dataset(
    DatasetMeta(
        ms_dataset_id='AI-ModelScope/ShareGPT4V',
        subsets=['ShareGPT4V', 'ShareGPT4V-PT'],
        preprocess_func=ShareGPT4VPreprocessor(),
        huge_dataset=True,
        tags=['chat', 'multi-modal', 'vision']))


class TextCapsPreprocessor(ResponsePreprocessor):
    """
    ç±»è¯´æ˜
    -----
    TextCaps å›¾åƒæ–‡å­—æè¿°ï¼šç»Ÿä¸€queryï¼Œè¿‡æ»¤ä¸å­˜åœ¨çš„å›¾ç‰‡è·¯å¾„ã€‚
    """

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        ç»Ÿä¸€ query å¹¶æ ¡éªŒå›¾ç‰‡è·¯å¾„å­˜åœ¨ã€‚
        """
        row['query'] = 'What is the caption of this image?'
        if not os.path.exists(row['images']['path']):  # å›¾ç‰‡ç¼ºå¤±ç›´æ¥è·³è¿‡
            return None
        return super().preprocess(row)  # æ ‡å‡†åŒ–


class TextCapsEmbPreprocessor(ResponsePreprocessor):
    """
    ç±»è¯´æ˜
    -----
    TextCaps Embedding ç‰ˆæœ¬ï¼šä¸æä¾› queryï¼Œä»…ç”¨äºå›¾åƒ-æ–‡æœ¬åµŒå…¥ä»»åŠ¡ï¼Œä»éœ€æ ¡éªŒå›¾ç‰‡å­˜åœ¨ã€‚
    """

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        æ„é€ ç©º query å¹¶æ ¡éªŒå›¾ç‰‡è·¯å¾„å­˜åœ¨ã€‚
        """
        row['query'] = ''
        if not os.path.exists(row['images']['path']):
            return None
        return super().preprocess(row)


register_dataset(
    DatasetMeta(
        ms_dataset_id='swift/TextCaps',
        hf_dataset_id='HuggingFaceM4/TextCaps',
        subsets=[
            SubsetDataset(
                name='default',
                preprocess_func=TextCapsPreprocessor(columns={'reference_strs': 'response'}),
                split=['train', 'validation'],
            ),
            SubsetDataset(
                name='emb',
                preprocess_func=TextCapsEmbPreprocessor(columns={'reference_strs': 'response'}),
                split=['train', 'validation'],
            ),
        ],
        huge_dataset=True,
        tags=['multi-modal', 'en', 'caption', 'quality']))


class RefCOCOPreprocessor(ResponsePreprocessor, GroundingMixin):  # ç»“åˆå“åº”å¼é¢„å¤„ç†ä¸ Grounding æ··å…¥
    """
    ç±»è¯´æ˜
    -----
    RefCOCO/RefCOCOg ç­‰æ•°æ®çš„é¢„å¤„ç†å™¨ï¼š
    - æ”¯æŒä¸¤ç±»ä»»åŠ¡ï¼š'caption' ä¸ 'grounding'ï¼ˆé€šè¿‡ `task_type` æ§åˆ¶ï¼‰ï¼›
    - åœ¨ `preprocess` ä¸­ç»„ç»‡ `objects`/`images` å¹¶æ„é€ æç¤º (query, response)ã€‚
    """
    task_type = 'caption'  # é»˜è®¤ä»»åŠ¡ç±»å‹ä¸º captionï¼Œå¯é€šè¿‡æ„é€ å‡½æ•°è¦†ç›–

    def __init__(self, task_type, **kwargs):
        """
        æŒ‡å®šä»»åŠ¡ç±»å‹ï¼ˆcaption/groundingï¼‰ï¼Œå¹¶åˆå§‹åŒ–çˆ¶ç±»ã€‚
        """
        self.task_type = task_type  # è®°å½•å…·ä½“ä»»åŠ¡ç±»å‹
        super().__init__(**kwargs)  # åˆå§‹åŒ–çˆ¶ç±»

    def prepare_dataset(self, dataset):
        """\
        ä¸‹è½½ COCO2014 èµ„æºå¹¶è®¾ç½®ç¼“å­˜ç›®å½•ã€‚
        """
        self.cache_dir = MediaResource.download(
            'https://www.modelscope.cn/api/v1/datasets/we_dont_produce_water/'
            'coco_res/repo?Revision=master&FilePath=coco_2014.zip', 'coco2014')  # ä¸‹è½½å¹¶ç¼“å­˜
        return dataset  # ä¸æ”¹å˜åŸå§‹ dataset

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """\
        æ„é€  grounding/caption ä»»åŠ¡æ‰€éœ€çš„ query/response ä¸ objectsã€‚
        """
        caption = row['captions'][0]  # å–ç¬¬ä¸€æ¡æè¿°
        bbox = row['bbox']  # è¾¹æ¡†åæ ‡
        image_path = os.path.join(self.cache_dir, row['image_path'].replace('coco/train2014', 'train2014'))  # ä¿®å¤è·¯å¾„
        if not os.path.exists(image_path):  # ç¼ºå¤±åˆ™è·³è¿‡
            return

        for i in range(len(bbox)):  # å½’ä¸€åŒ–ä¸ºæ•´æ•°åƒç´ 
            bbox[i] = round(float(bbox[i]))
        res = {}  # å¾…è¿”å›çš„è®°å½•

        objects = {
            'ref': [caption],  # å‚ç…§æ–‡æœ¬
            'bbox': [bbox],  # å¯¹åº”è¾¹æ¡†
        }
        res['query'], res['response'] = self.construct_grounding_prompt()  # ç”±æ··å…¥ç±»æ„é€ æç¤º
        res['images'] = [image_path]  # å›¾ç‰‡è·¯å¾„
        res['objects'] = objects  # ç›®æ ‡å¯¹è±¡
        return super().preprocess(res)  # æ ‡å‡†åŒ–


register_dataset(
    DatasetMeta(
        ms_dataset_id='swift/refcoco',
        hf_dataset_id='jxu124/refcoco',
        subsets=[
            SubsetDataset(
                name='caption',
                preprocess_func=RefCOCOPreprocessor('caption'),
            ),
            SubsetDataset(
                name='grounding',
                preprocess_func=RefCOCOPreprocessor('grounding'),
            )
        ],
        split=['train', 'validation'],
        tags=['multi-modal', 'en', 'grounding']))

register_dataset(
    DatasetMeta(
        ms_dataset_id='swift/refcocog',
        hf_dataset_id='jxu124/refcocog',
        subsets=[
            SubsetDataset(
                name='caption',
                preprocess_func=RefCOCOPreprocessor('caption'),
            ),
            SubsetDataset(
                name='grounding',
                preprocess_func=RefCOCOPreprocessor('grounding'),
            )
        ],
        split=['train', 'validation'],
        tags=['multi-modal', 'en', 'grounding']))

register_dataset(
    DatasetMeta(
        ms_dataset_id='swift/lnqa',
        hf_dataset_id='vikhyatk/lnqa',
        preprocess_func=MessagesPreprocessor(user_role='question', assistant_role='answer'),
        split=['train', 'validation'],
        huge_dataset=True,
        tags=['multi-modal', 'en', 'ocr-vqa', 'quality']))


class LLaVAInstructPreprocessor(MessagesPreprocessor):

    def prepare_dataset(self, dataset):
        self.all_folders = {}
        for media_type in ['coco', 'gqa', 'ocr_vqa', 'textvqa', 'VG_100K', 'VG_100K_2']:
            self.all_folders[media_type] = MediaResource.download(media_type)
        return super().prepare_dataset(dataset)

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        image = row['images']
        if 'coco/' in image:
            image = os.path.join(self.all_folders['coco'], image.replace('coco/', ''))
        elif 'gqa/' in image:
            image = os.path.join(self.all_folders['gqa'], image.replace('gqa/', ''))
        elif 'ocr_vqa/' in image:
            image = os.path.join(self.all_folders['ocr_vqa'], image)
        elif 'textvqa/' in image:
            image = os.path.join(self.all_folders['textvqa'], image.replace('textvqa/', ''))
        elif 'VG_100K/' in image:
            image = os.path.join(self.all_folders['VG_100K'], image.replace('vg/', ''))
        elif 'VG_100K_2/' in image:
            image = os.path.join(self.all_folders['VG_100K_2'], image.replace('vg/', ''))
        if os.path.exists(image):
            row['images'] = image
        else:
            return

        return super().preprocess(row)


register_dataset(
    DatasetMeta(
        ms_dataset_id='AI-ModelScope/LLaVA-Instruct-150K',
        ms_revision='d5db3806e395c60496630a206c336932e85a2d00',
        preprocess_func=LLaVAInstructPreprocessor(),
        split=['train'],
        tags=['chat', 'multi-modal', 'vision']))


class LLaVAPretrainPreprocessor(MessagesPreprocessor):
    """
    ç±»è¯´æ˜
    -----
    LLaVA é¢„è®­ç»ƒæ•°æ®é¢„å¤„ç†ï¼š
    - prepare_dataset é˜¶æ®µä¸‹è½½å›¾ç‰‡å‹ç¼©åŒ…å¹¶è®°å½•æœ¬åœ°ç›®å½•ï¼›
    - preprocess é˜¶æ®µå°†ç›¸å¯¹è·¯å¾„æ˜ å°„ä¸ºæœ¬åœ°ç»å¯¹è·¯å¾„ï¼Œä»…è¿”å›æœ‰æ•ˆæ ·æœ¬ã€‚
    """

    def prepare_dataset(self, dataset):
        if not use_hf_hub():  # æ ¹æ®ç¯å¢ƒå†³å®šä¸‹è½½æºï¼ˆMS/HFï¼‰
            url = ('https://www.modelscope.cn/api/v1/datasets/AI-ModelScope/LLaVA-Pretrain/repo?'
                   'Revision=master&FilePath=images.zip')  # MS æº
        else:
            url = f'{get_hf_endpoint()}/datasets/liuhaotian/LLaVA-Pretrain/resolve/main/images.zip'  # HF æº
        self.media_dir = MediaResource.download(
            url,
            # noqa
            'llava_pretrain')  # ä¸‹è½½å¹¶è¿”å›æœ¬åœ°ç¼“å­˜ç›®å½•
        return super().prepare_dataset(dataset)  # ç»§ç»­çˆ¶ç±»æµç¨‹

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        å°†ç›¸å¯¹å›¾ç‰‡è·¯å¾„æ˜ å°„ä¸ºæœ¬åœ°ç»å¯¹è·¯å¾„ï¼Œä»…å½“æ–‡ä»¶å­˜åœ¨æ—¶è¿”å›æœ‰æ•ˆè®°å½•ã€‚

        è¿”å›
        ----
        - Optional[Dict[str, Any]]: {'images': æœ¬åœ°è·¯å¾„} æˆ– None ä»¥ä¸¢å¼ƒæ— æ•ˆæ ·æœ¬ã€‚
        """
        row.update(super().preprocess(row))  # å…ˆæ ‡å‡†åŒ–åŸå§‹è¡Œ
        if row['image']:  # å­˜åœ¨å›¾ç‰‡å­—æ®µ
            file_path = os.path.join(self.media_dir, row['image'])  # æ‹¼æ¥ç»å¯¹è·¯å¾„
            if os.path.exists(file_path):  # æ–‡ä»¶å­˜åœ¨
                return {'images': file_path}  # è¿”å›ä»…å« images çš„è®°å½•
            else:
                return  # æ–‡ä»¶ç¼ºå¤±ä¸¢å¼ƒ
        else:
            return  # æ— å›¾ç‰‡å­—æ®µä¸¢å¼ƒ


register_dataset(  # æ³¨å†Œ LLaVA é¢„è®­ç»ƒæ•°æ®é›†
    DatasetMeta(
        ms_dataset_id='AI-ModelScope/LLaVA-Pretrain',  # MS æ•°æ®é›† ID
        ms_revision='e3a3f0bfaad05e90e46745152a32bf944e0f4a63',  # å›ºå®šç‰ˆæœ¬å·ï¼Œç¡®ä¿æ•°æ®ä¸€è‡´æ€§
        hf_dataset_id='liuhaotian/LLaVA-Pretrain',  # HF æ•°æ®é›† ID
        preprocess_func=LLaVAPretrainPreprocessor(),  # ç»‘å®šé¢„å¤„ç†å™¨
        huge_dataset=True,  # æ•°æ®é›†ä½“é‡è¾ƒå¤§
        tags=['chat', 'multi-modal', 'quality']))  # æ ‡ç­¾ï¼šå¯¹è¯/å¤šæ¨¡æ€/è´¨é‡

register_dataset(  # æ³¨å†Œ Midefics åŒ»å­¦ VQA æ•°æ®é›†
    DatasetMeta(
        ms_dataset_id='swift/MideficsDataset',  # MS ID
        hf_dataset_id='WinterSchool/MideficsDataset',  # HF ID
        preprocess_func=MessagesPreprocessor(inner_key='data', user_role='question', assistant_role='answer'),  # æŒ‡å®šæ¶ˆæ¯é”®ä¸è§’è‰²
        tags=['medical', 'en', 'vqa']))  # æ ‡ç­¾ï¼šåŒ»å­¦/è‹±æ–‡/VQA

register_dataset(  # æ³¨å†Œ OK-VQA è®­ç»ƒé›†
    DatasetMeta(
        ms_dataset_id='swift/OK-VQA_train',  # MS ID
        hf_dataset_id='Multimodal-Fatima/OK-VQA_train',  # HF ID
        preprocess_func=ResponsePreprocessor(),  # ä½¿ç”¨å“åº”å¼é¢„å¤„ç†
        tags=['multi-modal', 'en', 'vqa', 'quality']))  # æ ‡ç­¾

register_dataset(  # æ³¨å†Œ A-OKVQA æ•°æ®é›†
    DatasetMeta(
        ms_dataset_id='swift/A-OKVQA',  # MS ID
        hf_dataset_id='HuggingFaceM4/A-OKVQA',  # HF ID
        split=['train', 'validation'],  # è®­ç»ƒ/éªŒè¯åˆ’åˆ†
        preprocess_func=ResponsePreprocessor(columns={'rationales': 'response'}),  # å°† rationales ä½œä¸ºå“åº”
        tags=['multi-modal', 'en', 'vqa', 'quality']))  # æ ‡ç­¾


class OcrvqaPreprocessor(RowPreprocessor):
    """
    ç±»è¯´æ˜
    -----
    OCR-VQA ä»»åŠ¡ï¼šä» `questions/answers` éšæœºé€‰æ‹©åŒä¸€ç´¢å¼•æ„æˆé—®ç­”å¯¹ã€‚
    """

    def preprocess(self, row: Dict[str, Any]) -> Dict[str, Any]:
        """
        éšæœºé‡‡æ ·ä¸€ä¸ªé—®ç­”å¯¹å¹¶æ„é€ ä¸¤æ®µå¼æ¶ˆæ¯ã€‚
        """
        idx = np.random.choice(range(len(row['questions'])))  # éšæœºç´¢å¼•
        query = row['questions'][idx]  # å¯¹åº”é—®é¢˜
        response = row['answers'][idx]  # å¯¹åº”ç­”æ¡ˆ
        return {
            'messages': [{
                'role': 'user',
                'content': query
            }, {
                'role': 'assistant',
                'content': response
            }],
        }


register_dataset(  # æ³¨å†Œ OCR-VQA æ•°æ®é›†
    DatasetMeta(
        ms_dataset_id='swift/OCR-VQA',  # MS ID
        hf_dataset_id='howard-hou/OCR-VQA',  # HF ID
        split=['train', 'validation'],  # è®­ç»ƒ/éªŒè¯åˆ’åˆ†
        preprocess_func=OcrvqaPreprocessor(),  # ç»‘å®š OCR-VQA é¢„å¤„ç†å™¨
        tags=['multi-modal', 'en', 'ocr-vqa']))  # æ ‡ç­¾


class ScienceQAPreprocessor(RowPreprocessor):
    """
    ç±»è¯´æ˜
    -----
    ScienceQAï¼šå°†æ¨ç†è¿‡ç¨‹ä¸æœ€ç»ˆç­”æ¡ˆåˆå¹¶ä¸ºå“åº”ï¼Œé—®é¢˜ä½œä¸ºæŸ¥è¯¢ã€‚
    """

    def preprocess(self, row: Dict[str, Any]) -> Dict[str, Any]:
        """
        åˆå¹¶è§£æè¿‡ç¨‹ä¸æœ€ç»ˆç­”æ¡ˆä¸ºå“åº”æ–‡æœ¬ã€‚
        """
        query = row['question']  # åŸå§‹é—®é¢˜
        response = row['choices'][row['answer']]  # é€‰é¡¹ä¸­çš„æœ€ç»ˆç­”æ¡ˆ
        solution = row['solution']  # æ¨ç†è¿‡ç¨‹
        response = f'{solution}\nSo the final answer is: {response}'  # æ‹¼æ¥
        return {'messages': [{'role': 'user', 'content': query}, {'role': 'assistant', 'content': response}]}


register_dataset(  # æ³¨å†Œ ScienceQA æ•°æ®é›†
    DatasetMeta(
        ms_dataset_id='swift/ScienceQA',  # MS ID
        hf_dataset_id='derek-thomas/ScienceQA',  # HF ID
        split=['train', 'validation'],  # è®­ç»ƒ/éªŒè¯åˆ’åˆ†
        preprocess_func=ScienceQAPreprocessor(),  # ç»‘å®š ScienceQA é¢„å¤„ç†å™¨
        tags=['multi-modal', 'science', 'vqa', 'quality']))  # æ ‡ç­¾


class GritPreprocessor(RowPreprocessor, GroundingMixin):
    """
    ç±»è¯´æ˜
    -----
    GRIT æ•°æ®é¢„å¤„ç†å™¨ï¼šæ ¹æ® `ref_exps`ï¼ˆæŒ‡ä»£è¡¨è¾¾çš„èµ·æ­¢ä½ç½®ä¸ bboxï¼‰
    - åˆ‡åˆ† `caption` ä¸­å¯¹åº”çš„å¯¹è±¡çŸ­è¯­ï¼Œæ„é€  `objects.ref`ï¼›
    - æå–å½’ä¸€åŒ–åçš„ bboxï¼Œæ„é€  `objects.bbox`ï¼›
    - æ£€æµ‹åŒºé—´æ˜¯å¦é‡å ï¼Œè‹¥é‡å åˆ™ä¸¢å¼ƒæ ·æœ¬ï¼›
    - æ ¹æ® `task_type`ï¼ˆ'grounding'/'caption'/'vqa'ï¼‰æ„é€  query/responseã€‚
    """

    def __init__(self, task_type, **kwargs):
        """
        åˆå§‹åŒ– GRIT é¢„å¤„ç†å™¨ã€‚

        å‚æ•°
        ----
        - task_type: ä»»åŠ¡ç±»å‹ï¼Œ'grounding' æˆ– 'caption' æˆ–å…¶ä»–ï¼ˆå¦‚ 'vqa'ï¼‰ã€‚
        - **kwargs: ä¼ é€’ç»™çˆ¶ç±» `RowPreprocessor` çš„å‚æ•°ã€‚
        """
        self.task_type = task_type  # è®°å½•å½“å‰ä»»åŠ¡ç±»å‹ï¼Œå½±å“æŸ¥è¯¢/å“åº”æ¨¡æ¿
        super().__init__(**kwargs)  # åˆå§‹åŒ–åŸºç±»ï¼ˆåˆ—æ˜ å°„ã€éšæœºçŠ¶æ€ç­‰ï¼‰

    @staticmethod
    def has_overlap(start_ends):
        """
        åˆ¤æ–­ä¸€ç»„èµ·æ­¢åŒºé—´æ˜¯å¦å­˜åœ¨é‡å ï¼ˆæŒ‰èµ·ç‚¹æ’åºåæ£€æŸ¥ï¼‰ã€‚

        å‚æ•°
        ----
        - start_ends: List[List[float]]ï¼Œæ¯ä¸ªå…ƒç´ ä¸º [start, end]ã€‚

        è¿”å›
        ----
        - bool: å­˜åœ¨é‡å è¿”å› Trueï¼Œå¦åˆ™ Falseã€‚

        ç¤ºä¾‹
        ----
        >>> GritPreprocessor.has_overlap([[0, 3], [2, 5]])
        True
        """
        for i in range(1, len(start_ends)):  # ä»ç¬¬äºŒä¸ªåŒºé—´å¼€å§‹ä¸å‰ä¸€ä¸ªæ¯”è¾ƒ
            if start_ends[i][0] < start_ends[i - 1][1]:  # è‹¥å½“å‰èµ·ç‚¹ < å‰ä¸€åŒºé—´ç»ˆç‚¹ï¼Œåˆ™é‡å 
                return True  # å­˜åœ¨é‡å 
        return False  # æ— é‡å 

    @staticmethod
    def replace_intervals_with_tags(response, start_ends):
        """
        å°†æ–‡æœ¬ `response` ä¸­çš„è‹¥å¹²åŒºé—´æ›¿æ¢ä¸ºå ä½æ ‡ç­¾ `<ref-object><bbox>`ã€‚

        å‚æ•°
        ----
        - response: åŸå§‹æè¿°æ–‡æœ¬ã€‚
        - start_ends: åŒºé—´åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ ä¸º [start, end]ã€‚

        è¿”å›
        ----
        - str: æ›¿æ¢å ä½åçš„æ–‡æœ¬ã€‚
        """
        result = []  # ä¿å­˜æ‹¼æ¥ç‰‡æ®µ
        last_end = 0  # ä¸Šä¸€æ¬¡æˆªå–çš„ç»“æŸä½ç½®
        for start, end in start_ends:  # éå†æ¯ä¸ªåŒºé—´
            result.append(response[int(last_end):int(start)])  # è¿½åŠ åŒºé—´å‰çš„åŸæ–‡
            result.append('<ref-object><bbox>')  # ç”¨å ä½æ ‡ç­¾æ›¿æ¢è¯¥åŒºé—´
            last_end = end  # æ›´æ–°æœ«å°¾ä½ç½®
        result.append(response[int(last_end):])  # è¿½åŠ æœ€åä¸€ä¸ªåŒºé—´åçš„åŸæ–‡
        return ''.join(result)  # æ‹¼æ¥æˆå­—ç¬¦ä¸²è¿”å›

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        å°† GRIT åŸå§‹è¡Œè½¬ä¸ºæ ‡å‡†æ ·æœ¬ï¼šç”Ÿæˆ `messages/images/objects`ã€‚

        å‚æ•°
        ----
        - row: åŸå§‹è®°å½•ï¼Œéœ€åŒ…å« `images`ï¼Œ`caption`ï¼Œ`ref_exps`ï¼š
          - `ref_exps[i]` å½¢å¦‚ [start, end, x1, y1, x2, y2, ...]ã€‚

        è¿”å›
        ----
        - Optional[Dict[str, Any]]: æ ‡å‡†åŒ–æ ·æœ¬ï¼›è‹¥åŒºé—´éæ³•æˆ–ç¼ºå¤±ä¿¡æ¯åˆ™è¿”å› Noneã€‚

        ç¤ºä¾‹
        ----
        >>> sample = {'images': ['img.jpg'], 'caption': 'a cat on a mat', 'ref_exps': [[2, 5, 0.1,0.1,0.3,0.3]]}
        >>> out = GritPreprocessor('grounding').preprocess(sample)
        >>> isinstance(out, dict) and 'objects' in out
        True
        """
        images = row['images']  # å›¾åƒè·¯å¾„ï¼ˆåˆ—è¡¨æˆ–å•ä¸€è·¯å¾„ï¼‰
        caption = row['caption']  # åŸå§‹æ•´å¥æè¿°
        ref_exps = row['ref_exps']  # æŒ‡ä»£è¡¨è¾¾ä¸è¾¹æ¡†åˆ—è¡¨
        objects = {'ref': [], 'bbox': [], 'bbox_type': 'norm1'}  # åˆå§‹åŒ– objectsï¼šnorm1 è¡¨ç¤º 0~1 å½’ä¸€åŒ–åæ ‡
        start_end_pairs = []  # æ”¶é›† [start, end] åŒºé—´ä»¥ç”¨äºæ’åºä¸é‡å æ£€æµ‹
        for ref_exp in ref_exps:  # éå†æ¯ä¸ªæŒ‡ä»£è¡¨è¾¾
            start = ref_exp[0]  # èµ·å§‹å­—ç¬¦ä½ç½®
            end = ref_exp[1]  # ç»“æŸå­—ç¬¦ä½ç½®
            # conf = ref_exp[6] TODO filter low confidence rows?  # ç½®ä¿¡åº¦å¯é€‰è¿‡æ»¤ï¼ˆå¾…å®ç°ï¼‰
            start_end_pairs.append(ref_exp[0:2])  # ä»…ä¿å­˜ [start, end]

            object_part = caption[int(start):int(end)]  # ä» caption ä¸­åˆ‡å‡ºå¯¹è±¡çŸ­è¯­
            objects['ref'].append(object_part)  # è®°å½•å¯¹è±¡çŸ­è¯­åˆ—è¡¨
            objects['bbox'].append(ref_exp[2:6])  # è®°å½•å¯¹åº” bboxï¼ˆx1,y1,x2,y2ï¼‰

        start_end_pairs.sort(key=lambda x: (x[0], x[1]))  # å…ˆæŒ‰èµ·ç‚¹å†æŒ‰ç»ˆç‚¹æ’åº
        if self.has_overlap(start_end_pairs) or not ref_exps:  # å­˜åœ¨é‡å æˆ–æ— æŒ‡ä»£è¡¨è¾¾
            return  # ä¸¢å¼ƒè¯¥æ ·æœ¬

        if self.task_type in ('grounding', 'caption'):  # éœ€è¦ä½¿ç”¨ grounding/caption æ¨¡æ¿
            query, response = self.construct_grounding_prompt()  # ç”± GroundingMixin éšæœºç”Ÿæˆæ¨¡æ¿å¯¹
        else:  # å…¶ä»–ä»»åŠ¡ï¼ˆå¦‚ vqaï¼‰ä½¿ç”¨é€šç”¨é—®æ³•
            query = 'what is the proper caption of this image?'  # é€šç”¨æŸ¥è¯¢
            response = caption  # ç›´æ¥è¿”å›æ•´å¥ caption ä½œä¸ºå‚è€ƒç­”æ¡ˆ
        return {  # è¿”å›æ ‡å‡†åŒ–æ ·æœ¬
            'messages': [{  # ä¸¤æ®µå¼æ¶ˆæ¯ï¼šç”¨æˆ·/åŠ©æ‰‹
                'role': 'user',
                'content': query
            }, {
                'role': 'assistant',
                'content': response
            }],
            'images': images,  # å›¾åƒè·¯å¾„ï¼ˆæˆ–åˆ—è¡¨ï¼‰
            'objects': objects  # å¯¹è±¡çŸ­è¯­ä¸å¯¹åº” bbox ä¿¡æ¯
        }


register_dataset(  # æ³¨å†Œ GRIT æ•°æ®é›†ï¼ˆå¤šä»»åŠ¡ï¼‰
    DatasetMeta(
        ms_dataset_id='swift/GRIT',  # MS ID
        hf_dataset_id='zzliang/GRIT',  # HF ID
        subsets=[  # å®šä¹‰å¤šä¸ªå­ä»»åŠ¡å­é›†
            SubsetDataset(
                name='caption',  # å›¾åƒæè¿°
                preprocess_func=GritPreprocessor('caption', columns={'url': 'images'}),  # ç»‘å®š caption é¢„å¤„ç†
            ),
            SubsetDataset(
                name='grounding',  # ç›®æ ‡æŒ‡ä»£
                preprocess_func=GritPreprocessor('grounding', columns={'url': 'images'}),  # ç»‘å®š grounding é¢„å¤„ç†
            ),
            SubsetDataset(
                name='vqa',  # è§†è§‰é—®ç­”
                preprocess_func=GritPreprocessor('vqa', columns={'url': 'images'}),  # ç»‘å®š vqa é¢„å¤„ç†
            )
        ],
        huge_dataset=True,  # æ•°æ®è§„æ¨¡è¾ƒå¤§
        tags=['multi-modal', 'en', 'caption-grounding', 'vqa', 'quality']))  # æ ‡ç­¾


class GQAPreprocessor(RowPreprocessor):
    """
    ç±»è¯´æ˜
    -----
    GQA é—®ç­”æ•°æ®ï¼š
    - prepare_dataset é˜¶æ®µä¸‹è½½ `gqa` åª’ä½“æ ¹ç›®å½•ï¼›
    - preprocess é˜¶æ®µç»‘å®šæœ¬åœ°å›¾ç‰‡è·¯å¾„å¹¶ç”Ÿæˆä¸¤æ®µå¼æ¶ˆæ¯ã€‚
    """

    def prepare_dataset(self, dataset):
        """
        ä¸‹è½½/å®šä½ gqa èµ„æºç›®å½•ã€‚
        """
        self.local_cache = MediaResource.download('gqa')  # ä¸‹è½½æˆ–ä½¿ç”¨æœ¬åœ°ç¼“å­˜
        return super().prepare_dataset(dataset)

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        æ„é€ é—®ç­”æ¶ˆæ¯å¹¶ç»‘å®šå›¾ç‰‡è·¯å¾„ï¼Œè‹¥å›¾ç‰‡ä¸å­˜åœ¨åˆ™è·³è¿‡ã€‚
        """
        image_path = os.path.join(self.local_cache, 'images', row['imageId'] + '.jpg')  # æ‹¼æ¥å›¾ç‰‡è·¯å¾„
        if os.path.exists(image_path):  # æ–‡ä»¶å­˜åœ¨
            return {
                'messages': [{
                    'role': 'user',
                    'content': row['question']  # é—®é¢˜æ–‡æœ¬
                }, {
                    'role': 'assistant',
                    'content': row['fullAnswer']  # ç­”æ¡ˆæ–‡æœ¬
                }],
                'images': image_path,  # æœ¬åœ°å›¾ç‰‡
            }
        else:
            return  # ç¼ºå¤±åˆ™è·³è¿‡


register_dataset(  # æ³¨å†Œ GQA æ•°æ®é›†
    DatasetMeta(
        hf_dataset_id='lmms-lab/GQA',  # HF IDï¼ˆæ—  MS IDï¼‰
        split=['train_all_instructions'],  # ä½¿ç”¨ç‰¹å®šåˆ’åˆ†
        preprocess_func=GQAPreprocessor(),  # ç»‘å®š GQA é¢„å¤„ç†å™¨
        huge_dataset=True,  # æ•°æ®é‡å¤§
        tags=['multi-modal', 'en', 'vqa', 'quality']))  # æ ‡ç­¾


class CocoPreprocessor(ResponsePreprocessor):
    category = [
        'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus', 'train', 'truck', 'boat', 'traffic light',
        'fire hydrant', 'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse', 'sheep', 'cow',
        'elephant', 'bear', 'zebra', 'giraffe', 'backpack', 'umbrella', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',
        'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove', 'skateboard', 'surfboard',
        'tennis racket', 'bottle', 'wine glass', 'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',
        'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake', 'chair', 'couch', 'potted plant', 'bed',
        'dining table', 'toilet', 'tv', 'laptop', 'mouse', 'remote', 'keyboard', 'cell phone', 'microwave', 'oven',
        'toaster', 'sink', 'refrigerator', 'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier', 'toothbrush'
    ]

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        row['query'] = 'Task: Object Detection'
        objects = row['objects']
        objects['ref'] = [self.category[c] for c in objects['category']]
        row['response'] = '\n'.join(['<ref-object><bbox>'] * len(objects['ref']))
        return super().preprocess(row)


register_dataset(  # æ³¨å†Œ COCO æ£€æµ‹æ ¼å¼æ•°æ®
    DatasetMeta(
        ms_dataset_id='AI-ModelScope/coco',  # MS ID
        hf_dataset_id='detection-datasets/coco',  # HF ID
        preprocess_func=CocoPreprocessor(),  # ç»‘å®š COCO é¢„å¤„ç†å™¨
        huge_dataset=True,  # æ•°æ®é‡å¤§
        tags=['multi-modal', 'en', 'vqa', 'quality']))  # æ ‡ç­¾


class LLaVAMixSFTPreprocessor(RowPreprocessor):
    """
    ç±»è¯´æ˜
    -----
    LLaVA æ··åˆæŒ‡ä»¤æ•°æ®ï¼ˆè§†è§‰ SFTï¼‰é¢„å¤„ç†ï¼š
    - å°†å¤šæ¨¡æ€æ¶ˆæ¯çš„ content åˆ—è¡¨ç»„è£…æˆçº¯æ–‡æœ¬ï¼ˆå›¾ç‰‡ä½ç½®ä»¥ `<image>` å ä½ï¼‰ï¼›
    - è¾“å‡ºæ ‡å‡† `messages` åˆ—è¡¨ã€‚
    """

    def preprocess(self, row: Dict[str, Any]) -> Dict[str, Any]:
        """
        å°†å¤šæ¨¡æ€å†…å®¹å±•å¹³æˆæ–‡æœ¬æ¶ˆæ¯åˆ—è¡¨ã€‚
        """
        messages = row['messages']  # åŸå§‹å¤šæ¨¡æ€æ¶ˆæ¯
        rounds = []  # è¾“å‡ºçš„æ–‡æœ¬åŒ–æ¶ˆæ¯
        for msg in messages:  # éå†æ¯è½®
            role = msg['role']  # è§’è‰²
            content = msg['content']  # å†…å®¹åˆ—è¡¨ï¼ˆtext/imageï¼‰
            text = ''  # è¯¥è½®èšåˆæ–‡æœ¬
            for index in content:  # éå†å†…å®¹ç‰‡æ®µ
                if index['type'] == 'text':  # æ–‡æœ¬ç‰‡æ®µ
                    text += index['text']
                elif index['type'] == 'image':  # å›¾ç‰‡ç‰‡æ®µ
                    text += '<image>'  # ç”¨å ä½ç¬¦è¡¨ç¤º

            rounds.append({'role': role, 'content': text})  # è¿½åŠ è¯¥è½®

        return {'messages': rounds}  # è¿”å›ç»“æœ


register_dataset(  # æ³¨å†Œ LLaVA æŒ‡ä»¤æ··åˆé›†ï¼ˆè§†è§‰ SFT éªŒè¯ï¼‰
    DatasetMeta(
        ms_dataset_id='swift/llava-instruct-mix-vsft',  # MS ID
        hf_dataset_id='HuggingFaceH4/llava-instruct-mix-vsft',  # HF ID
        split=['test'],  # æµ‹è¯•åˆ’åˆ†
        preprocess_func=LLaVAMixSFTPreprocessor(),  # ç»‘å®šé¢„å¤„ç†å™¨
        tags=['multi-modal', 'en', 'vqa', 'quality']))  # æ ‡ç­¾


class LatexocrPreprocessor(ResponsePreprocessor):
    """
    ç±»è¯´æ˜
    -----
    LaTeX OCR ä»»åŠ¡é¢„å¤„ç†ï¼šå°† query ç»Ÿä¸€ä¸ºè‹±æ–‡è¯´æ˜ï¼Œäº¤ç”±çˆ¶ç±»å®Œæˆæ ‡å‡†åŒ–ã€‚
    """

    def preprocess(self, row: Dict[str, Any]) -> Dict[str, Any]:
        """è®¾ç½®ç»Ÿä¸€æŸ¥è¯¢å¹¶ç”Ÿæˆæ ‡å‡† messagesã€‚"""
        row['query'] = 'Using LaTeX to perform OCR on the image.'  # ç»Ÿä¸€ä»»åŠ¡æè¿°
        return super().preprocess(row)  # æ ‡å‡†åŒ–


register_dataset(  # æ³¨å†Œ LaTeX OCR æ•°æ®é›†
    DatasetMeta(
        ms_dataset_id='AI-ModelScope/LaTeX_OCR',  # MS ID
        hf_dataset_id='linxy/LaTeX_OCR',  # HF ID
        subsets=['default', 'human_handwrite', 'human_handwrite_print', 'synthetic_handwrite', 'small'],  # å¤šå­é›†
        preprocess_func=LatexocrPreprocessor(),  # ç»‘å®šé¢„å¤„ç†å™¨
        split=['train', 'validation', 'test'],  # è®­ç»ƒ/éªŒè¯/æµ‹è¯•
        tags=['chat', 'ocr', 'multi-modal', 'vision'],  # æ ‡ç­¾
    ))


class CapchaImagesPreprocessor(ResponsePreprocessor):
    """
    ç±»è¯´æ˜
    -----
    éªŒè¯ç å›¾ç‰‡è¯†åˆ«ä»»åŠ¡é¢„å¤„ç†ï¼šç»Ÿä¸€ queryï¼Œä¿ç•™åŸå§‹ responseã€‚
    """

    def preprocess(self, row: Dict[str, Any]) -> Dict[str, Any]:
        """è®¾ç½®ç»Ÿä¸€æŸ¥è¯¢å¹¶ç”Ÿæˆæ ‡å‡† messagesã€‚"""
        row['query'] = 'recognize the content.'  # æŒ‡å®šä»»åŠ¡æ„å›¾
        return super().preprocess(row)  # æ ‡å‡†åŒ–


register_dataset(  # æ³¨å†ŒéªŒè¯ç å›¾ç‰‡æ•°æ®é›†
    DatasetMeta(
        ms_dataset_id='AI-ModelScope/captcha-images',  # MS ID
        split=['train', 'validation'],  # è®­ç»ƒ/éªŒè¯
        preprocess_func=CapchaImagesPreprocessor(columns={'solution': 'response'}),  # åˆ—æ˜ å°„ï¼šç­”æ¡ˆä¸º response
        tags=['chat', 'multi-modal', 'vision']))  # æ ‡ç­¾


class ClevrPreprocessor(ResponsePreprocessor):

    def preprocess(self, row: Dict[str, Any]) -> Dict[str, Any]:
        query = row.get('query', '')
        query = (f'{query} Output the thinking process in <think> </think> and '
                 'final answer (number) in <answer> </answer> tags.')
        row.update({'query': query})
        return super().preprocess(row)


register_dataset(
    DatasetMeta(
        ms_dataset_id='okwinds/clevr_cogen_a_train',
        hf_dataset_id='leonardPKU/clevr_cogen_a_train',
        preprocess_func=ClevrPreprocessor(),
        tags=['qa', 'math', 'vision', 'grpo']))
