"""
æ¨¡å—åŠŸèƒ½
-------
æœ¬æ¨¡å—å®šä¹‰äº†å¤šç§æ•°æ®é¢„å¤„ç†åŸºç±»ä¸å·¥å…·ï¼Œè´Ÿè´£å°†åŸå§‹æ•°æ®é›†æ ·æœ¬åˆ—ï¼ˆæ–‡æœ¬ã€å¯¹è¯ã€å¤šæ¨¡æ€å­—æ®µç­‰ï¼‰
è§„èŒƒåŒ–ä¸ºè®­ç»ƒæ‰€éœ€çš„æ ‡å‡†ç»“æ„ï¼ˆå¦‚ `messages/query/response/images/videos/audios/tools/objects`ï¼‰ã€‚

æ ¸å¿ƒèƒ½åŠ›
-------
- `RowPreprocessor`: æŠ½è±¡åŸºç±»ï¼Œæä¾›æ‰¹å¤„ç†æ˜ å°„ã€å­—æ®µé‡å‘½åã€æ¶ˆæ¯æ ¡éªŒã€MM æ•°æ®è½¬æ¢ç­‰èƒ½åŠ›ï¼›
- `ResponsePreprocessor/AlpacaPreprocessor`: ç»Ÿä¸€å°† `query/response/history/system` è½¬æ¢ä¸º `messages`ï¼›
- `MessagesPreprocessor`: å…¼å®¹å¤šç§æ¶ˆæ¯å­—æ®µ/è§’è‰²åï¼Œä¿®å¤å†å²å¹¶å¯¹é½åˆ°æ ‡å‡† `messages`ï¼›
- `ClsPreprocessor`: åœ¨å“åº”å¼é¢„å¤„ç†åŸºç¡€ä¸Šè¡¥å…… `label`ï¼›
- `AutoPreprocessor`: æ ¹æ®æ•°æ®é›†ç‰¹å¾è‡ªåŠ¨é€‰æ‹©åˆé€‚çš„é¢„å¤„ç†å™¨ï¼›
- å…¶ä½™å·¥å…·å‡½æ•°ï¼šä¿®è¡¥å‡½æ•°ã€ArrowWriter æ‰“è¡¥ä¸ç­‰ï¼Œä¿éšœæµå¼ä¸åˆ†å¸ƒå¼ç¯å¢ƒä¸‹æ˜ å°„ç¨³å®šã€‚

å…¸å‹ç”¨æ³•
-------
>>> proc = ResponsePreprocessor(columns={'input': 'query', 'answer': 'response'})
>>> new_ds = proc(dataset, num_proc=4, load_from_cache_file=True)

è¯´æ˜ï¼šä»£ç å·²æŒ‰è¡Œé™„åŠ ä¸­æ–‡æ³¨é‡Šæˆ–åœ¨å‡½æ•°/ç±»æ–‡æ¡£ä¸­è¯¦è¿°å„æ­¥éª¤çš„ä½œç”¨ï¼Œä¾¿äºç»´æŠ¤ä¸æ’éšœã€‚
"""

# Copyright (c) Alibaba, Inc. and its affiliates.  # ç‰ˆæƒå£°æ˜
import ast  # å®‰å…¨è§£æå­—ç¬¦ä¸²å­—é¢é‡
import os  # OS è·¯å¾„ä¸ç¯å¢ƒå˜é‡æ“ä½œ
from collections import Counter  # è®¡æ•°å™¨ï¼Œç”¨äºç»Ÿè®¡åˆ—é‡å‘½åå†²çª
from contextlib import contextmanager  # ä¸Šä¸‹æ–‡ç®¡ç†å™¨è£…é¥°å™¨
from typing import Any, Callable, Dict, List, Optional, Union  # ç±»å‹æ³¨è§£å·¥å…·

import numpy as np  # éšæœºä¸æ•°ç»„å·¥å…·
from datasets import Dataset as HfDataset  # HF å¸¸è§„æ•°æ®é›†
from datasets import Image  # HF å›¾åƒåˆ—ç±»å‹
from datasets import IterableDataset as HfIterableDataset  # HF å¯è¿­ä»£æ•°æ®é›†
from datasets import Sequence, Value  # HF ç±»å‹ç³»ç»Ÿï¼šåºåˆ—ä¸å€¼ç±»å‹

from swift.llm import history_to_messages  # å°† (query, response) å†å²è½¬æ¢ä¸º messages çš„å·¥å…·
from swift.utils import get_logger, is_dist, is_master, safe_ddp_context  # æ—¥å¿—ä¸åˆ†å¸ƒå¼å·¥å…·

DATASET_TYPE = Union[HfDataset, HfIterableDataset]  # ç»Ÿä¸€è¡¨ç¤ºä¸¤ç§ HF æ•°æ®é›†ç±»å‹

logger = get_logger()  # æ¨¡å—çº§æ—¥å¿—è®°å½•å™¨


class RowPreprocessor:
    """
    è¡Œçº§é¢„å¤„ç†å™¨æŠ½è±¡åŸºç±»ï¼šæä¾›å­—æ®µå¯¹é½ã€æ¶ˆæ¯æ ¡éªŒã€å¤šæ¨¡æ€æ•°æ®è§„æ•´ã€æ‰¹å¤„ç†æ˜ å°„ç­‰é€šç”¨èƒ½åŠ›ã€‚

    ä½¿ç”¨è¯´æ˜
    -------
    - å­ç±»éœ€å®ç° `preprocess(self, row)` å°†å•æ¡æ ·æœ¬è½¬ä¸ºæ ‡å‡†ç»“æ„ï¼›
    - è°ƒç”¨ `__call__` å¯å¯¹æ•´ä¸ªæ•°æ®é›†æ‰§è¡Œ map æ“ä½œå¹¶è¿”å›æ–°æ•°æ®é›†ã€‚
    """
    standard_keys = [
        'messages', 'rejected_response', 'rejected_images', 'label', 'images', 'videos', 'audios', 'tools', 'objects',
        'channel', 'margin'  # é¢„å¤„ç†åå¯èƒ½å‡ºç°çš„æ ‡å‡†å­—æ®µé›†åˆ
    ]

    def __init__(self,
                 *,
                 columns: Optional[Dict[str, str]] = None,
                 dataset_sample: Optional[int] = None,
                 random_state: Optional[Union[np.random.RandomState, int]] = 42,
                 traceback_limit: int = 10) -> None:
        self.columns = columns or {}  # åˆ—æ˜ å°„ï¼šæºåˆ—å -> ç›®æ ‡æ ‡å‡†åˆ—å
        self.origin_columns = self.columns.copy()  # æœ€é«˜ä¼˜å…ˆçº§çš„åŸå§‹æ˜ å°„ï¼Œå†²çªæ—¶ä¼˜å…ˆ
        images_keys = ['images', 'image']  # å¯èƒ½çš„å›¾åƒåˆ—åˆ«å
        audios_keys = ['audios', 'audio']  # å¯èƒ½çš„éŸ³é¢‘åˆ—åˆ«å
        videos_keys = ['videos', 'video']  # å¯èƒ½çš„è§†é¢‘åˆ—åˆ«å
        for mm_type in ['images', 'audios', 'videos']:  # ç»Ÿä¸€è¡¥å……å¤šæ¨¡æ€åˆ—åˆ«åæ˜ å°„
            keys = locals()[f'{mm_type}_keys']  # å–å¯¹åº”åˆ«ååˆ—è¡¨
            for key in keys:  # å°†åˆ«åæ˜ å°„åˆ°æ ‡å‡†åˆ—å
                self.columns[key] = mm_type

        self.traceback_limit = traceback_limit  # è®°å½•å¯æ‰“å°å›æº¯çš„æœ€å¤§æ¬¡æ•°
        self._traceback_counter = 0  # å·²æ‰“å°å›æº¯è®¡æ•°
        self.dataset_sample = dataset_sample  # å¯é€‰é‡‡æ ·æ¡æ•°ï¼Œç”¨äºå¿«é€Ÿè°ƒè¯•
        if not isinstance(random_state, np.random.RandomState):  # å½’ä¸€åŒ–ä¸º RandomState
            random_state = np.random.RandomState(random_state)  # ä»¥ seed åˆå§‹åŒ–
        self.random_state = random_state  # ä¿å­˜éšæœºçŠ¶æ€

    @staticmethod
    def _check_messages(row: Dict[str, Any]) -> None:
        """æ ¡éªŒ `messages` ç»“æ„ä¸è§’è‰²å­—æ®µçš„åˆæ³•æ€§ï¼Œç§»é™¤éæ ‡å‡†é”®ã€‚"""
        if 'messages' not in row:  # æ— æ¶ˆæ¯å­—æ®µåˆ™è·³è¿‡
            return
        messages = row['messages']  # å–æ¶ˆæ¯åˆ—è¡¨
        assert len(messages) > 0, f'messages: {messages}'  # è‡³å°‘ä¸€æ¡
        # fix swift/SlimOrca  # å…¼å®¹æ€§ï¼šåªä¿ç•™ role/content ä¸¤é”®
        for message in messages:
            keys = set(message.keys()) - {'role', 'content'}  # æ‰¾åˆ°å¤šä½™é”®
            for key in keys:  # é€ä¸ªç§»é™¤
                message.pop(key)

        for message in messages:  # éå†æ£€æŸ¥æ¯æ¡æ¶ˆæ¯
            role, content = message['role'], message['content']  # å–è§’è‰²ä¸å†…å®¹
            # The terms 'tool' and 'tool_response' have the same meaning, ensuring compatibility.  # å…¼å®¹å·¥å…·æ¶ˆæ¯
            assert role in {'system', 'user', 'tool_call', 'tool_response', 'tool', 'assistant'}, f'message: {message}'  # è§’è‰²åˆæ³•
            assert content is not None, f'message: {message}'  # å†…å®¹ä¸å¯ä¸º None

    @staticmethod
    def _cast_mm_data(row: Dict[str, Any]) -> None:
        """å°†å¤šæ¨¡æ€å­—æ®µç»Ÿä¸€ä¸ºæ ‡å‡†ç»“æ„ï¼šimages/rejected_images -> [{'bytes','path'}]ï¼Œvideos/audios -> listã€‚"""
        for key in ['images', 'rejected_images']:  # å¤„ç†å›¾åƒç±»å­—æ®µ
            images = row.get(key, None)  # è¯»å–å­—æ®µ
            if images is None:  # æ— åˆ™è·³è¿‡
                continue

            if isinstance(images, str) or (isinstance(images, list) and images and isinstance(images[0], str)):  # å­—ç¬¦ä¸²æˆ–å­—ç¬¦ä¸²åˆ—è¡¨
                if isinstance(images, str):  # å•å­—ç¬¦ä¸²è½¬åˆ—è¡¨
                    images = [images]
                for i, image in enumerate(images):  # åŒ…è£…ä¸º dict ç»“æ„
                    images[i] = {'bytes': None, 'path': image}
                row[key] = images  # å›å†™
            elif isinstance(images, dict):  # å•ä¸ª dict è½¬åˆ—è¡¨
                row[key] = [images]

        for key in ['videos', 'audios']:  # å¤„ç†è§†é¢‘/éŸ³é¢‘å­—æ®µ
            mm_data = row.get(key)  # è¯»å–å­—æ®µ
            if mm_data is None:  # æ— åˆ™è·³è¿‡
                continue
            elif isinstance(mm_data, str):  # å•å­—ç¬¦ä¸² -> åˆ—è¡¨
                row[key] = [mm_data]

    @staticmethod
    def _check_rejected_response(row: Dict[str, Any]) -> None:
        """
        å…¼å®¹ DPO/ORPOï¼š
        - è‹¥æä¾› `rejected_messages`ï¼Œåˆå¹¶ä¸ `messages` å¯¹é½ï¼Œå¹¶æå– `rejected_response`ï¼›
        - è‹¥å·²æœ‰ `rejected_response`ï¼Œéœ€ä¸æœ€åä¸€æ¡ assistant å›å¤ä¸åŒï¼Œå¦åˆ™æŠ¥é”™ã€‚
        """
        if 'rejected_messages' in row:  # åŒæ—¶æä¾›æ­£/è´Ÿæ¶ˆæ¯
            chosen_messages = row['messages']  # æ­£æ ·æœ¬æ¶ˆæ¯
            rejected_messages = row['rejected_messages']  # è´Ÿæ ·æœ¬æ¶ˆæ¯
            messages = []  # åˆå¹¶åçš„æ¶ˆæ¯
            rejected_response = None  # å­˜æ”¾æ‹’ç»å›å¤
            for chosen_user, chosen_assistant, rejected_user, rejected_assistant in zip(
                    chosen_messages[::2], chosen_messages[1::2], rejected_messages[::2], rejected_messages[1::2]):  # æˆå¯¹éå†
                assert chosen_user == rejected_user  # ç”¨æˆ·æ¶ˆæ¯åº”ä¸€è‡´
                messages.append(chosen_user)  # æ·»åŠ ç”¨æˆ·
                messages.append(chosen_assistant)  # æ·»åŠ æ­£æ ·æœ¬åŠ©æ‰‹
                if chosen_assistant != rejected_assistant:  # å¦‚æ­£è´ŸåŠ©æ‰‹ä¸åŒ
                    rejected_response = rejected_assistant['content']  # è®°å½•è´Ÿæ ·æœ¬å†…å®¹
            row['messages'] = messages  # å›å†™åˆå¹¶æ¶ˆæ¯
            row['rejected_response'] = rejected_response  # å›å†™æ‹’ç»å›å¤

        if 'rejected_response' in row:  # æ˜ç¡®ç»™å‡ºæ‹’ç»å›å¤
            messages = row['messages']  # å½“å‰æ¶ˆæ¯
            rejected_response = row['rejected_response']  # è´Ÿæ ·æœ¬å›å¤
            if rejected_response is None or rejected_response == messages[-1]['content']:  # ä¸åº”ä¸ºç©ºæˆ–ç­‰äºæœ€åä¸€æ¡æ­£æ ·æœ¬
                raise ValueError(f'rejected_response: {rejected_response}')  # æŠ›é”™æç¤º

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """æŠ½è±¡æ–¹æ³•ï¼šå­ç±»éœ€å®ç°å¯¹å•æ¡æ ·æœ¬çš„è§„èŒƒåŒ–ã€‚"""
        raise NotImplementedError  # ç”±å­ç±»å®ç°

    def prepare_dataset(self, dataset: DATASET_TYPE) -> DATASET_TYPE:
        """å¯åœ¨æ˜ å°„å‰å¯¹æ•°æ®é›†è¿›è¡Œå‡†å¤‡ï¼ˆä¸‹è½½åª’ä½“/ä¿®å¤åˆ—ç­‰ï¼‰ï¼Œé»˜è®¤ç›´æ¥è¿”å›ã€‚"""
        return dataset  # é»˜è®¤ä¸å˜

    @staticmethod
    def batched_to_rows(batched_row: Dict[str, Any]):
        """å°† batched è¡Œè½¬æ¢ä¸ºè¡Œåˆ—è¡¨ï¼Œæ–¹ä¾¿é€è¡Œå¤„ç†ã€‚"""
        keys = list(batched_row.keys())  # å–æ‰€æœ‰é”®
        batch_size = len(batched_row[keys[0]])  # æ‰¹å¤§å°æŒ‰ç¬¬ä¸€ä¸ªé”®é•¿åº¦å†³å®š
        return [{key: batched_row[key][i] for key in keys} for i in range(batch_size)]  # é€æ ·æœ¬é‡ç»„

    @staticmethod
    def rows_to_batched(rows: List[Dict[str, Any]]):
        """å°†è¡Œåˆ—è¡¨é‡æ–°æ‹¼æˆ batched ç»“æ„ï¼Œå¡«è¡¥ç¼ºå¤±åˆ—ä½¿é•¿åº¦ä¸€è‡´ã€‚"""
        batched = {}  # ç»“æœå®¹å™¨
        for i, row in enumerate(rows):  # æšä¸¾è¡Œ
            for k, v in row.items():  # éå†é”®å€¼
                if k not in batched:  # æ–°é”®åˆ™è¡¥é½ä¹‹å‰ä½ç½®
                    batched[k] = [None] * i
                batched[k].append(v)  # è¿½åŠ å½“å‰å€¼
            # Make all the lengths of v the same.  # å¯¹ç¼ºå¤±åˆ—ç”¨ None è¡¥é½
            for k in set(batched.keys()) - set(row.keys()):
                batched[k].append(None)
        return batched  # è¿”å› batched å­—å…¸

    @staticmethod
    def _remove_prefix_keys(row, prefix: str):
        """ç§»é™¤å­—å…¸é”®çš„å‰ç¼€ï¼ˆå…¼å®¹æµå¼/GRPO ç¼“å­˜å­—æ®µå‘½åï¼‰ã€‚"""
        for k in list(row.keys()):  # éå†åŸå§‹é”®åˆ—è¡¨
            if k.startswith(prefix):  # å‘½ä¸­å‰ç¼€
                new_k = k[len(prefix):]  # å»æ‰å‰ç¼€åçš„æ–°é”®
                new_v = row.pop(k)  # å¼¹å‡ºæ—§é”®çš„å€¼
                if new_k not in row:  # é¿å…è¦†ç›–
                    row[new_k] = new_v  # å†™å›æ–°é”®

    @staticmethod
    def _check_objects(row):
        """è§„èŒƒ objects å­—æ®µé¡ºåºå¹¶æ£€æŸ¥ bbox åˆæ³•æ€§ï¼ˆåæ ‡æœ‰åº/é•¿åº¦ 2 æˆ– 4ï¼‰ã€‚"""
        objects = row.get('objects')  # è¯»å–å¯¹è±¡å­—æ®µ
        if objects is None:  # æ— åˆ™è·³è¿‡
            return
        new_objects = {}  # è§„èŒƒåçš„å¯¹è±¡
        # Ensure the order  # ä¿æŒé”®é¡ºåºä¸€è‡´
        for k in ['ref', 'bbox', 'bbox_type', 'image_id']:
            if k in objects.keys():  # å­˜åœ¨åˆ™ä¿ç•™
                new_objects[k] = objects[k]
        row['objects'] = new_objects  # å›å†™
        bbox = new_objects['bbox']  # å–å‡º bbox åˆ—è¡¨

        # check bbox  # åˆæ³•æ€§æ£€æŸ¥
        for box in bbox:  # éå†æ¯ä¸ªæ¡†
            assert len(box) in {2, 4}, f'len(box): {len(box)}'  # æ”¯æŒç‚¹æˆ–çŸ©å½¢
            if len(box) == 2:  # ç‚¹æ¡†æ— éœ€è°ƒæ•´
                continue
            if box[0] > box[2]:  # ç¡®ä¿ x1<=x2
                box[0], box[2] = box[2], box[0]
            if box[1] > box[3]:  # ç¡®ä¿ y1<=y2
                box[1], box[3] = box[3], box[1]

    def batched_preprocess(self, batched_row: Dict[str, Any], *, strict: bool,
                           ignore_max_length_error: bool) -> Dict[str, Any]:
        """
        å¯¹ batched è¡Œæ‰§è¡Œå®‰å…¨çš„é€è¡Œé¢„å¤„ç†ï¼š
        - æ”¯æŒå­ç±» `preprocess` è¿”å›å•æ¡æˆ–å¤šæ¡æ ·æœ¬ï¼›
        - æ ¡éªŒ/ä¿®è¡¥ objects/messages/rejected_response/mm æ•°æ®ï¼›
        - åœ¨é strict æ¨¡å¼ä¸‹ï¼Œå¯¹å¼‚å¸¸æ ·æœ¬è¿›è¡Œè¿‡æ»¤å¹¶é™é‡æ‰“å°å›æº¯ã€‚

        å‚æ•°
        ----
        - batched_row: batched æ ¼å¼çš„è¾“å…¥å­—å…¸
        - strict: True æ—¶é‡åˆ°é”™è¯¯ç›´æ¥æŠ›å‡ºï¼›False æ—¶è¿‡æ»¤é”™è¯¯æ ·æœ¬
        - ignore_max_length_error: True æ—¶å¿½ç•¥æ¨¡æ¿é•¿åº¦ç›¸å…³é”™è¯¯

        è¿”å›
        ----
        - Dict[str, Any]: batched ç»“æ„çš„æ ‡å‡†åŒ–ç»“æœ
        """
        from ...template import MaxLengthError  # å»¶è¿Ÿå¯¼å…¥ï¼Œé¿å…å¾ªç¯ä¾èµ–
        batched_row = dict(batched_row)  # å¤åˆ¶ï¼Œé¿å…åŸåœ°ä¿®æ”¹
        assert len(batched_row) > 0  # éç©ºæ–­è¨€
        self._remove_prefix_keys(batched_row, '__@')  # compat streaming  # å»é™¤æµå¼å‰ç¼€
        rows = self.batched_to_rows(batched_row)  # æ‹†ä¸ºé€è¡Œ

        new_rows = []  # æ”¶é›†åˆæ³•è¡Œ
        for row in rows:  # éå†æ¯è¡Œ
            try:
                row = self.preprocess(row)  # ç”±å­ç±»å®ç°
                # support [row1, row2, ...]  # å…è®¸è¿”å›åˆ—è¡¨æˆ–å•æ¡/None
                if row is None:
                    row = []  # è¿‡æ»¤
                if isinstance(row, dict):  # å•æ¡ -> åˆ—è¡¨
                    row = [row]
                for r in row:  # å¯¹æ¯ä¸ªè¿”å›æ ·æœ¬åšæ ¡éªŒä¸è§„æ•´
                    self._check_objects(r)  # è§„èŒƒ objects
                    self._check_messages(r)  # æ ¡éªŒ messages
                    self._check_rejected_response(r)  # å¤„ç†æ‹’ç»å›å¤
                    self._cast_mm_data(r)  # ç»Ÿä¸€ MM æ•°æ®æ ¼å¼
            except Exception as e:  # æ•è·é¢„å¤„ç†å¼‚å¸¸
                if strict:  # ä¸¥æ ¼æ¨¡å¼æŠ›å‡º
                    logger.warning('To avoid errors, you can pass `strict=False`.')  # æç¤ºå¯åˆ‡æ¢
                    raise
                if isinstance(e, MaxLengthError) and ignore_max_length_error:  # å¯å¿½ç•¥çš„é•¿åº¦é”™è¯¯
                    pass
                elif self.traceback_limit is not None and self._traceback_counter < self.traceback_limit:  # é™é‡æ‰“å°å›æº¯
                    import traceback  # å»¶è¿Ÿå¯¼å…¥
                    logger.info(traceback.format_exc())  # æ‰“å°å †æ ˆ
                    logger.warning('ğŸ‘†ğŸ‘†ğŸ‘†There are errors in the dataset, the data will be deleted')  # å‘Šè­¦è¿‡æ»¤
                    self._traceback_counter += 1  # è®¡æ•°
                row = []  # è¯¥æ ·æœ¬ä¸¢å¼ƒ
            new_rows += row  # ç´¯åŠ ç»“æœ
        res = self.rows_to_batched(new_rows)  # é‡æ–°æ‹¼ä¸º batched
        self._remove_prefix_keys(res, '__#')  # compat GRPO  # å»é™¤ solution ç¼“å­˜å‰ç¼€
        if len(res) == 0:  # è‹¥å…¨éƒ¨è¢«è¿‡æ»¤
            res['messages'] = []  # è‡³å°‘ä¿ç•™ç©º messages åˆ—

        return res  # è¿”å›ç»“æœ

    @staticmethod
    def get_features_dataset(dataset: DATASET_TYPE) -> DATASET_TYPE:
        """ç¡®ä¿ dataset å¸¦æœ‰ featuresï¼ˆå¯¹ IterableDataset éœ€å…ˆè§£æï¼‰ã€‚"""
        if dataset.features is None:  # æ—  features éœ€è§£æ
            assert isinstance(dataset, HfIterableDataset)  # ä»…å¯è¿­ä»£æ•°æ®é›†æœ‰æ­¤æƒ…å½¢
            dataset = dataset._resolve_features()  # è§£æç‰¹å¾
        return dataset  # è¿”å›å¸¦ features çš„æ•°æ®é›†

    @staticmethod
    def safe_rename_columns(dataset, columns):
        """å®‰å…¨åœ°é‡å‘½ååˆ—ï¼šä»…å¯¹å­˜åœ¨çš„åˆ—è¿›è¡Œé‡å‘½åï¼Œä¸”é¿å…ç›®æ ‡åå†²çªã€‚"""
        dataset = RowPreprocessor.get_features_dataset(dataset)  # ç¡®ä¿æœ‰ features
        columns_keys = {k.lower(): k for k in dataset.features.keys()}  # lower -> åŸå§‹å¤§å°å†™é”®
        safe_columns = {columns_keys[k.lower()]: v for k, v in columns.items() if k.lower() in columns_keys}  # è¿‡æ»¤å­˜åœ¨çš„æºåˆ—

        counter = Counter(safe_columns.values())  # ç»Ÿè®¡ç›®æ ‡åå‡ºç°æ¬¡æ•°
        for k, new_k in list(safe_columns.items()):  # ç§»é™¤ä¼šäº§ç”Ÿç›®æ ‡åå†²çªçš„æ˜ å°„
            if counter[new_k] > 1:
                # For example, if "response" and "answer" match, then no processing is done.  # å†²çªåˆ™è·³è¿‡
                safe_columns.pop(k)
                continue

        # e.g. Keep {'query': 'query'} to ensure that the query has the highest priority.  # å»æ‰åŒåæ˜ å°„ï¼ˆæ— æ„ä¹‰ï¼‰
        safe_columns = {k: v for k, v in safe_columns.items() if k != v}
        if safe_columns:  # å­˜åœ¨éœ€é‡å‘½åçš„åˆ—
            dataset = dataset.rename_columns(safe_columns)  # æ‰§è¡Œé‡å‘½å

        return dataset  # è¿”å›æ•°æ®é›†

    def _rename_columns(self, dataset: DATASET_TYPE) -> DATASET_TYPE:
        """ä¸¤é˜¶æ®µé‡å‘½åï¼šå…ˆæŒ‰ origin_columnsï¼Œå†æŒ‰ columnsï¼›æµå¼æ•°æ®é›†è¿½åŠ å‰ç¼€ä»¥å…¼å®¹å†™å…¥ã€‚"""
        dataset = self.safe_rename_columns(dataset, self.origin_columns)  # å…ˆåº”ç”¨åŸå§‹é«˜ä¼˜å…ˆçº§æ˜ å°„
        dataset = self.safe_rename_columns(dataset, self.columns)  # å†åº”ç”¨è¡¥å……æ˜ å°„
        if isinstance(dataset, HfIterableDataset):  # æµå¼æ•°æ®é›†å†™å…¥å…¼å®¹
            # fix: https://github.com/huggingface/datasets/issues/6408  # åŠ å‰ç¼€ç»•è¿‡å†™å…¥é™åˆ¶
            columns = {k: f'__@{k}' for k in RowPreprocessor.standard_keys if k in dataset.features}
            if columns:
                dataset = dataset.rename_columns(columns)  # é‡å‘½ååŠ å‰ç¼€
        return dataset  # è¿”å›æ•°æ®é›†

    @staticmethod
    def remove_useless_columns(dataset: DATASET_TYPE) -> DATASET_TYPE:
        """ä»…ä¿ç•™æ ‡å‡†é”®åˆ—ï¼Œå»é™¤æ— ç”¨åˆ—ä»¥èŠ‚çœå­˜å‚¨ä¸ä¼ è¾“ã€‚"""
        dataset = RowPreprocessor.get_features_dataset(dataset)  # ç¡®ä¿ features å¯ç”¨
        features = dataset.features  # å–ç‰¹å¾æè¿°
        k_list = [k for k in RowPreprocessor.standard_keys if k in features]  # ä»…æ ‡å‡†åˆ—
        if len(k_list) != len(features):  # å­˜åœ¨å†—ä½™åˆ—
            dataset = dataset.select_columns(k_list)  # é€‰æ‹©å­é›†
        return dataset  # è¿”å›è£å‰ªåæ•°æ®é›†

    @staticmethod
    @contextmanager
    def _patch_arrow_writer():
        """ä¸º ArrowWriter æ‰“è¡¥ä¸ï¼Œç¡®ä¿å†™å…¥æ—¶æ ‡å‡†åˆ— features æ­£ç¡®å£°æ˜ï¼ˆå°¤å…¶ messages/images/objectsï¼‰ã€‚"""
        # fix AI-ModelScope/ms_agent_for_agentfabric:all  # é’ˆå¯¹éƒ¨åˆ†æ•°æ®é›†çš„å…¼å®¹ä¿®å¤
        from datasets.arrow_writer import ArrowWriter  # å¯¼å…¥ ArrowWriter ç±»

        def _new_init(self, schema=None, features=None, *args, **kwargs):  # æ›¿æ¢æ„é€ å‡½æ•°

            if features is not None:  # è‹¥ features å­˜åœ¨åˆ™è¡¥å……æ ‡å‡†åˆ— schema
                features['messages'] = [{'role': Value(dtype='string'), 'content': Value(dtype='string')}]
                features['images'] = [{'bytes': Value(dtype='binary'), 'path': Value(dtype='string')}]
                features['objects'] = {
                    'ref': Sequence(feature=Value(dtype='string'), length=-1),
                    'bbox': Sequence(feature=Sequence(feature=Value(dtype='float64'), length=-1), length=-1),
                    'bbox_type': Value(dtype='string'),
                    'image_id': Sequence(feature=Value(dtype='int64'), length=-1),
                }
            ArrowWriter.__origin_init__(self, schema, features, *args, **kwargs)  # è°ƒç”¨åŸå§‹æ„é€ 

        ArrowWriter.__origin_init__ = ArrowWriter.__init__  # å¤‡ä»½åŸå§‹ __init__
        ArrowWriter.__init__ = _new_init  # æ³¨å…¥æ–°æ„é€ 
        try:
            yield  # è¿›å…¥è¡¥ä¸ä½œç”¨èŒƒå›´
        finally:
            ArrowWriter.__init__ = ArrowWriter.__origin_init__  # æ¢å¤åŸæ„é€ 
            del ArrowWriter.__origin_init__  # æ¸…ç†å¤‡ä»½å¼•ç”¨

    def _cast_pil_image(self, dataset):
        """å°†å¯è§£ç çš„ Image åˆ—åˆ‡æ¢ä¸ºéè§£ç æ¨¡å¼ï¼Œé¿å… map æ—¶éšå¼è§£ç å¸¦æ¥çš„å¼€é”€ã€‚"""
        features = dataset.features  # å½“å‰ç‰¹å¾å®šä¹‰
        for col in ['images', 'rejected_images']:  # ä¸¤ä¸ªå›¾åƒç›¸å…³åˆ—
            if (col in features and isinstance(features[col], Image) and getattr(features[col], 'decode', False)):
                dataset = dataset.cast_column(col, Image(decode=False))  # å…³é—­ decode æ ‡å¿—
        return dataset  # è¿”å›æ•°æ®é›†

    def __call__(
        self,
        dataset: DATASET_TYPE,
        *,
        num_proc: int = 1,
        load_from_cache_file: bool = True,
        strict: bool = False,
        batch_size: Optional[int] = None,
    ) -> DATASET_TYPE:
        """
        å¯¹ HF æ•°æ®é›†æ‰§è¡Œæ ‡å‡†åŒ–é¢„å¤„ç†ï¼ˆæ”¯æŒå¹¶è¡Œ/ç¼“å­˜/æµå¼ï¼‰ï¼š
        - å¯é€‰é‡‡æ ·ï¼›
        - åˆ—é‡å‘½åä¸å‡†å¤‡ï¼›
        - æ‰¹å¤„ç†æ˜ å°„å¹¶æ•è·å¼‚å¸¸æ ·æœ¬ï¼›
        - å…¼å®¹ `solution` å­—æ®µä¿ç•™ï¼ˆGRPOï¼‰ã€‚

        å‚æ•°
        ----
        - dataset: HF æ•°æ®é›†æˆ–å¯è¿­ä»£æ•°æ®é›†
        - num_proc: å¹¶è¡Œè¿›ç¨‹æ•°ï¼ˆä»… HfDataset ç”Ÿæ•ˆï¼‰
        - load_from_cache_file: æ˜¯å¦ä½¿ç”¨ç¼“å­˜æ–‡ä»¶
        - strict: ä¸¥æ ¼æ¨¡å¼ï¼ˆå¼‚å¸¸å³æŠ›å‡ºï¼‰
        - batch_size: æ˜ å°„æ‰¹å¤§å°ï¼Œé»˜è®¤ 1000ï¼ˆHfDatasetï¼‰æˆ– 16ï¼ˆè¿­ä»£å¼ï¼‰

        è¿”å›
        ----
        - é¢„å¤„ç†åçš„æ•°æ®é›†ï¼ˆä¸è¾“å…¥ç±»å‹ä¸€è‡´ï¼‰
        """
        from ..utils import sample_dataset  # æ•°æ®å­é‡‡æ ·å·¥å…·
        if batch_size is None:  # è®¾ç½®é»˜è®¤æ‰¹å¤§å°
            batch_size = 1000 if isinstance(dataset, HfDataset) else 16
        if self.dataset_sample is not None:  # è‹¥è¦æ±‚é‡‡æ ·
            dataset = sample_dataset(dataset, self.dataset_sample, True, self.random_state)  # é‡‡æ ·åè¿”å›

        map_kwargs = {'batched': True, 'batch_size': batch_size}  # map å…¬å…±å‚æ•°
        if isinstance(dataset, HfDataset):  # å¸¸è§„æ•°æ®é›†æ”¯æŒå¤šè¿›ç¨‹ä¸ç¼“å­˜
            if not load_from_cache_file and is_dist() and not is_master():  # åˆ†å¸ƒå¼ä¸‹éä¸»è¿›ç¨‹å¼ºåˆ¶ä½¿ç”¨ç¼“å­˜
                load_from_cache_file = True
            map_kwargs.update({
                'num_proc': num_proc,
                'load_from_cache_file': load_from_cache_file,
            })
        # compat GRPO: The solution field will be retained.  # å…¼å®¹ä¿ç•™ solution å­—æ®µ
        dataset = RowPreprocessor.get_features_dataset(dataset)  # ç¡®ä¿ features å¯ç”¨
        if 'solution' in dataset.features:  # è‹¥åŒ…å« solution åˆ—
            with safe_ddp_context(None, True):  # DDP å®‰å…¨ä¸Šä¸‹æ–‡
                dataset = dataset.map(lambda x: {'__#solution': x['solution']}, **map_kwargs)  # ä¸´æ—¶ç¼“å­˜ solution
        dataset = self._rename_columns(dataset)  # åº”ç”¨åˆ—é‡å‘½åé€»è¾‘
        dataset = self.prepare_dataset(dataset)  # å­ç±»å‡†å¤‡ï¼ˆä¸‹è½½/ä¿®å¤ï¼‰
        dataset = self._cast_pil_image(dataset)  # è°ƒæ•´å›¾åƒ decode è¡Œä¸º

        ignore_max_length_error = True if isinstance(dataset, HfDataset) and num_proc > 1 else False  # å¤šè¿›ç¨‹å¿½ç•¥é•¿åº¦é”™
        with self._patch_arrow_writer(), safe_ddp_context(None, True):  # å†™å…¥è¡¥ä¸ä¸ DDP å®‰å…¨ç¯å¢ƒ
            try:
                dataset_mapped = dataset.map(
                    self.batched_preprocess,  # æ‰¹å¤„ç†é¢„å¤„ç†å‡½æ•°
                    fn_kwargs={
                        'strict': strict,
                        'ignore_max_length_error': ignore_max_length_error
                    },
                    remove_columns=list(dataset.features.keys()),  # ç§»é™¤åŸåˆ—ï¼Œä»…ä¿ç•™æ–°åˆ—
                    **map_kwargs)
            except NotImplementedError:  # å­ç±»æœªå®ç° preprocess æ—¶è·³è¿‡
                pass
        if isinstance(dataset_mapped, HfDataset) and len(dataset) != len(dataset_mapped):  # è¿‡æ»¤ç»Ÿè®¡
            logger.info(
                f'Dataset filtered, origin length: {len(dataset)}, filtered dataset length: {len(dataset_mapped)}')

        return dataset_mapped  # è¿”å›æ˜ å°„åçš„æ•°æ®é›†


class ResponsePreprocessor(RowPreprocessor):
    """
    å“åº”å¼é¢„å¤„ç†å™¨ï¼šå…¼å®¹æ—©æœŸ ms-swift æ•°æ®æ ¼å¼ï¼Œå°† `system/query/response/history`
    ç»Ÿä¸€è½¬æ¢ä¸ºæ ‡å‡† `messages` åºåˆ—ã€‚
    """

    def __init__(self, *, columns: Optional[Dict[str, str]] = None, **kwargs) -> None:
        """æ‰©å±•åˆ—æ˜ å°„ï¼šå¸¸è§çš„ system/query/response åˆ«åå½’ä¸€åŒ–åˆ°æ ‡å‡†é”®ã€‚"""
        super().__init__(columns=columns, **kwargs)  # åˆå§‹åŒ–åŸºç±»
        system_keys = ['system', 'system_prompt']  # system åˆ«å
        query_keys = ['query', 'prompt', 'input', 'instruction', 'question', 'problem']  # query åˆ«å
        response_keys = ['response', 'answer', 'output', 'targets', 'target', 'answer_key', 'answers', 'solution'
                         ] + ['text', 'completion', 'content']  # response åˆ«å
        for key in system_keys:  # å½’ä¸€åŒ– system
            self.columns[key] = 'system'
        for key in query_keys:  # å½’ä¸€åŒ– query
            self.columns[key] = 'query'
        for key in response_keys:  # å½’ä¸€åŒ– response
            self.columns[key] = 'response'

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        å°† `query/response/history/system` ç»„è£…ä¸ºæ ‡å‡† `messages`ï¼š
        - response æ”¯æŒ list/tupleï¼Œå¹¶å¯é…ç½®éšæœºé€‰å–ï¼›
        - history æ”¯æŒå­—ç¬¦ä¸²å­—é¢é‡è¡¨ç¤ºã€‚
        """
        response = row.pop('response', None)  # å–å‡ºå“åº”å¹¶ä»è¡Œä¸­ç§»é™¤
        if response is not None:
            if isinstance(response, (list, tuple)):  # å“åº”å¯èƒ½æ˜¯å¤šä¸ªå€™é€‰
                from transformers.utils import strtobool  # å­—ç¬¦ä¸²è½¬å¸ƒå°”
                # sometimes response is a list, pick one randomly  # å¯éšæœºé€‰æ‹©å›åº”
                if strtobool(os.environ.get('RANDOM_DATASET_RESPONSE', 'True')):  # å—ç¯å¢ƒå˜é‡æ§åˆ¶
                    response = self.random_state.choice(response)  # éšæœºæŒ‘é€‰
                else:
                    response = response[0]  # å–ç¬¬ä¸€ä¸ª
        history = row.pop('history', None) or []  # å–å†å²å¯¹è¯ï¼Œæ— åˆ™ç©ºåˆ—è¡¨
        query = row.pop('query', None)  # å– query å¹¶ç§»é™¤
        system = row.pop('system', None)  # å– system å¹¶ç§»é™¤
        if isinstance(history, str):  # e.g. "[['query1', 'response1']]"  # å­—ç¬¦ä¸²å½¢å¼å†å²
            history = ast.literal_eval(history)  # å®‰å…¨è§£æ
        history.append([query, response])  # è¿½åŠ å½“å‰è½®

        row.update({'messages': history_to_messages(history, system)})  # è½¬ä¸ºæ ‡å‡† messages
        return row  # è¿”å›


class AlpacaPreprocessor(ResponsePreprocessor):
    """
    å…¼å®¹ Alpaca é£æ ¼æ•°æ®ï¼š`instruction/input/output` -> `query/response` å¹¶ç”Ÿæˆ messagesã€‚
    """

    @classmethod
    def concat_inst_input(cls, instruction, input_):
        """æ‹¼æ¥ `instruction` ä¸ `input` ç”Ÿæˆ `query`ï¼Œè‹¥ä¸€æ–¹ä¸ºç©ºåˆ™å–å¦ä¸€æ–¹ã€‚"""
        if instruction and input_:  # ä¸¤è€…çš†æœ‰
            query = f'{instruction}\n{input_}'  # ä»¥æ¢è¡Œæ‹¼æ¥
        else:
            query = instruction or input_  # å–å­˜åœ¨çš„ä¸€æ–¹
        assert isinstance(query, str), f'query: {query}'  # æ–­è¨€ä¸ºå­—ç¬¦ä¸²
        return query  # è¿”å›æŸ¥è¯¢

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ä» `instruction/input/output` æ„é€  `query/response`ï¼Œå¹¶è°ƒç”¨çˆ¶ç±»ç”Ÿæˆ messagesã€‚"""
        instruction = row.pop('instruction', None)  # å–å¹¶ç§»é™¤ instruction
        input_ = row.pop('input', None)  # å–å¹¶ç§»é™¤ input
        output = row.pop('output', None)  # å–å¹¶ç§»é™¤ output
        if output is not None:  # å­˜åœ¨è¾“å‡ºåˆ™ä½œä¸ºå“åº”
            row['response'] = output
        row['query'] = self.concat_inst_input(instruction, input_)  # ç”Ÿæˆ query
        return super().preprocess(row)  # äº¤ç”±çˆ¶ç±»ç”Ÿæˆ messages


def default_repair_messages(s: Union[str, Any]) -> Any:
    """é»˜è®¤ä¿®è¡¥å‡½æ•°ï¼šè‹¥è¾“å…¥ä¸ºå­—ç¬¦ä¸²ï¼Œåˆ™ç”¨ `ast.literal_eval` è§£æä¸º Python å¯¹è±¡ã€‚"""
    if isinstance(s, str):  # å­—ç¬¦ä¸²å½¢å¼
        return ast.literal_eval(s)  # å®‰å…¨è§£æ
    return s  # éå­—ç¬¦ä¸²ç›´æ¥è¿”å›


class MessagesPreprocessor(RowPreprocessor):
    """
    æ¶ˆæ¯å¼é¢„å¤„ç†å™¨ï¼šå…¼å®¹å„ç§æ¶ˆæ¯é”®åä¸è§’è‰²åï¼Œä¿®å¤å¹¶å¯¹é½ä¸ºæ ‡å‡† `messages` åºåˆ—ã€‚

    å…³é”®å‚æ•°
    -------
    - role_key/content_key: æ¶ˆæ¯ä¸­è§’è‰²ä¸å†…å®¹é”®åï¼ˆé»˜è®¤è‡ªåŠ¨åŒ¹é…ï¼‰ï¼›
    - user_role/assistant_role/system_role: è§’è‰²å‘½ååˆ«åï¼ˆ'human'/'gpt' ç­‰ï¼‰ï¼›
    - columns: è¾“å…¥åˆ—åˆ°æ ‡å‡†åˆ—çš„æ˜ å°„ï¼›
    - repair_messages: ä¿®è¡¥å‡½æ•°ï¼Œæ”¯æŒå­—ç¬¦ä¸²è¡¨è¾¾çš„æ¶ˆæ¯å†å²ï¼›
    - inner_key: å½“ messages æ˜¯åµŒå¥—ç»“æ„æ—¶å–å…¶å­é”®ã€‚
    """

    def __init__(
            self,
            *,
            # If set to None, automatic matching will be performed.
            role_key: Optional[str] = None,  # 'role', 'from'
            content_key: Optional[str] = None,  # 'content', 'value'
            user_role: Optional[str] = None,  # 'user', 'human'
            assistant_role: Optional[str] = None,  # 'assistant', 'gpt', 'bot'
            system_role: str = 'system',
            # 'conversation', 'conversations' -> 'messages'
            columns: Optional[Dict[str, str]] = None,
            repair_messages: Callable[[Union[str, List[Dict[str, str]]]],
                                      Optional[List[Dict[str, str]]]] = default_repair_messages,
            inner_key: Optional[str] = None,
            **kwargs):
        super().__init__(columns=columns, **kwargs)  # åˆå§‹åŒ–çˆ¶ç±»
        self.role_keys = ['role', 'from'] if role_key is None else [role_key]  # è§’è‰²é”®å€™é€‰
        self.content_keys = ['content', 'value'] if content_key is None else [content_key]  # å†…å®¹é”®å€™é€‰
        self.user_roles = ['user', 'human'] if user_role is None else [user_role]  # ç”¨æˆ·è§’è‰²åˆ«å
        self.assistant_roles = ['assistant', 'gpt', 'bot'] if assistant_role is None else [assistant_role]  # åŠ©æ‰‹åˆ«å
        self.tool_call_roles = ['function_call']  # å·¥å…·è°ƒç”¨è§’è‰²åˆ«å
        self.tool_response_roles = ['function_response', 'observation', 'observations']  # å·¥å…·å“åº”åˆ«å

        self.system_role = system_role  # ç³»ç»Ÿè§’è‰²å
        self.repair_messages = repair_messages  # æ¶ˆæ¯ä¿®è¡¥å‡½æ•°
        self.inner_key = inner_key  # åµŒå¥—æ¶ˆæ¯é”®

        message_keys = ['messages', 'conversation', 'conversations']  # å¸¸è§æ¶ˆæ¯é”®
        for key in message_keys:  # å½’ä¸€åŒ–ä¸º messages
            self.columns[key] = 'messages'
        # sharegptq  # ç³»ç»Ÿæç¤ºé”®å½’ä¸€åŒ–
        system_keys = ['system', 'system_prompt']
        if system_role not in system_keys:  # è¡¥å……è‡ªå®šä¹‰ç³»ç»Ÿé”®
            system_keys.append(system_role)
        for key in system_keys:  # å½’ä¸€åŒ–ä¸º system
            self.columns[key] = 'system'

    @staticmethod
    def _is_sharegpt_format(message: Dict[str, str]) -> bool:
        """åˆ¤æ–­æ¶ˆæ¯æ˜¯å¦ä¸º ShareGPT é”®é£æ ¼ï¼ˆæ—  role/contentï¼‰ã€‚"""
        if 'role' in message or 'content' in message:  # å«æ ‡å‡†é”®åˆ™ä¸æ˜¯ ShareGPT é£æ ¼
            return False
        return True  # å¦åˆ™æ˜¯

    def sharegpt_to_messages(self, messages: List[Dict[str, str]], system: Optional[str]) -> List[Dict[str, str]]:
        """å°† ShareGPT é£æ ¼æ¶ˆæ¯è½¬ä¸ºæ ‡å‡† `messages` åˆ—è¡¨ã€‚"""
        self._to_std_key(messages, 'user', self.user_roles)  # ç»Ÿä¸€ç”¨æˆ·é”®
        self._to_std_key(messages, 'assistant', self.assistant_roles)  # ç»Ÿä¸€åŠ©æ‰‹é”®
        new_messages = []  # è¾“å‡ºåˆ—è¡¨
        if system is not None:  # æœ‰ç³»ç»Ÿæç¤ºåˆ™ç½®äºé¦–ä½
            new_messages.append({'role': 'system', 'content': system})
        for message in messages:  # äº¤æ›¿åŠ å…¥ user/assistant
            user_message = {'role': 'user', 'content': message['user']}
            assistant_message = {'role': 'assistant', 'content': message['assistant']}
            new_messages.append(user_message)
            new_messages.append(assistant_message)
        return new_messages  # è¿”å›

    def to_std_messages(self, messages: List[Dict[str, str]], system: Optional[str]) -> None:
        """å°±åœ°å°†æ··åˆè§’è‰²åå¯¹é½ä¸ºæ ‡å‡†è§’è‰²åï¼Œå¹¶åœ¨å¿…è¦æ—¶æ’å…¥ system æ¶ˆæ¯ã€‚"""
        if messages[0]['role'] == self.system_role:  # é¦–æ¡ç³»ç»Ÿæ¶ˆæ¯è§’è‰²åå¯¹é½
            messages[0]['role'] = 'system'
        elif system is not None:  # å¦åˆ™è‹¥æä¾› system æ–‡æœ¬ï¼Œåˆ™æ’å…¥é¦–æ¡
            messages.insert(0, {'role': 'system', 'content': system})
        for message in messages:  # éå†å¯¹é½è§’è‰²
            role = message['role']
            if role in self.user_roles:
                message['role'] = 'user'
            elif role in self.assistant_roles:
                message['role'] = 'assistant'
            elif role.replace('-', '_') in self.tool_call_roles:  # function-call åˆ«å
                message['role'] = 'tool_call'
            elif role.replace('-', '_') in self.tool_response_roles:  # function-response åˆ«å
                message['role'] = 'tool_response'

    @staticmethod
    def _to_std_key(messages: List[Dict[str, str]], std_key: str, optional_keys: List[str]) -> None:
        """å°†æ¶ˆæ¯çš„å¯é€‰é”®ä¹‹ä¸€æ˜ å°„ä¸ºæ ‡å‡†é”® `std_key`ï¼ˆå¦‚ user/assistantï¼‰ã€‚"""
        for message in messages:  # éå†æ¯æ¡æ¶ˆæ¯
            for key in optional_keys:  # å°è¯•æ¯ä¸ªå€™é€‰é”®
                if key in message:  # å‘½ä¸­åˆ™æ›¿æ¢
                    message[std_key] = message.pop(key)

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """ä¿®å¤ä¸æ ‡å‡†åŒ– `messages`ï¼šæ”¯æŒ rejected_messagesã€åµŒå¥—é”®ä¸å­—ç¬¦ä¸²å†å²ã€‚"""
        if 'rejected_messages' in row:  # æå‰è§„èŒƒè´Ÿæ ·æœ¬æ¶ˆæ¯
            row['rejected_messages'] = MessagesPreprocessor.preprocess(
                self, {'messages': row['rejected_messages']})['messages']
        messages = row['messages']  # å–æ¶ˆæ¯
        if self.inner_key is not None:  # å†…åµŒé”®åœºæ™¯
            messages = messages[self.inner_key]
        messages: Optional[List[Dict[str, str]]] = self.repair_messages(messages)  # ä¿®è¡¥æ¶ˆæ¯ï¼ˆå­—ç¬¦ä¸² -> åˆ—è¡¨ï¼‰
        if not messages or isinstance(messages, str):  # ä¿®è¡¥å¤±è´¥åˆ™è·³è¿‡
            return
        self._to_std_key(messages, 'role', self.role_keys)  # å¯¹é½ role é”®
        self._to_std_key(messages, 'content', self.content_keys)  # å¯¹é½ content é”®
        system = row.pop('system', None)  # å–å‡º system æ–‡æœ¬
        if self._is_sharegpt_format(messages[0]):  # ShareGPT é£æ ¼
            messages = self.sharegpt_to_messages(messages, system)
        else:
            self.to_std_messages(messages, system)  # inplace æ ‡å‡†åŒ–
        row['messages'] = messages  # å†™å›æ¶ˆæ¯
        return row  # è¿”å›


class ClsPreprocessor(ResponsePreprocessor):
    """
    åˆ†ç±»é¢„å¤„ç†å™¨ï¼šåœ¨å“åº”å¼é¢„å¤„ç†åŸºç¡€ä¸Šï¼Œå°† `label` è½¬ä¸ºæ•´å‹ã€‚
    """

    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """è°ƒç”¨çˆ¶ç±»ç”Ÿæˆ messagesï¼Œéšåå°† `label` è½¬ä¸ºæ•´å‹ã€‚"""
        res = super().preprocess(row)  # å…ˆç”Ÿæˆ messages
        res['label'] = int(res['label'])  # label -> int
        return res  # è¿”å›


class AutoPreprocessor:
    """
    é¢„å¤„ç†å™¨è‡ªåŠ¨é€‰æ‹©å™¨ï¼šæ ¹æ®æ•°æ®é›†ç‰¹å¾è‡ªåŠ¨é€‰æ‹© `Messages/Alpaca/Response` é¢„å¤„ç†å™¨ã€‚

    ç”¨æ³•
    ---
    >>> auto = AutoPreprocessor(columns={'instruction': 'instruction'})
    >>> ds2 = auto(ds)
    """

    def __init__(self, *, columns: Optional[Dict[str, str]] = None, **kwargs) -> None:
        self.columns = columns or {}  # éœ€è¦æå‰é‡å‘½åçš„åˆ—æ˜ å°„
        self.kwargs = kwargs  # ä¼ é€’ç»™å…·ä½“é¢„å¤„ç†å™¨çš„å…¶ä»–å‚æ•°

    def _get_preprocessor(self, dataset: DATASET_TYPE) -> RowPreprocessor:
        """æ ¹æ® features é€‰æ‹©æœ€é€‚åˆçš„é¢„å¤„ç†å™¨ç±»å‹ã€‚"""
        features = dataset.features  # ç‰¹å¾å­—å…¸
        for key in ['conversation', 'conversations', 'messages']:  # è‹¥æœ‰æ¶ˆæ¯ç±»å­—æ®µ
            if key in features:
                return MessagesPreprocessor(**self.kwargs)  # ä½¿ç”¨æ¶ˆæ¯é¢„å¤„ç†
        if 'instruction' in features and 'input' in features:  # Alpaca é£æ ¼
            return AlpacaPreprocessor(**self.kwargs)
        return ResponsePreprocessor(**self.kwargs)  # é»˜è®¤å“åº”å¼

    def __call__(
        self,
        dataset: DATASET_TYPE,
        *,
        num_proc: int = 1,
        load_from_cache_file: bool = True,
        strict: bool = False,
    ) -> DATASET_TYPE:
        """å…ˆå®‰å…¨é‡å‘½ååˆ—ï¼Œå†é€‰æ‹©å¹¶è°ƒç”¨å…·ä½“é¢„å¤„ç†å™¨å®Œæˆæ•°æ®æ ‡å‡†åŒ–ã€‚"""
        dataset = RowPreprocessor.safe_rename_columns(dataset, self.columns)  # å…ˆåšåˆ—å¯¹é½
        preprocessor = self._get_preprocessor(dataset)  # è‡ªåŠ¨é€‰æ‹©å…·ä½“é¢„å¤„ç†å™¨
        return preprocessor(dataset, num_proc=num_proc, load_from_cache_file=load_from_cache_file, strict=strict)  # æ‰§è¡Œ
