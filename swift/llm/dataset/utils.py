"""
æ¨¡å—åŠŸèƒ½æ¦‚è¿°ï¼š
æœ¬æ¨¡å—æä¾›å›´ç»•å¤§è¯­è¨€æ¨¡å‹ï¼ˆLLMï¼‰è®­ç»ƒ/æ¨ç†çš„æ•°æ®å¤„ç†å®ç”¨å·¥å…·ä¸æ•°æ®é›†å°è£…ï¼š
- sample_dataset: å¯¹HuggingFaceæ•°æ®é›†æŒ‰æ ·æœ¬æ•°è¿›è¡ŒæŠ½æ ·ï¼ˆæ”¯æŒé‡å¤é‡‡æ ·ä¸éšæœºæ‰“ä¹±ï¼‰ã€‚
- LazyLLMDataset: æƒ°æ€§tokenize/encodeçš„æ•°æ®é›†ï¼Œè®­ç»ƒæ—¶é‡åˆ°åæ ·æœ¬å¯è·³è¿‡ä»¥ä¿è¯ç¨³å®šæ€§ã€‚
- calculate_matched_group: ä½¿ç”¨è£…ç®±ç®—æ³•å°†è‹¥å¹²æ ·æœ¬æŒ‰é•¿åº¦ä¸Šé™è¿›è¡Œåˆ†ç»„åŒ¹é…ã€‚
- PackingDataset: é¢„è®¡ç®—å¹¶ç¼“å­˜â€œæ‰“åŒ…åçš„ç´¢å¼•â€çš„Datasetï¼Œ__getitem__æŒ‰ç»„å–æ ·å¹¶packingã€‚
- IterablePackingDataset: åŸºäºå¤šè¿›ç¨‹+é˜Ÿåˆ—çš„â€œè¾¹å–è¾¹packâ€å¯è¿­ä»£æ•°æ®é›†ï¼Œé€‚åˆå¤§è§„æ¨¡æµå¼æ•°æ®ã€‚
- EncodePreprocessor: è¡Œçº§é¢„å¤„ç†å™¨ï¼Œå°è£…æ¨¡æ¿encodeï¼›å¯é€‰æ‹©ä»…å†™å…¥lengthä¾›åç»­packingã€‚

ç®€è¦ç¤ºä¾‹ï¼š
>>> from datasets import Dataset as HfDataset
>>> ds = HfDataset.from_dict({"text": ["hello", "world"]})
>>> ds2 = sample_dataset(ds, dataset_sample=3, shuffle=True)
"""

# ç‰ˆæƒå£°æ˜ï¼šé˜¿é‡Œå·´å·´åŠå…¶é™„å±å…¬å¸ä¿ç•™æ‰€æœ‰æƒåˆ©
# è¯¥è¡Œç”¨äºæ ‡æ³¨æœ¬æ–‡ä»¶çš„ç‰ˆæƒä¿¡æ¯ä¸å½’å±
# Copyright (c) Alibaba, Inc. and its affiliates.

# å¯¼å…¥å¤šè¿›ç¨‹æ¨¡å—å¹¶ç®€å†™ä¸ºmpï¼šç”¨äºåˆ›å»ºå­è¿›ç¨‹å’Œè¿›ç¨‹é—´é€šä¿¡é˜Ÿåˆ—
import multiprocessing as mp
# ä»typingå¯¼å…¥ç±»å‹æç¤ºå·¥å…·ï¼šç”¨äºé™æ€ç±»å‹æ£€æŸ¥ä¸æ›´æ¸…æ™°çš„æ¥å£å®šä¹‰
from typing import TYPE_CHECKING, Any, Callable, Dict, Optional, Union

# å¯¼å…¥æ•°å€¼è®¡ç®—åº“numpyï¼šç”¨äºç´¢å¼•é‡‡æ ·ã€æ‰“ä¹±ä¸æ•°ç»„æ‹¼æ¥ç­‰æ“ä½œ
import numpy as np
# å¯¼å…¥åˆ†å¸ƒå¼é€šä¿¡åº“torch.distributedï¼šç”¨äºå¤šè¿›ç¨‹/å¤šæœºç¯å¢ƒä¸‹çš„å¯¹è±¡å¹¿æ’­
import torch.distributed as dist
# ä»datasetsåº“å¯¼å…¥HuggingFaceæ•°æ®é›†ç±»å‹å¹¶é‡å‘½åä¸ºHfDatasetï¼šç»Ÿä¸€ç±»å‹æ ‡æ³¨
from datasets import Dataset as HfDataset
# ä»PyTorchå¯¼å…¥Datasetä¸IterableDatasetï¼šå®šä¹‰è‡ªå®šä¹‰æ•°æ®é›†ä¸å¯è¿­ä»£æ•°æ®é›†
from torch.utils.data import Dataset, IterableDataset
# å¯¼å…¥tqdmè¿›åº¦æ¡ï¼šç”¨äºæ˜¾ç¤ºæ‰“åŒ…è¿›åº¦
from tqdm import tqdm

# ä»swift.utilså¯¼å…¥å·¥å…·å‡½æ•°ï¼šget_loggerè·å–æ—¥å¿—å™¨ï¼Œis_dist/is_masterç”¨äºåˆ†å¸ƒå¼çŠ¶æ€åˆ¤æ–­
from swift.utils import get_logger, is_dist, is_master
# ä»ä¸Šçº§templateæ¨¡å—å¯¼å…¥MaxLengthErrorï¼šç”¨äºåŒºåˆ†å¯å¿½ç•¥çš„è¶…é•¿é”™è¯¯
from ..template import MaxLengthError
# ä»å½“å‰åŒ…çš„preprocessoræ¨¡å—å¯¼å…¥RowPreprocessorï¼šå®šä¹‰è¡Œçº§é¢„å¤„ç†åŸºç±»
from .preprocessor import RowPreprocessor

# åˆå§‹åŒ–æ¨¡å—çº§æ—¥å¿—å™¨ï¼šä¾›æœ¬æ¨¡å—å†…éƒ¨ç»Ÿä¸€æ‰“å°æ—¥å¿—ä½¿ç”¨
logger = get_logger()

# ä»…åœ¨ç±»å‹æ£€æŸ¥é˜¶æ®µå¯¼å…¥ä»¥é¿å…å¾ªç¯ä¾èµ–ä¸è¿è¡Œæ—¶å¼€é”€
if TYPE_CHECKING:
    # ä»…ç”¨äºç±»å‹æç¤ºçš„Templateç±»å¼•ç”¨ï¼ˆè¿è¡Œæ—¶ä¸ä¼šçœŸæ­£å¯¼å…¥ï¼‰
    from swift.llm import Template


# å®šä¹‰å‡½æ•°ï¼šæŒ‰ç»™å®šæ ·æœ¬æ•°æŠ½æ ·HFæ•°æ®é›†ï¼ˆæ”¯æŒé‡å¤é‡‡æ ·ä¸éšæœºæ‰“ä¹±ï¼‰
def sample_dataset(
    dataset: HfDataset,
    dataset_sample: Optional[int],
    shuffle: bool = True,
    random_state: Optional[np.random.RandomState] = None,
) -> HfDataset:
    """
    å‡½æ•°åŠŸèƒ½ï¼š
        æ ¹æ®æœŸæœ›æ ·æœ¬æ•°å¯¹ç»™å®šHFæ•°æ®é›†è¿›è¡ŒæŠ½æ ·ã€‚è‹¥æœŸæœ›æ•°å¤§äºæ•°æ®é›†é•¿åº¦ï¼Œä½¿ç”¨é‡å¤é‡‡æ ·ï¼›
        å¯é€‰æ˜¯å¦å…ˆæ‰“ä¹±å†å–ä½™æ•°æ ·æœ¬ã€‚

    å…¥å‚ï¼š
        dataset (HfDataset): HuggingFaceæ•°æ®é›†å®ä¾‹ï¼ˆä¸æ”¯æŒå¯è¿­ä»£æµå¼æ•°æ®é›†ï¼‰ã€‚
        dataset_sample (Optional[int]): æœŸæœ›æŠ½æ ·å¾—åˆ°çš„æ ·æœ¬æ€»æ•°ï¼›ä¸ºNoneæ—¶è¿”å›åŸæ•°æ®é›†ã€‚
        shuffle (bool): å½“éœ€è¦è¡¥å……ä½™æ•°æ ·æœ¬æ—¶ï¼Œæ˜¯å¦å¯¹ç´¢å¼•è¿›è¡Œéšæœºæ‰“ä¹±åå†æˆªå–ã€‚
        random_state (Optional[np.random.RandomState]): æŒ‡å®šéšæœºçŠ¶æ€ä»¥ä¿è¯å¯å¤ç°ï¼›
            è‹¥ä¸ºNoneä¸”éœ€è¦shuffleï¼Œå°†å†…éƒ¨åˆ›å»ºæ–°çš„éšæœºçŠ¶æ€ã€‚

    è¿”å›å€¼ï¼š
        HfDataset: æŠ½æ ·åçš„æ–°æ•°æ®é›†è§†å›¾ï¼ˆé€šè¿‡selectç´¢å¼•å®ç°ï¼Œä¸ä¼šæ‹·è´åŸå§‹æ•°æ®ï¼‰ã€‚

    ç¤ºä¾‹ï¼š
        >>> sampled = sample_dataset(dataset, dataset_sample=1000, shuffle=True)
    """
    # è‹¥æœªæŒ‡å®šæŠ½æ ·æ•°ï¼Œç›´æ¥è¿”å›åŸæ•°æ®é›†
    if dataset_sample is None:
        return dataset

    # è®¡ç®—æ•´å€æ•°é‡å¤æ¬¡æ•°ï¼šå¯æ•´é™¤éƒ¨åˆ†é€šè¿‡é‡å¤ç´¢å¼•å®ç°
    n_repeat_sample = dataset_sample // len(dataset)
    # è®¡ç®—å‰©ä½™æ ·æœ¬æ•°ï¼šç”¨äºè¡¥é½ä¸è¶³æ•´å€æ•°çš„éƒ¨åˆ†
    n_remain_sample = dataset_sample % len(dataset)
    # è‹¥æ—¢æœ‰æ•´å€æ•°é‡å¤åˆæœ‰ä½™æ•°ï¼Œæç¤ºå°†æ‰§è¡Œé‡å¤é‡‡æ ·ï¼ˆæ—¥å¿—çº§åˆ«warningï¼‰
    if n_repeat_sample >= 1 and n_remain_sample >= 1:
        logger.warning(
            f'dataset_sample:{dataset_sample} is greater than len(dataset):{len(dataset)}, '
            'repeated sampling will be performed.'
        )
    # ç”Ÿæˆæ•´å€æ•°é‡å¤çš„åŸºç¡€ç´¢å¼•åºåˆ—ï¼šnp.tileæŒ‰é‡å¤æ¬¡æ•°å¤åˆ¶ç´¢å¼•èŒƒå›´
    idx = np.tile(range(len(dataset)), n_repeat_sample)
    # è‹¥ä»æœ‰ä½™æ•°æ ·æœ¬éœ€è¦è¡¥é½
    if n_remain_sample >= 1:
        # å½“éœ€è¦éšæœºæ‰“ä¹±æ—¶
        if shuffle:
            # è‹¥æœªæä¾›éšæœºçŠ¶æ€ï¼Œåˆ™åˆ›å»ºæ–°çš„ä»¥ä¿è¯éšæœºæ€§
            if random_state is None:
                random_state = np.random.RandomState()
            # ä»æ‰“ä¹±åçš„å…¨ç´¢å¼•ä¸­æˆªå–å‰n_remain_sampleä¸ªä½œä¸ºè¡¥å……ç´¢å¼•
            idx_remain = random_state.permutation(len(dataset))[:n_remain_sample]
        else:
            # ä¸æ‰“ä¹±æ—¶ï¼Œç›´æ¥ä½¿ç”¨ä»0å¼€å§‹çš„é¡ºåºç´¢å¼•è¡¥é½ä½™æ•°
            idx_remain = np.arange(n_remain_sample)
        # å°†æ•´å€æ•°ç´¢å¼•ä¸ä½™æ•°ç´¢å¼•æ‹¼æ¥å¾—åˆ°æœ€ç»ˆç´¢å¼•åºåˆ—
        idx = np.concatenate([idx, idx_remain])
    # ä½¿ç”¨HFçš„selectæ–¹æ³•æ ¹æ®ç´¢å¼•é€‰æ‹©å­é›†ï¼Œå½¢æˆæŠ½æ ·åçš„æ•°æ®é›†è§†å›¾
    dataset = dataset.select(idx)
    # è¿”å›æŠ½æ ·åçš„æ•°æ®é›†
    return dataset


# å®šä¹‰æƒ°æ€§ç¼–ç æ•°æ®é›†ï¼šæŒ‰éœ€å¯¹åŸå§‹æ ·æœ¬è¿›è¡Œencodeï¼Œå¤±è´¥æ—¶åœ¨è®­ç»ƒä¸­è·³è¿‡åæ ·æœ¬
class LazyLLMDataset(Dataset):
    """
    ç±»åŠŸèƒ½ï¼š
        åœ¨__getitem__æ—¶æ‰å¯¹æ ·æœ¬è¿›è¡Œencode/tokenizeï¼Œè‹¥ç¼–ç å¤±è´¥ï¼ˆä¾‹å¦‚æ¨¡æ¿å¼‚å¸¸ï¼‰ï¼Œ
        éstrictæ¨¡å¼ä¸‹ä¼šå°è¯•å¤šæ¬¡å¹¶è·³è¿‡åæ ·æœ¬ï¼Œé¿å…ä¸­æ–­è®­ç»ƒã€‚

    å…³é”®å±æ€§ï¼š
        dataset (HfDataset): åŸå§‹HFæ•°æ®é›†ã€‚
        encode_func (Callable): å¯¹å•æ¡æ ·æœ¬è¿›è¡Œç¼–ç çš„å‡½æ•°ï¼Œéœ€æ”¯æŒreturn_lengthå‚æ•°ã€‚
        n_try_fetch (int): æœ€å¤§å°è¯•æ¬¡æ•°ï¼›strict=Trueæ—¶å›ºå®šä¸º1ã€‚
        strict (bool): ä¸¥æ ¼æ¨¡å¼ï¼Œå‡ºé”™å³æŠ›å¼‚å¸¸ï¼›å¦åˆ™è·³è¿‡åæ ·æœ¬ã€‚
        random_state (np.random.RandomState): ç”¨äºéšæœºé€‰æ‹©å¤‡é€‰æ ·æœ¬ã€‚
        traceback_limit (int): æœ€å¤šæ‰“å°å‡ æ¬¡è¯¦ç»†å †æ ˆï¼Œé¿å…åˆ·å±ã€‚
    """

    # æ„é€ å‡½æ•°ï¼šä¿å­˜é…ç½®å¹¶åˆå§‹åŒ–å†…éƒ¨çŠ¶æ€
    def __init__(
        self,
        dataset: HfDataset,
        encode_func: Callable[[Dict[str, Any]], Dict[str, Any]],
        *,
        n_try_fetch: int = 10,
        strict: bool = False,
        random_state: Optional[Union[np.random.RandomState, int]] = None,
        traceback_limit: int = 10,
    ) -> None:
        """
        å‡½æ•°åŠŸèƒ½ï¼š
            åˆå§‹åŒ–æƒ°æ€§æ•°æ®é›†ï¼Œé…ç½®ç¼–ç å‡½æ•°ã€é‡è¯•ç­–ç•¥ä¸éšæœºé€‰æ‹©ç­–ç•¥ã€‚

        å…¥å‚ï¼š
            dataset (HfDataset): åŸå§‹HFæ•°æ®é›†ã€‚
            encode_func (Callable): ç¼–ç å‡½æ•°ï¼Œç­¾åå½¢å¦‚ f(row, return_length=True) -> Dictã€‚
            n_try_fetch (int): æ¯æ¬¡getitemæœ€å¤šå°è¯•æ¬¡æ•°ï¼ˆstrict=Trueæ—¶å°†è¢«ç½®ä¸º1ï¼‰ã€‚
            strict (bool): æ˜¯å¦ä¸¥æ ¼æ¨¡å¼ï¼›ä¸¥æ ¼æ¨¡å¼ä¸‹é¦–æ¬¡å¤±è´¥å³æŠ›å‡ºå¼‚å¸¸ã€‚
            random_state (Optional[Union[np.random.RandomState, int]]): éšæœºçŠ¶æ€æˆ–ç§å­ã€‚
            traceback_limit (int): æœ€å¤šæ‰“å°çš„å¼‚å¸¸å †æ ˆæ¬¡æ•°ä¸Šé™ã€‚

        è¿”å›å€¼ï¼š
            None

        ç¤ºä¾‹ï¼š
            >>> ds = LazyLLMDataset(dataset, encode_func, n_try_fetch=5, strict=False)
        """
        # ä¿å­˜åŸå§‹æ•°æ®é›†å¼•ç”¨
        self.dataset = dataset
        # ä¿å­˜ç”¨æˆ·æä¾›çš„ç¼–ç å‡½æ•°
        self.encode_func = encode_func

        # è‹¥ä¸ºä¸¥æ ¼æ¨¡å¼ï¼Œä»…å…è®¸å°è¯•1æ¬¡ï¼›å¦åˆ™é™å®šæœ€å¤§å°è¯•æ¬¡æ•°ä¸è¶…è¿‡æ•°æ®é›†é•¿åº¦
        n_try_fetch = 1 if strict else min(n_try_fetch, len(self.dataset))
        # åŸºæœ¬æ ¡éªŒï¼šå°è¯•æ¬¡æ•°è‡³å°‘ä¸º1
        assert n_try_fetch >= 1
        # ä¿å­˜ä¸¥æ ¼æ¨¡å¼æ ‡å¿—
        self.strict = strict
        # ä¿å­˜æœ€ç»ˆçš„æœ€å¤§å°è¯•æ¬¡æ•°
        self.n_try_fetch = n_try_fetch

        # å½’ä¸€åŒ–random_stateï¼šè‹¥ä¸æ˜¯RandomStateå®ä¾‹ï¼Œåˆ™ç”¨å…¶æ„é€ ä¸€ä¸ªå®ä¾‹ï¼ˆæ”¯æŒä¼ å…¥seedæˆ–Noneï¼‰
        if not isinstance(random_state, np.random.RandomState):
            random_state = np.random.RandomState(random_state)
        # ä¿å­˜éšæœºçŠ¶æ€
        self.random_state = random_state

        # ä¿å­˜æœ€å¤šæ‰“å°å¼‚å¸¸å †æ ˆæ¬¡æ•°
        self.traceback_limit = traceback_limit
        # å½“å‰å·²æ‰“å°å¼‚å¸¸å †æ ˆçš„æ¬¡æ•°è®¡æ•°å™¨
        self._traceback_counter = 0
        # è½®è¯¢å¤‡é€‰ç´¢å¼•ç”¨çš„æ¸¸æ ‡
        self._idx = 0
        # å°†æ•°æ®é›†é•¿åº¦èŒƒå›´è¿›è¡Œéšæœºæ’åˆ—ï¼Œè½¬ä¸ºåˆ—è¡¨ä¾›è½®è¯¢ä½¿ç”¨
        self._idx_list = self.random_state.permutation(len(self.dataset)).tolist()

    # è¯»å–å•æ¡æ ·æœ¬ï¼šè‹¥å¤±è´¥åˆ™åœ¨é™åˆ¶å†…é‡è¯•å…¶å®ƒéšæœºæ ·æœ¬
    def __getitem__(self, idx: int) -> Dict[str, Any]:
        """
        å‡½æ•°åŠŸèƒ½ï¼š
            è¿”å›ç´¢å¼•ä¸ºidxçš„ç¼–ç åæ ·æœ¬ï¼›è‹¥ç¼–ç å¤±è´¥ä¸”éä¸¥æ ¼æ¨¡å¼ï¼Œåˆ™ä»é¢„ç”Ÿæˆçš„éšæœºç´¢å¼•åˆ—è¡¨
            ä¸­æŒ‰è½®è¯¢æ–¹å¼å°è¯•å…¶å®ƒæ ·æœ¬ï¼Œæœ€å¤šå°è¯•n_try_fetchæ¬¡ã€‚

        å…¥å‚ï¼š
            idx (int): è¯·æ±‚çš„æ ·æœ¬ç´¢å¼•ï¼›å½“ä¸ºstræ—¶ï¼Œç›´æ¥èµ°HFæ•°æ®é›†çš„é”®è®¿é—®ã€‚

        è¿”å›å€¼ï¼š
            Dict[str, Any]: ç¼–ç åçš„æ ·æœ¬å­—å…¸ï¼Œéœ€åŒ…å«lengthç­‰å­—æ®µã€‚

        ç¤ºä¾‹ï¼š
            >>> item = ds[0]
        """
        # è‹¥ä¼ å…¥çš„æ˜¯å­—ç¬¦ä¸²ç´¢å¼•ï¼Œåˆ™ç›´æ¥å§”æ‰˜HFæ•°æ®é›†æŒ‰é”®è®¿é—®è¿”å›åŸå§‹æ•°æ®
        if isinstance(idx, str):
            return self.dataset[idx]
        # å°è¯•æœ€å¤šn_try_fetchæ¬¡
        for i in range(self.n_try_fetch):
            # è®°å½•æœ¬æ¬¡å°è¯•æ¬¡æ•°ï¼ˆç”¨äºåç»­åˆ¤æ–­æ˜¯å¦æœ€åä¸€æ¬¡ï¼‰
            n_try = i
            # ç¬¬ä¸€æ¬¡ä½¿ç”¨ç”¨æˆ·è¯·æ±‚çš„ç´¢å¼•
            if i == 0:
                i = idx
            else:
                # åç»­å°è¯•ä½¿ç”¨éšæœºæ’åˆ—åˆ—è¡¨ä¸­çš„ç´¢å¼•ï¼Œå¹¶è½®è¯¢æ¨è¿›æ¸¸æ ‡
                i = self._idx_list[self._idx]
                self._idx = (self._idx + 1) % len(self.dataset)
            # ä»åŸå§‹æ•°æ®é›†ä¸­å–å‡ºè¯¥æ¡æ•°æ®
            data = self.dataset[i]
            try:
                # è°ƒç”¨ç¼–ç å‡½æ•°è¿›è¡Œç¼–ç ï¼Œå¹¶å¼ºåˆ¶è¦æ±‚è¿”å›é•¿åº¦ä¿¡æ¯
                return self.encode_func(data, return_length=True)
            except Exception:
                # è‹¥å·²è¾¾æœ€åä¸€æ¬¡å°è¯•ï¼Œæˆ–å¤„äºä¸¥æ ¼æ¨¡å¼ï¼Œåˆ™æŠ›å‡ºå¼‚å¸¸
                if n_try == self.n_try_fetch - 1 or self.strict:
                    if self.strict:
                        logger.warning('To avoid errors, you can pass `strict=False`.')
                    raise
                # è‹¥å…è®¸æ‰“å°å †æ ˆä¸”æœªè¶…è¿‡æ¬¡æ•°ä¸Šé™ï¼Œåˆ™æ‰“å°ä¸€æ¬¡å¹¶é€’å¢è®¡æ•°
                if self.traceback_limit is not None and self._traceback_counter < self.traceback_limit:
                    import traceback
                    logger.info(traceback.format_exc())
                    logger.warning(
                        'ğŸ‘†ğŸ‘†ğŸ‘†There are errors in the template.encode, '
                        'and another piece of data will be randomly selected.'
                    )
                    self._traceback_counter += 1

        # å¤šæ¬¡å°è¯•åä¾ç„¶å¤±è´¥ï¼Œç»™å‡ºæ›´å…·æ“ä½œæ€§çš„é”™è¯¯æç¤º
        raise ValueError(
            'Failed to retrieve the dataset. You can avoid this issue by increasing `max_length` or '
            'modifying the `truncation_strategy`.'
        )

    # è¿”å›æ•°æ®é›†é•¿åº¦ï¼šç›´æ¥è½¬äº¤ç»™åº•å±‚HFæ•°æ®é›†
    def __len__(self) -> int:
        """
        å‡½æ•°åŠŸèƒ½ï¼š
            è¿”å›æƒ°æ€§æ•°æ®é›†çš„æ ·æœ¬æ€»æ•°ï¼ˆä¸åº•å±‚HFæ•°æ®é›†ä¸€è‡´ï¼‰ã€‚

        å…¥å‚ï¼š
            æ— 

        è¿”å›å€¼ï¼š
            int: æ ·æœ¬æ•°é‡ã€‚

        ç¤ºä¾‹ï¼š
            >>> n = len(ds)
        """
        return len(self.dataset)


# å®šä¹‰å‡½æ•°ï¼šä½¿ç”¨è£…ç®±ç®—æ³•å°†æ ·æœ¬æŒ‰é•¿åº¦è¿›è¡ŒåŒ¹é…æ‰“åŒ…
def calculate_matched_group(template, sequences, is_finished: bool = True):
    """
    å‡½æ•°åŠŸèƒ½ï¼š
        ä½¿ç”¨binpackingç®—æ³•æŒ‰æ¨¡æ¿çš„æœ€å¤§é•¿åº¦çº¦æŸï¼Œå°†åºåˆ—æŒ‰æ€»é•¿åº¦æ‰“åŒ…ä¸ºè‹¥å¹²ç»„ï¼›
        è‹¥æœªç»“æŸï¼ˆis_finished=Falseï¼‰ï¼Œåˆ™ä¿ç•™æœ€åä¸€ç»„ä½œä¸ºæ®‹ç•™ä»¥ä¾¿ä¸‹è½®ç»§ç»­å¡«å……ã€‚

    å…¥å‚ï¼š
        template: æ¨¡æ¿å¯¹è±¡ï¼Œéœ€åŒ…å«max_lengthå±æ€§ç”¨äºçº¦æŸæ¯ç»„æ€»é•¿åº¦ä¸Šé™ã€‚
        sequences: å¾…æ‰“åŒ…çš„åºåˆ—åˆ—è¡¨ï¼Œå…ƒç´ å½¢å¦‚ (payload, length)ã€‚
        is_finished (bool): æ˜¯å¦æ‰€æœ‰æ•°æ®å·²ç»å–‚å…¥ï¼›è‹¥å¦ï¼Œå°†æŠŠæœ€åä¸€ç»„ä½œä¸ºæ®‹ç•™è¿”å›ã€‚

    è¿”å›å€¼ï¼š
        Tuple[List[List], List]: (å·²å®Œæˆçš„è‹¥å¹²ç»„, æ®‹ç•™çš„æœªæ»¡ä¸€ç»„)ã€‚

    ç¤ºä¾‹ï¼š
        >>> seqs, rest = calculate_matched_group(template, [(x, l) for x,l in data], True)
    """
    # è¾¹ç•Œæƒ…å†µï¼šè‹¥æ²¡æœ‰ä»»ä½•å¾…æ‰“åŒ…å…ƒç´ ï¼Œè¿”å›ä¸¤ä¸ªç©ºåˆ—è¡¨
    if len(sequences) == 0:
        return [], []
    # å¼•ç”¨è®ºæ–‡èƒŒæ™¯ï¼šhttps://arxiv.org/pdf/2404.10830
    # åŠ¨æ€å¯¼å…¥binpackingåº“ï¼šå°†å…ƒç´ ä»¥lengthä½œä¸ºé‡é‡è¿›è¡Œè£…ç®±
    import binpacking
    # ä½¿ç”¨æ’å®šä½“ç§¯è£…ç®±ï¼šweight_pos=1æŒ‡æ˜é•¿åº¦å­—æ®µä½ç½®ï¼Œé™åˆ¶æ¯ç»„ä¸è¶…è¿‡template.max_length
    sequences = binpacking.to_constant_volume(sequences, template.max_length, weight_pos=1)
    # è‹¥å­˜åœ¨è‡³å°‘ä¸€ç»„ä¸”æœªç»“æŸï¼Œå°†æœ€åä¸€ç»„ä½œä¸ºæ®‹ç•™ï¼Œå…¶ä½™ä½œä¸ºå®Œæˆç»„
    if sequences and not is_finished:
        sequences, ret_sequences = sequences[:-1], sequences[-1]
    else:
        # å·²ç»“æŸæˆ–æ²¡æœ‰åˆ†ç»„ï¼Œæ®‹ç•™ä¸ºç©º
        ret_sequences = []
    # è¿”å›ï¼ˆå·²å®Œæˆçš„åˆ†ç»„ï¼Œæ®‹ç•™åˆ†ç»„ï¼‰
    return sequences, ret_sequences


# å®šä¹‰é¢„è®¡ç®—æ‰“åŒ…ç´¢å¼•çš„æ•°æ®é›†ï¼šä¸€æ¬¡æ€§ç¦»çº¿è®¡ç®—æ‰€æœ‰packç»„
class PackingDataset(Dataset):
    """
    ç±»åŠŸèƒ½ï¼š
        åŸºäºé™æ€æ•°æ®é›†ä¸æ¨¡æ¿ï¼Œé¢„è®¡ç®—æ‰“åŒ…åçš„ç´¢å¼•åˆ—è¡¨ä¸å¯¹åº”é•¿åº¦ï¼Œ__getitem__æŒ‰ç»„å–å‡ºå¹¶è°ƒç”¨æ¨¡æ¿è¿›è¡Œpackingã€‚

    å…³é”®å±æ€§ï¼š
        template: æ¨¡æ¿å¯¹è±¡ï¼Œéœ€æä¾›max_lengthä¸packing_rowç­‰æ¥å£ã€‚
        dataset: åº•å±‚æ•°æ®é›†ï¼ˆåº”åŒ…å«'length'åˆ—ä»¥ä¾›æ‰“åŒ…ï¼‰ã€‚
        num_proc (int): é¢„ç•™å‚æ•°ï¼Œå½“å‰å®ç°æœªä½¿ç”¨å¤šè¿›ç¨‹ã€‚
        strict (bool): æ˜¯å¦ä¸¥æ ¼å¤„ç†ç¼–ç é”™è¯¯ï¼ˆä¼ é€’ç»™æ¨¡æ¿é˜¶æ®µä½¿ç”¨ï¼‰ã€‚
        load_from_cache_file (bool): é¢„ç•™å‚æ•°ï¼Œæ§åˆ¶æ˜¯å¦ä»ç¼“å­˜åŠ è½½ã€‚
        workers (List): é¢„ç•™çš„å·¥ä½œè¿›ç¨‹åˆ—è¡¨ï¼ˆå½“å‰æœªä½¿ç”¨ï¼‰ã€‚
        packed_idx (List[List[int]]): é¢„è®¡ç®—çš„åˆ†ç»„ç´¢å¼•ã€‚
        packed_length (List[int]): æ¯ç»„çš„æ€»é•¿åº¦ï¼ˆsum of lengthsï¼‰ã€‚
    """

    # æ„é€ å‡½æ•°ï¼šä¿å­˜é…ç½®å¹¶ï¼ˆåœ¨ä¸»è¿›ç¨‹ï¼‰åˆ›å»ºæ‰“åŒ…ç´¢å¼•ï¼Œç„¶ååœ¨åˆ†å¸ƒå¼ç¯å¢ƒä¸­å¹¿æ’­
    def __init__(
        self,
        template,
        dataset,
        num_proc: int = 1,
        *,
        strict: bool = False,
        load_from_cache_file: bool = True,
        **kwargs,
    ):
        """
        å‡½æ•°åŠŸèƒ½ï¼š
            åˆå§‹åŒ–æ‰“åŒ…æ•°æ®é›†ï¼›åœ¨ä¸»è¿›ç¨‹ä¸Šåˆ›å»ºpackç´¢å¼•ï¼Œå¹¶åœ¨åˆ†å¸ƒå¼åˆå§‹åŒ–å®Œæˆæ—¶å¹¿æ’­ç»™å…¶ä»–è¿›ç¨‹ã€‚

        å…¥å‚ï¼š
            template: æ¨¡æ¿å¯¹è±¡ï¼ˆéœ€å…·å¤‡packingèƒ½åŠ›ï¼‰ã€‚
            dataset: å«'length'åˆ—çš„åº•å±‚æ•°æ®é›†ã€‚
            num_proc (int): é¢„ç•™å¹¶å‘å¤„ç†å‚æ•°ã€‚
            strict (bool): æ˜¯å¦ä¸¥æ ¼æ¨¡å¼ã€‚
            load_from_cache_file (bool): æ˜¯å¦ä»ç¼“å­˜åŠ è½½ï¼ˆå½“å‰æœªç”¨ï¼‰ã€‚
            **kwargs: é¢„ç•™æ‰©å±•å‚æ•°ã€‚

        è¿”å›å€¼ï¼š
            None

        ç¤ºä¾‹ï¼š
            >>> ds = PackingDataset(template, dataset)
        """
        # æŒ‡ç¤ºæ¨¡æ¿è¿›å…¥packingæ¨¡å¼ï¼ˆå¯èƒ½å½±å“encodeè¡Œä¸ºï¼‰
        template._packing = True
        # ä¿å­˜æ¨¡æ¿ä¸æ•°æ®é›†å¼•ç”¨
        self.template = template
        self.dataset = dataset
        # ä¿å­˜é¢„ç•™å‚æ•°ï¼šè¿›ç¨‹æ•°ä¸ä¸¥æ ¼æ¨¡å¼
        self.num_proc = num_proc
        self.strict = strict
        # ä¿å­˜æ˜¯å¦ä»ç¼“å­˜åŠ è½½æ ‡å¿—
        self.load_from_cache_file = load_from_cache_file
        # é¢„ç•™å·¥ä½œè¿›ç¨‹åˆ—è¡¨ï¼ˆå½“å‰æœªä½¿ç”¨ï¼‰
        self.workers = []
        # ä»…ä¸»è¿›ç¨‹é¢„è®¡ç®—æ‰“åŒ…ç´¢å¼•ä¸æ€»é•¿åº¦ï¼›å…¶ä»–è¿›ç¨‹ç­‰å¾…å¹¿æ’­
        self.packed_idx, self.packed_length = self.create_packed_idx() if is_master() else (None, None)
        # è‹¥å¤„äºåˆ†å¸ƒå¼ç¯å¢ƒä¸”é€šä¿¡å·²åˆå§‹åŒ–ï¼Œåˆ™å¹¿æ’­å¯¹è±¡åˆ—è¡¨åˆ°æ‰€æœ‰è¿›ç¨‹
        if dist.is_initialized() and is_dist():
            obj_list = [(self.packed_idx, self.packed_length)]
            dist.broadcast_object_list(obj_list)
            self.packed_idx, self.packed_length = obj_list[0]
        
    # åˆ›å»ºæ‰“åŒ…ç´¢å¼•ï¼šåŸºäºlengthåˆ—è¿›è¡Œè£…ç®±ï¼Œç”Ÿæˆæ¯ç»„çš„æ ·æœ¬ç´¢å¼•ä¸æ€»é•¿åº¦
    def create_packed_idx(self):
        """
        å‡½æ•°åŠŸèƒ½ï¼š
            è¯»å–æ•°æ®é›†ä¸­'length'åˆ—ï¼Œç»“åˆæ¨¡æ¿æœ€å¤§é•¿åº¦ï¼Œé€šè¿‡è£…ç®±ç”Ÿæˆåˆ†ç»„ç´¢å¼•ä¸æ¯ç»„æ€»é•¿åº¦ã€‚

        å…¥å‚ï¼š
            æ— 

        è¿”å›å€¼ï¼š
            Tuple[List[List[int]], List[int]]: (æ¯ç»„çš„æ ·æœ¬ç´¢å¼•åˆ—è¡¨, æ¯ç»„çš„é•¿åº¦å’Œ)ã€‚

        ç¤ºä¾‹ï¼š
            >>> packed_idx, packed_len = self.create_packed_idx()
        """
        # è·å–æ¯æ¡æ ·æœ¬çš„é•¿åº¦æ•°ç»„
        lengths = self.dataset['length']
        # æ„é€ å½¢å¦‚ (æ ·æœ¬ç´¢å¼•, é•¿åº¦) çš„åˆ—è¡¨ï¼Œä¾›è£…ç®±ç®—æ³•ä½¿ç”¨
        data = [(i, length) for i, length in enumerate(lengths)]
        # åˆå§‹åŒ–æ»‘åŠ¨çª—å£èµ·ç‚¹ç´¢å¼•
        i = 0
        # è®¾ç½®æ¯æ‰¹é€å…¥è£…ç®±ç®—æ³•çš„æ‰¹å¤§å°ï¼Œå¹³è¡¡æ•ˆæœä¸é€Ÿåº¦
        PACKING_BATCH_SIZE = 1000
        # åˆå§‹åŒ–è¾“å…¥ç¼“å­˜ã€ç»“æœç´¢å¼•ä¸ç»“æœé•¿åº¦åˆ—è¡¨
        input_data, packed_idx, packed_length = [], [], []
        # ä½¿ç”¨tqdmæ˜¾ç¤ºæ€»è¿›åº¦æ¡ï¼ˆtotalä¸ºæ ·æœ¬æ•°ï¼‰ï¼ŒåŠ¨æ€åˆ—å®½ï¼Œæè¿°ä¸º'Packing: '
        with tqdm(total=len(data), dynamic_ncols=True, desc='Packing: ') as prog_bar:
            # æŒç»­åœ°æŒ‰æ‰¹æ¬¡æ¨è¿›ç›´è‡³å¤„ç†å®Œæ‰€æœ‰æ•°æ®
            while True:
                # å–å½“å‰æ‰¹æ¬¡çš„æ–°æ•°æ®å¹¶è¿½åŠ åˆ°è¾“å…¥ç¼“å­˜
                # NOTE: Python çš„åˆ—è¡¨åˆ‡ç‰‡ï¼ˆsliceï¼‰æ˜¯â€œå®‰å…¨åˆ‡ç‰‡â€ï¼Œå³ä½¿èµ·å§‹ç´¢å¼•è¶…å‡ºèŒƒå›´ï¼Œä¹Ÿä¸ä¼šæŠ¥ IndexErrorï¼Œè€Œæ˜¯è¿”å›ç©ºåˆ—è¡¨ []
                new_data = data[i:i + PACKING_BATCH_SIZE]
                input_data += new_data
                # æ›´æ–°è¿›åº¦æ¡ï¼Œå¢é‡ä¸ºæœ¬æ‰¹æ¬¡æ•°æ®é‡
                prog_bar.update(len(new_data))
                # è‹¥ç¼“å­˜å·²ç©ºï¼ˆæ— æ•°æ®å¯ä¾›è£…ç®±ï¼‰ï¼Œè·³å‡ºå¾ªç¯
                if not input_data:
                    break
                # å‰ç§»æ‰¹æ¬¡èµ·ç‚¹
                i += PACKING_BATCH_SIZE
                # æ ‡è®°æœ¬è½®ç»“æŸçŠ¶æ€ï¼ˆå¤„ç†åˆ°æœ«å°¾ï¼‰
                is_finished = i >= len(data)
                # è°ƒç”¨è£…ç®±å‡½æ•°ï¼šè¿”å›å®Œæˆçš„åˆ†ç»„ä¸å¯èƒ½çš„æ®‹ç•™ç¼“å­˜
                sequences, input_data = calculate_matched_group(self.template, input_data, is_finished=is_finished)
                # å°†æ¯ä¸ªåˆ†ç»„ä¸­çš„æ ·æœ¬ç´¢å¼•æå–å‡ºæ¥å¹¶è¿½åŠ åˆ°ç»“æœåˆ—è¡¨
                packed_idx += [[x[0] for x in seq] for seq in sequences]
                # è®¡ç®—æ¯ä¸ªåˆ†ç»„çš„é•¿åº¦å’Œå¹¶è¿½åŠ 
                packed_length += [sum(x[1] for x in seq) for seq in sequences]
        # è¿”å›æ‰“åŒ…ç´¢å¼•ä¸æ¯ç»„é•¿åº¦
        return packed_idx, packed_length

    # è¯»å–ä¸€ç»„æ•°æ®ï¼šæ ¹æ®é¢„è®¡ç®—ç´¢å¼•å–å‡ºå¤šæ¡æ ·æœ¬å¹¶è°ƒç”¨æ¨¡æ¿è¿›è¡Œpacking
    def __getitem__(self, index):
        """
        å‡½æ•°åŠŸèƒ½ï¼š
            æ ¹æ®indexç´¢å¼•å–å‡ºé¢„è®¡ç®—çš„æ ·æœ¬åºåˆ—ï¼Œå¹¶ä½¿ç”¨æ¨¡æ¿è¿›è¡Œæ‰“åŒ…ï¼Œè¿”å›å•ä¸ªæ‰“åŒ…æ ·æœ¬ã€‚

        å…¥å‚ï¼š
            index (int): ç»„ç´¢å¼•ã€‚

        è¿”å›å€¼ï¼š
            Any: æ¨¡æ¿packing_rowè¿”å›çš„å·²æ‰“åŒ…æ ·æœ¬ã€‚

        ç¤ºä¾‹ï¼š
            >>> batch = self[0]
        """
        # å–å‡ºè¯¥ç»„å¯¹åº”çš„æ ·æœ¬ç´¢å¼•åºåˆ—
        sequence = self.packed_idx[index]
        # æ ¹æ®ç´¢å¼•ä»åº•å±‚æ•°æ®é›†ä¸­å–å‡ºå¤šæ¡æ ·æœ¬ï¼Œå½¢æˆåˆ—è¡¨
        row = [self.dataset[i] for i in sequence]
        # è°ƒç”¨æ¨¡æ¿çš„packing_rowå¯¹è¯¥ç»„æ ·æœ¬è¿›è¡Œæ‹¼æ¥/è£å‰ªç­‰å¤„ç†
        return self.template.packing_row(row)

    # è¿”å›æ‰“åŒ…åçš„ç»„æ•°
    def __len__(self):
        """
        å‡½æ•°åŠŸèƒ½ï¼š
            è¿”å›é¢„è®¡ç®—çš„æ‰“åŒ…ç»„åˆæ•°é‡ã€‚

        å…¥å‚ï¼š
            æ— 

        è¿”å›å€¼ï¼š
            int: ç»„çš„æ•°é‡ã€‚

        ç¤ºä¾‹ï¼š
            >>> n = len(self)
        """
        return len(self.packed_idx)


# å®šä¹‰å¯è¿­ä»£çš„æµå¼æ‰“åŒ…æ•°æ®é›†ï¼šä½¿ç”¨å­è¿›ç¨‹å¹¶é€šè¿‡é˜Ÿåˆ—ä¸è£…ç®±åä½œå®Œæˆpacking
class IterablePackingDataset(IterableDataset):
    """
    ç±»åŠŸèƒ½ï¼š
        è¾¹è¿­ä»£è¾¹æ‰“åŒ…çš„æ•°æ®é›†å®ç°ã€‚é€šè¿‡å¤šè¿›ç¨‹å¯¹æ ·æœ¬è¿›è¡Œç¼–ç ï¼Œä¸»è¿›ç¨‹èšåˆè¿”å›ï¼Œ
        æŒ‰intervalæ‰¹æ¬¡åšè£…ç®±ï¼Œå¹¶å°†æ‰“åŒ…åçš„æ ·æœ¬æŒ‰éœ€yieldã€‚

    å…³é”®å±æ€§ï¼š
        template: æ¨¡æ¿å¯¹è±¡ã€‚
        dataset: å¯è¿­ä»£/å¯ç´¢å¼•çš„æ•°æ®æºã€‚
        num_proc (int): åå°ç¼–ç è¿›ç¨‹æ•°ã€‚
        packing_interval (int): æ¯è½®é€å…¥ç¼–ç /è£…ç®±çš„æ ·æœ¬æ•°ä¸Šé™ã€‚
        strict (bool): ç¼–ç å¼‚å¸¸æ—¶æ˜¯å¦ä¸¥æ ¼æŠ›é”™ï¼ˆå¿½ç•¥MaxLengthErrorï¼‰ã€‚
        cyclic (bool): æ˜¯å¦å¾ªç¯éå†æ•°æ®æºï¼ˆæ— é™æµï¼‰ã€‚
        _in_queue/_out_queue (mp.Queue): è¿›/å‡ºé˜Ÿåˆ—ï¼Œç”¨äºä¼ é€’åŸå§‹æ•°æ®ä¸ç¼–ç ç»“æœã€‚
        workers (List[mp.Process]): åå°å·¥ä½œè¿›ç¨‹åˆ—è¡¨ã€‚
    """

    # æ„é€ å‡½æ•°ï¼šå¯åŠ¨å­è¿›ç¨‹ï¼Œåˆå§‹åŒ–é˜Ÿåˆ—ä¸æ§åˆ¶å‚æ•°
    def __init__(
        self,
        template,
        dataset,
        num_proc: int = 1,
        *,
        packing_interval: int = 128,
        strict: bool = False,
        cyclic: bool = False,
        **kwargs,
    ):
        """
        å‡½æ•°åŠŸèƒ½ï¼š
            åˆå§‹åŒ–æµå¼æ‰“åŒ…æ•°æ®é›†ï¼šè®¾ç½®å¹¶å¯åŠ¨ç¼–ç å­è¿›ç¨‹ï¼Œå‡†å¤‡é˜Ÿåˆ—ä¸æ§åˆ¶å‚æ•°ã€‚

        å…¥å‚ï¼š
            template: æ¨¡æ¿å¯¹è±¡ã€‚
            dataset: æ•°æ®æºã€‚
            num_proc (int): å­è¿›ç¨‹æ•°é‡ã€‚
            packing_interval (int): æ¯è½®å¤„ç†çš„æ ·æœ¬ä¸Šé™ã€‚
            strict (bool): æ˜¯å¦ä¸¥æ ¼å¤„ç†ç¼–ç å¼‚å¸¸ã€‚
            cyclic (bool): æ˜¯å¦å¾ªç¯éå†æ•°æ®ã€‚
            **kwargs: é¢„ç•™å‚æ•°ã€‚

        è¿”å›å€¼ï¼š
            None

        ç¤ºä¾‹ï¼š
            >>> ds = IterablePackingDataset(template, dataset, num_proc=2)
        """
        # æŒ‡ç¤ºæ¨¡æ¿è¿›å…¥packingæ¨¡å¼
        template._packing = True
        # ä¿å­˜æ¨¡æ¿ä¸æ•°æ®å¼•ç”¨
        self.template = template
        self.dataset = dataset
        # ä¿å­˜å­è¿›ç¨‹æ•°é‡ä¸ä¸¥æ ¼æ¨¡å¼å‚æ•°
        self.num_proc = num_proc
        self.strict = strict

        # ä¿å­˜packingé—´éš”æ ·æœ¬æ•°
        self.packing_interval = packing_interval
        # åˆ›å»ºè¿›/å‡ºé˜Ÿåˆ—ç”¨äºä¸å­è¿›ç¨‹é€šä¿¡
        self._in_queue = mp.Queue()
        self._out_queue = mp.Queue()
        # åˆå§‹åŒ–å·¥ä½œè¿›ç¨‹åˆ—è¡¨
        self.workers = []
        # ä¿å­˜æ˜¯å¦å¾ªç¯å–æ•°æ ‡å¿—
        self.cyclic = cyclic
        # æŒ‰num_procåˆ›å»ºå¹¶å¯åŠ¨å­è¿›ç¨‹ï¼Œæ¯ä¸ªè¿›ç¨‹è¿è¡Œ_processorä½œä¸ºå·¥ä½œå¾ªç¯
        for _ in range(self.num_proc):
            worker = mp.Process(target=self._processor, daemon=True)
            worker.start()
            self.workers.append(worker)

    # å­è¿›ç¨‹å·¥ä½œå¾ªç¯ï¼šä»è¾“å…¥é˜Ÿåˆ—å–æ•°æ®ï¼Œç¼–ç åæ”¾å…¥è¾“å‡ºé˜Ÿåˆ—
    def _processor(self):
        """
        å‡½æ•°åŠŸèƒ½ï¼š
            å­è¿›ç¨‹ä¸­æ‰§è¡Œçš„å¾ªç¯ï¼šæŒç»­ä»_in_queueå–æ ·æœ¬ï¼Œè°ƒç”¨æ¨¡æ¿encodeï¼Œ
            è‹¥strictä¸”å¼‚å¸¸å¹¶éMaxLengthErroråˆ™æŠ›å‡ºï¼›å¦åˆ™å°†ç»“æœæ”¾å…¥_out_queueã€‚

        ç¤ºä¾‹ï¼š
            ï¼ˆå†…éƒ¨ä½¿ç”¨ï¼Œæ— éœ€ç›´æ¥è°ƒç”¨ï¼‰
        """
        # æŒç»­å¤„ç†ç›´åˆ°ä¸»è¿›ç¨‹ç»“æŸï¼ˆå®ˆæŠ¤è¿›ç¨‹éšä¸»è¿›ç¨‹é€€å‡ºï¼‰
        while True:
            # ä»è¾“å…¥é˜Ÿåˆ—å–å‡ºä¸€ä¸ªç¼–å·ä¸æ•°æ®å¯¹
            i, data = self._in_queue.get()
            # é¢„è®¾ç¼–ç ç»“æœä¸ºç©ºå­—å…¸ï¼Œç”¨äºæ ‡è¯†å¤±è´¥æƒ…å½¢
            encoded_data = {}
            try:
                # å°è¯•è°ƒç”¨æ¨¡æ¿è¿›è¡Œç¼–ç ï¼Œå¹¶è¦æ±‚è¿”å›é•¿åº¦ä¿¡æ¯
                encoded_data = self.template.encode(data, return_length=True)
            except Exception as e:
                # ä¸¥æ ¼æ¨¡å¼ä¸‹ï¼Œé™¤æœ€å¤§é•¿åº¦å¼‚å¸¸å¤–ï¼Œå…¶ä»–å¼‚å¸¸éœ€è¦æŠ›å‡ºä»¥ä¾¿ä¸Šå±‚å¤„ç†
                if self.strict and not isinstance(e, MaxLengthError):
                    raise
            # å°†ç»“æœï¼ˆå¯èƒ½ä¸ºç©ºï¼‰æ”¾å…¥è¾“å‡ºé˜Ÿåˆ—ï¼Œä¿ç•™å…¶åœ¨æ‰¹æ¬¡ä¸­çš„ä½ç½®i
            self._out_queue.put((i, encoded_data))

    # ä¸»è¿›ç¨‹ï¼šå°†è‹¥å¹²æ¡æ ·æœ¬æ”¾å…¥è¾“å…¥é˜Ÿåˆ—ï¼Œè¿”å›å®é™…æ”¾å…¥çš„æ ·æœ¬æ•°
    def _put_data_in_queue(self, iterator) -> int:
        """
        å‡½æ•°åŠŸèƒ½ï¼š
            ä»è¿­ä»£å™¨ä¸­æœ€å¤šå–packing_intervalæ¡æ•°æ®ï¼Œæ”¾å…¥è¾“å…¥é˜Ÿåˆ—ï¼Œè¿”å›æœ¬è½®æ”¾å…¥çš„æ ·æœ¬æ•°ã€‚

        å…¥å‚ï¼š
            iterator: æ•°æ®è¿­ä»£å™¨ã€‚

        è¿”å›å€¼ï¼š
            int: å®é™…æ”¾å…¥é˜Ÿåˆ—çš„æ ·æœ¬æ•°é‡ã€‚

        ç¤ºä¾‹ï¼š
            >>> n = self._put_data_in_queue(iter(self.dataset))
        """
        # åœ¨å½“å‰è½®å†…ï¼ŒæŒ‰é¡ºåºä¸ºæ¯æ¡æ•°æ®åˆ†é…ä¸€ä¸ªä½ç½®ç´¢å¼•i
        for i in range(self.packing_interval):
            try:
                # ä»è¿­ä»£å™¨å–å‡ºä¸‹ä¸€æ¡æ•°æ®
                data = next(iterator)
            except StopIteration:
                # è¿­ä»£å™¨è€—å°½ï¼Œè¿”å›å½“å‰å·²æ”¾å…¥çš„æ•°é‡
                return i
            # å°†ä½ç½®ç´¢å¼•ä¸æ•°æ®æ”¾å…¥è¾“å…¥é˜Ÿåˆ—
            self._in_queue.put((i, data))
        # è‹¥å¾ªç¯å®Œæ•´æ‰§è¡Œï¼Œè¯´æ˜æ”¾æ»¡äº†intervalæ¡ï¼Œè¿”å›æ€»æ•°
        return i + 1

    # ä¸»è¿›ç¨‹ï¼šä»è¾“å‡ºé˜Ÿåˆ—æ”¶é›†ç¼–ç ç»“æœï¼ŒæŒ‰åŸä½ç½®è¿˜åŸé¡ºåºå¹¶è¿”å›ç´¯åŠ åçš„ç»“æœåˆ—è¡¨
    def _fetch_data_out_queue(self, last_res, num_samples):
        """
        å‡½æ•°åŠŸèƒ½ï¼š
            ä»_out_queueå–å›num_samplesæ¡ç¼–ç ç»“æœï¼ŒæŒ‰ä½ç½®ç´¢å¼•æ”¾å›åˆ—è¡¨ï¼Œ
            è¿‡æ»¤æ‰ç¼–ç å¤±è´¥é¡¹ï¼Œå¹¶ç´¯åŠ åˆ°last_resåè¿”å›ã€‚

        å…¥å‚ï¼š
            last_res (List): ä¸Šè½®å‰©ä½™/æœªå¤„ç†çš„ç»“æœåˆ—è¡¨ã€‚
            num_samples (int): æœ¬è½®æœŸæœ›æ”¶é›†çš„ç»“æœæ•°é‡ã€‚

        è¿”å›å€¼ï¼š
            List: è¿½åŠ æœ¬è½®ç»“æœåçš„æ€»ç»“æœåˆ—è¡¨ï¼Œå…ƒç´ ä¸º (ç¼–ç ç»“æœ, é•¿åº¦)ã€‚

        ç¤ºä¾‹ï¼š
            >>> data = self._fetch_data_out_queue([], 32)
        """
        # åˆå§‹åŒ–å›ºå®šé•¿åº¦çš„å ä½åˆ—è¡¨ï¼Œç”¨äºæŒ‰iæ”¾ç½®ç»“æœ
        res = [None] * num_samples
        # é€é¡¹ä»è¾“å‡ºé˜Ÿåˆ—å–å›ç»“æœ
        for _ in range(num_samples):
            i, data = self._out_queue.get()
            # è‹¥ç¼–ç ç»“æœä¸ºç©ºï¼ˆå¤±è´¥ï¼‰ï¼Œè·³è¿‡
            if not data:
                continue
            # å°†ç¼–ç ç»“æœä¸å…¶é•¿åº¦ç»„æˆäºŒå…ƒç»„ï¼ŒæŒ‰ä½ç½®iæ”¾å›
            res[i] = (data, len(data['input_ids']))
        # è¿‡æ»¤Noneå ä½ï¼Œå¾—åˆ°æœ‰æ•ˆç»“æœåˆ—è¡¨
        res = [data for data in res if data]
        # å°†æœ¬è½®æœ‰æ•ˆç»“æœè¿½åŠ åˆ°ç´¯è®¡ç»“æœä¸­
        last_res += res
        # è¿”å›ç´¯è®¡åçš„ç»“æœ
        return last_res

    @staticmethod
    def cyclic_iter(iterable):
        """
        å‡½æ•°åŠŸèƒ½ï¼š
            å¯¹ä»»æ„å¯è¿­ä»£å¯¹è±¡è¿›è¡Œæ— é™å¾ªç¯è¿­ä»£çš„ç”Ÿæˆå™¨ã€‚

        å…¥å‚ï¼š
            iterable: ä»»æ„å¯è¿­ä»£å¯¹è±¡ã€‚

        è¿”å›å€¼ï¼š
            Iterator: æ— é™å¾ªç¯åœ°yieldå…ƒç´ ã€‚

        ç¤ºä¾‹ï¼š
            >>> it = IterablePackingDataset.cyclic_iter([1, 2])
        """
        # å¤–å±‚æ— é™å¾ªç¯ï¼Œç¡®ä¿æºè¢«åå¤éå†
        while True:
            # å†…å±‚éå†ä¸€æ¬¡å¯è¿­ä»£å¯¹è±¡ï¼Œå°†å…ƒç´ é€ä¸ªäº§å‡º
            for x in iterable:
                yield x

    # è¿­ä»£å™¨ï¼šè¾¹æ”¾å…¥é˜Ÿåˆ—ã€è¾¹å–å›ç»“æœã€è¾¹æ‰“åŒ…ã€è¾¹yield
    def __iter__(self):
        """
        å‡½æ•°åŠŸèƒ½ï¼š
            å®ç°å¯è¿­ä»£æ¥å£ï¼šä¸æ–­åœ°å‘å­è¿›ç¨‹æŠ•å–‚æ•°æ®å¹¶æ”¶é›†ç¼–ç ç»“æœï¼ŒæŒ‰æœ€å¤§é•¿åº¦è¿›è¡Œè£…ç®±ï¼Œ
            å°†æ‰“åŒ…åçš„æ ·æœ¬é€ä¸ªyieldï¼Œç›´è‡³æ•°æ®æºè€—å°½ï¼ˆæˆ–å¾ªç¯æ¨¡å¼ä¸‹æŒç»­ï¼‰ã€‚

        å…¥å‚ï¼š
            æ— 

        è¿”å›å€¼ï¼š
            Iterator: é€ä¸ªäº§å‡ºçš„æ‰“åŒ…æ ·æœ¬ã€‚

        ç¤ºä¾‹ï¼š
            >>> for packed in self: ...
        """
        # å¿«é€Ÿæ£€æµ‹æ•°æ®é›†æ˜¯å¦ä¸ºç©ºï¼šè‹¥ç«‹åˆ»æŠ›å‡ºStopIterationï¼Œåˆ™ç›´æ¥ç»“æŸ
        try:
            next(iter(self.dataset))
        except StopIteration:
            return

        # æ ¹æ®æ˜¯å¦å¾ªç¯æ¨¡å¼ï¼Œé€‰æ‹©ä¸åŒçš„è¿­ä»£å™¨å®ç°
        if self.cyclic:
            iterator = self.cyclic_iter(self.dataset)
        else:
            iterator = iter(self.dataset)
        # ç”¨äºç´¯ç§¯ç¼–ç ç»“æœï¼ˆpayload, lengthï¼‰å¯¹
        data = []
        # ä¸»å¾ªç¯ï¼šæ¯è½®æ”¾å…¥ä¸€æ‰¹æ•°æ®ï¼Œå–å›ç»“æœï¼Œåšè£…ç®±å¹¶yield
        while True:
            # æ”¾å…¥ä¸€æ‰¹æ•°æ®ï¼Œè·å–æœ¬è½®å®é™…æ ·æœ¬æ•°
            num_samples = self._put_data_in_queue(iterator)
            # è‹¥ä¸è¶³ä¸€ä¸ªpacking_intervalï¼Œåˆ™æœ¬è½®ç»“æŸåæ•´ä½“ç»“æŸ
            finished = num_samples != self.packing_interval
            # ä»è¾“å‡ºé˜Ÿåˆ—å–å›æœ¬è½®ç»“æœï¼Œå¹¶ä¸å†å²ç»“æœç´¯è®¡
            data = self._fetch_data_out_queue(data, num_samples)
            # æŒ‰æ¨¡æ¿æœ€å¤§é•¿åº¦è¿›è¡Œè£…ç®±ï¼Œè¿”å›å®Œæˆçš„åˆ†ç»„ä¸æ®‹ç•™ï¼ˆè¦†ç›–dataä¸ºæ®‹ç•™ï¼‰
            sequences, data = calculate_matched_group(self.template, data, is_finished=finished)
            # ä¸´æ—¶ç¼“å†²æœ¬è½®è¦äº§å‡ºçš„å·²æ‰“åŒ…æ ·æœ¬
            res = []
            # éå†æ¯ä¸ªåˆ†ç»„ï¼Œæå–payloadå¹¶è°ƒç”¨æ¨¡æ¿è¿›è¡Œpacking
            for row in sequences:
                packed = self.template.packing_row([r[0] for r in row])
                res.append(packed)
            # é€ä¸ªyieldæœ¬è½®çš„å·²æ‰“åŒ…æ ·æœ¬
            yield from res
            # è‹¥æ•°æ®æºå·²è€—å°½ï¼ˆéå¾ªç¯æ¨¡å¼ï¼‰ï¼Œè·³å‡ºä¸»å¾ªç¯
            if finished:
                break


# å®šä¹‰è¡Œçº§ç¼–ç é¢„å¤„ç†å™¨ï¼šå¯é€‰æ‹©ä»…é¢„å…ˆè®¡ç®—lengthä¾›packingä½¿ç”¨
class EncodePreprocessor(RowPreprocessor):
    """
    ç±»åŠŸèƒ½ï¼š
        è¡Œçº§é¢„å¤„ç†å™¨ï¼Œå°è£…å¯¹æ¨¡æ¿encodeçš„è°ƒç”¨ï¼›å¯é€‰æ‹©ä»…å†™å…¥lengthåˆ°åŸrowï¼Œ
        ä»¥ä¾¿åç»­çš„PackingDataset/IterablePackingDatasetä½¿ç”¨ã€‚

    å…³é”®å±æ€§ï¼š
        template (Template): æ¨¡æ¿å¯¹è±¡ã€‚
        pre_tokenize (bool): æ˜¯å¦ä»…é¢„å…ˆè®¡ç®—é•¿åº¦è€Œä¸è¿”å›å®Œæ•´ç¼–ç ç»“æœã€‚
    """

    # æ„é€ å‡½æ•°ï¼šä¿å­˜æ¨¡æ¿å¼•ç”¨ä¸é¢„æ ‡å¿—
    def __init__(self, template: 'Template', pre_tokenize: bool = False):
        """
        å‡½æ•°åŠŸèƒ½ï¼š
            åˆå§‹åŒ–é¢„å¤„ç†å™¨ï¼Œä¿å­˜æ¨¡æ¿ä¸æ˜¯å¦é¢„æ ‡æ³¨é•¿åº¦çš„é…ç½®ã€‚

        å…¥å‚ï¼š
            template (Template): æ¨¡æ¿å¯¹è±¡ã€‚
            pre_tokenize (bool): ä¸ºTrueæ—¶ï¼Œä»…è¿”å›é™„åŠ äº†lengthå­—æ®µçš„åŸrowã€‚

        è¿”å›å€¼ï¼š
            None

        ç¤ºä¾‹ï¼š
            >>> pp = EncodePreprocessor(template, pre_tokenize=True)
        """
        # è°ƒç”¨çˆ¶ç±»æ„é€ å‡½æ•°ï¼Œå®ŒæˆåŸºç±»åˆå§‹åŒ–
        super().__init__()
        # ä¿å­˜æ¨¡æ¿å¼•ç”¨
        self.template = template
        # ä¿å­˜æ˜¯å¦ä»…è¿›è¡Œé¢„æ ‡æ³¨é•¿åº¦çš„å¼€å…³
        self.pre_tokenize = pre_tokenize

    # è¡Œçº§é¢„å¤„ç†ï¼šç¼–ç æˆ–ä»…å†™å…¥length
    def preprocess(self, row: Dict[str, Any]) -> Optional[Dict[str, Any]]:
        """
        å‡½æ•°åŠŸèƒ½ï¼š
            å¯¹å•è¡Œrowæ‰§è¡Œæ¨¡æ¿encodeï¼›è‹¥pre_tokenizeä¸ºTrueï¼Œåˆ™ä»…å°†lengthå†™å›rowå¹¶è¿”å›rowã€‚

        å…¥å‚ï¼š
            row (Dict[str, Any]): å•æ¡åŸå§‹æ ·æœ¬ã€‚

        è¿”å›å€¼ï¼š
            Optional[Dict[str, Any]]: ç¼–ç åçš„æ ·æœ¬å­—å…¸ï¼Œæˆ–ä»…å†™å…¥é•¿åº¦åçš„åŸrowã€‚

        ç¤ºä¾‹ï¼š
            >>> out = self.preprocess({"text": "hello"})
        """
        # è°ƒç”¨æ¨¡æ¿encodeä»¥è·å¾—å®Œæ•´ç¼–ç ï¼ˆå«lengthï¼‰
        encoded = self.template.encode(row, return_length=True)
        # è‹¥ä»…éœ€é¢„æ ‡æ³¨é•¿åº¦ï¼Œåˆ™å°†lengthå†™å›åˆ°åŸrowï¼Œå¹¶è¦†ç›–encodedä¸ºrow
        if self.pre_tokenize:
            row['length'] = encoded['length']
            encoded = row
        # è¿”å›ç¼–ç ç»“æœæˆ–ä»…å¸¦lengthçš„åŸrow
        return encoded
