"""
æ¨¡å—åŠŸèƒ½
-------
æœ¬æ¨¡å—é›†ä¸­å®šä¹‰ LLM è®­ç»ƒç›¸å…³çš„å‚æ•°ç±»åŠå…¶åˆå§‹åŒ–é€»è¾‘ï¼Œå°è£…äº†ä»¥ä¸‹èƒ½åŠ›ï¼š

- ç»§æ‰¿ä¸è¦†ç›– transformers çš„ `Seq2SeqTrainingArguments`ï¼Œå¯¹è¯„ä¼°/ä¿å­˜ç­–ç•¥ã€å­¦ä¹ ç‡ç­‰é»˜è®¤è¡Œä¸ºåšå®šåˆ¶ï¼›
- é›†æˆæ’ä»¶ç”Ÿæ€ï¼ˆå¦‚æŸå¤±å‡½æ•°æ˜ å°„ã€ä¼˜åŒ–å™¨å·¥å‚ï¼‰ä¸è‡ªå®šä¹‰ `Trainer` è·å–è®­ç»ƒå‚æ•°ï¼›
- æä¾› SwanLab å¯é€‰é›†æˆï¼ˆå®éªŒè®°å½•ä¸æ¶ˆæ¯é€šçŸ¥ï¼‰ï¼›
- æ”¯æŒ DeepSpeed é…ç½®ï¼ˆå«é¢„è®¾æ˜ å°„ã€ZeRO++/AutoTP åŠ¨æ€æ³¨å…¥ï¼‰ï¼›
- å¤„ç†åœ¨ PAI å¹³å°ä¸Šçš„å…¼å®¹é€»è¾‘ï¼ˆæ—¥å¿—ç›®å½•ã€ç‰ˆæœ¬è¿½åŠ ç­–ç•¥ï¼‰ï¼›
- ç»Ÿä¸€å‡†å¤‡è¾“å‡ºç›®å½•ã€æ—¥å¿—ç›®å½•ä¸è¿è¡Œåç­‰å…ƒä¿¡æ¯ã€‚

å…¸å‹ç”¨æ³•
-------
1. ç›´æ¥é€šè¿‡å‘½ä»¤è¡Œæˆ–é…ç½®æ–‡ä»¶å®ä¾‹åŒ– `TrainArguments`ï¼Œå…¶å†…éƒ¨çš„ `__post_init__` ä¼šè‡ªåŠ¨å®Œæˆè®¾å¤‡/åˆ†å¸ƒå¼ã€
   DeepSpeedã€è¯„ä¼°ç­–ç•¥ã€è¾“å‡ºè·¯å¾„ã€SwanLab ç­‰åˆå§‹åŒ–ï¼›
2. ä¹‹åå°† `TrainArguments.training_args` äº¤ç»™ `TrainerFactory` åˆ›å»ºçš„è®­ç»ƒå™¨ä½¿ç”¨ã€‚

æ³¨æ„ï¼šæœ¬æ–‡ä»¶ä¸­çš„æ¯ä¸€è¡Œä»£ç éƒ½é…æœ‰ä¸­æ–‡æ³¨é‡Šï¼Œä¾¿äºå¿«é€Ÿç†è§£æ¯ä¸€å¤„è¡Œä¸ºä¸ç›®çš„ã€‚
"""

# Copyright (c) Alibaba, Inc. and its affiliates.  # ç‰ˆæƒå£°æ˜ï¼Œæ ‡è¯†å½’å±ä¸è®¸å¯èŒƒå›´
import os  # å¼•å…¥æ ‡å‡†åº“ osï¼Œç”¨äºè·¯å¾„æ‹¼æ¥ä¸ç›®å½•æ“ä½œ
from dataclasses import dataclass, field  # å¼•å…¥ dataclass è£…é¥°å™¨ä¸å­—æ®µå·¥å‚ï¼Œç”¨äºå£°æ˜å‚æ•°æ•°æ®ç±»
from typing import Literal, Optional  # ç±»å‹æ³¨è§£ï¼šé™å®šå­—é¢é‡å–å€¼ä¸å¯é€‰ç±»å‹

from transformers import Seq2SeqTrainingArguments  # å¼•å…¥ HuggingFace çš„åºåˆ—åˆ°åºåˆ—è®­ç»ƒå‚æ•°åŸºç±»
from transformers.utils.versions import require_version  # ç‰ˆæœ¬æ£€æµ‹å·¥å…·ï¼Œç”¨äºç¡®ä¿å¯é€‰ä¾èµ–å­˜åœ¨

from swift.plugin import LOSS_MAPPING  # æ’ä»¶ä¾§æš´éœ²çš„æŸå¤±å‡½æ•°åç§°åˆ°å®ç°çš„æ˜ å°„å­—å…¸
from swift.trainers import TrainerFactory  # è®­ç»ƒå™¨å·¥å‚ï¼Œç”¨äºç”Ÿæˆåº•å±‚è®­ç»ƒå‚æ•°ä¸ Trainer
from swift.trainers.arguments import TrainArgumentsMixin  # è®­ç»ƒå‚æ•°æ··å…¥ç±»ï¼Œæä¾›é€šç”¨è®­ç»ƒå‚æ•°è¡Œä¸º
from swift.utils import (add_version_to_work_dir, get_device_count, get_logger, get_pai_tensorboard_dir, is_master,  # å·¥å…·æ–¹æ³•é›†åˆ
                         is_mp, is_pai_training_job, is_swanlab_available, json_parse_to_dict)  # åˆ†å¸ƒå¼/å¹³å°/é…ç½®è§£æè¾…åŠ©
from .base_args import BaseArguments, to_abspath  # æœ¬åŒ…å®šä¹‰çš„åŸºç¡€å‚æ•°ä¸ç»å¯¹è·¯å¾„è½¬æ¢å·¥å…·
from .tuner_args import TunerArguments  # å¾®è°ƒç›¸å…³å‚æ•°å®šä¹‰

logger = get_logger()  # è·å–æ¨¡å—çº§æ—¥å¿—è®°å½•å™¨ï¼Œç”¨äºè¾“å‡ºä¿¡æ¯ä¸å‘Šè­¦


@dataclass  # ä½¿ç”¨æ•°æ®ç±»ç®€åŒ–å‚æ•°å®šä¹‰ä¸åˆå§‹åŒ–
class Seq2SeqTrainingOverrideArguments(TrainArgumentsMixin, Seq2SeqTrainingArguments):
    """
    ç±»è¯´æ˜
    -----
    åœ¨ transformers çš„ `Seq2SeqTrainingArguments` åŸºç¡€ä¸Šè¦†ç›–/è¡¥å……è‹¥å¹²é»˜è®¤å‚æ•°ä¸ä¾¿æ·åˆå§‹åŒ–é€»è¾‘ï¼Œ
    ä»¥ä¾¿ä¸ ms-swift çš„è®­ç»ƒç”Ÿæ€é…åˆä½¿ç”¨ï¼ˆå¦‚ä¿å­˜/è¯„ä¼°ç­–ç•¥è”åŠ¨ã€æŒ‡æ ‡æ–¹å‘è‡ªåŠ¨æ¨æ–­ç­‰ï¼‰ã€‚

    ç»§æ‰¿å…³ç³»
    -------
    - TrainArgumentsMixin: æä¾›é€šç”¨è®­ç»ƒå‚æ•°çš„æ··å…¥èƒ½åŠ›ã€‚
    - Seq2SeqTrainingArguments: HuggingFace æ ‡å‡†è®­ç»ƒå‚æ•°ã€‚

    ä¸»è¦å±æ€§
    -------
    - output_dir: è®­ç»ƒè¾“å‡ºç›®å½•ï¼Œè‹¥æœªæŒ‡å®šåˆ™åŸºäº `model_suffix` è‡ªåŠ¨ç”Ÿæˆã€‚
    - learning_rate: å­¦ä¹ ç‡ï¼ŒæŒ‰è®­ç»ƒç±»å‹ç»™å‡ºåˆç†é»˜è®¤å€¼ã€‚
    - eval_strategy: è¯„ä¼°ç­–ç•¥ï¼Œæ”¯æŒ 'no'/'steps'/'epoch'ï¼Œæœªè®¾ç½®æ—¶ä¸ä¿å­˜ç­–ç•¥å¯¹é½ã€‚
    - fp16/bf16: åŠç²¾åº¦/æ··åˆç²¾åº¦å¼€å…³ï¼Œäº¤ç”±ä¸Šå±‚é…ç½®å¯ç”¨ã€‚

    ç¤ºä¾‹
    ---
    >>> args = Seq2SeqTrainingOverrideArguments(output_dir=None, eval_strategy=None)
    >>> # args.__post_init__ ä¼šåœ¨ç»§æ‰¿é“¾ä¸­è¢«è°ƒç”¨ï¼Œè‡ªåŠ¨è¡¥é½ output_dir ä¸ eval ç­–ç•¥
    """
    output_dir: Optional[str] = None  # è®­ç»ƒäº§ç‰©ä¿å­˜çš„æ ¹ç›®å½•ï¼›None æ—¶è‡ªåŠ¨æ¨å¯¼
    learning_rate: Optional[float] = None  # å­¦ä¹ ç‡ï¼›ä¸ºç©ºæ—¶æŒ‰ train_type è®¾ç½®é»˜è®¤å€¼
    eval_strategy: Optional[str] = None  # steps, epoch  # è¯„ä¼°ç­–ç•¥ï¼›ä¸ä¿å­˜ç­–ç•¥å¯¹é½æˆ–æ˜¾å¼æŒ‡å®š
    fp16: Optional[bool] = None  # æ˜¯å¦ä½¿ç”¨ FP16 è®­ç»ƒ
    bf16: Optional[bool] = None  # æ˜¯å¦ä½¿ç”¨ BF16 è®­ç»ƒ

    def _init_output_dir(self):
        """\
        åˆå§‹åŒ–è¾“å‡ºç›®å½•ã€‚

        ç¤ºä¾‹
        ----
        >>> args.output_dir = None
        >>> args.model_suffix = 'qwen2'
        >>> args._init_output_dir()  # output_dir å°†è¢«è®¾ç½®ä¸ºç»å¯¹è·¯å¾„ 'output/qwen2'
        """
        if self.output_dir is None:  # è‹¥æœªæ˜¾å¼æŒ‡å®šè¾“å‡ºç›®å½•
            self.output_dir = f'output/{self.model_suffix}'  # ä½¿ç”¨æ¨¡å‹åç¼€æ‹¼æ¥é»˜è®¤è¾“å‡ºç›®å½•
        self.output_dir = to_abspath(self.output_dir)  # ç»Ÿä¸€è½¬æ¢ä¸ºç»å¯¹è·¯å¾„ï¼Œé¿å…ç›¸å¯¹è·¯å¾„å¼•å‘æ­§ä¹‰

    def _init_eval_strategy(self):
        """
        åˆå§‹åŒ–è¯„ä¼°ç­–ç•¥ï¼š
        - æœªè®¾ç½®æ—¶ä¸ä¿å­˜ç­–ç•¥ä¸€è‡´ï¼›
        - å½“ä¸è¯„ä¼°æ—¶ï¼ˆ'no'ï¼‰ç¦ç”¨ eval_steps å¹¶å¼ºåˆ¶ä¸åˆ’åˆ†éªŒè¯é›†ï¼›
        - å½“æŒ‰æ­¥è¯„ä¼°ä¸”æœªæŒ‡å®š eval_steps æ—¶ï¼Œæ²¿ç”¨ save_stepsã€‚

        ç¤ºä¾‹
        ----
        >>> args.save_strategy = 'steps'; args.save_steps = 100
        >>> args.eval_strategy = None; args.eval_steps = None
        >>> args._init_eval_strategy(); assert args.eval_steps == 100
        """
        if self.eval_strategy is None:  # è‹¥æœªè®¾ç½®è¯„ä¼°ç­–ç•¥
            self.eval_strategy = self.save_strategy  # ä¸ä¿å­˜ç­–ç•¥ä¿æŒä¸€è‡´ä»¥ç®€åŒ–é…ç½®
        if self.eval_strategy == 'no':  # ä¸è¿›è¡Œè¯„ä¼°
            self.eval_steps = None  # å…³é—­æŒ‰æ­¥è¯„ä¼°
            if self.split_dataset_ratio > 0:  # è‹¥å…ˆå‰å¯ç”¨äº†åˆ’åˆ†éªŒè¯é›†
                self.split_dataset_ratio = 0.  # å…³é—­æ•°æ®é›†åˆ‡åˆ†ï¼Œé¿å…äº§ç”Ÿæ— ç”¨éªŒè¯é›†
                logger.info(f'Setting args.split_dataset_ratio: {self.split_dataset_ratio}')  # è®°å½•è‡ªåŠ¨è°ƒæ•´
        elif self.eval_strategy == 'steps' and self.eval_steps is None:  # æŒ‰æ­¥è¯„ä¼°ä½†æœªè®¾ç½®æ­¥æ•°
            self.eval_steps = self.save_steps  # å¤ç”¨ä¿å­˜æ­¥æ•°ä»¥ä¿æŒèŠ‚å¥ä¸€è‡´
        self.evaluation_strategy = self.eval_strategy  # å†™å› transformers æœŸæœ›çš„å­—æ®µå

    def _init_metric_for_best_model(self):
        """
        åˆå§‹åŒ–ç”¨äºé€‰æ‹©æœ€ä½³æ¨¡å‹çš„æŒ‡æ ‡ï¼š
        - ç”Ÿæˆå¼ä»»åŠ¡é»˜è®¤ä½¿ç”¨ 'rouge-l'ï¼›
        - å…¶ä»–ä»»åŠ¡é»˜è®¤ä½¿ç”¨ 'loss'ï¼ˆè¶Šä½è¶Šå¥½ï¼‰ã€‚

        ç¤ºä¾‹
        ----
        >>> args.predict_with_generate = True
        >>> args.metric_for_best_model = None
        >>> args._init_metric_for_best_model(); assert args.metric_for_best_model == 'rouge-l'
        """
        if self.metric_for_best_model is None:  # ä»…åœ¨æœªè®¾ç½®æ—¶æ¨æ–­é»˜è®¤æŒ‡æ ‡
            self.metric_for_best_model = 'rouge-l' if self.predict_with_generate else 'loss'  # åŸºäºæ˜¯å¦ç”Ÿæˆå¼æ¨æ–­

    def __post_init__(self):
        """
        dataclass åˆå§‹åŒ–åé’©å­ï¼šè¡¥é½ç›®å½•/æŒ‡æ ‡ä¸é»˜è®¤å­¦ä¹ ç‡ï¼Œå¹¶è”åŠ¨è¯„ä¼°ç­–ç•¥ã€‚

        å‚æ•°/è¿”å›
        --------
        æ— ï¼›è¯¥æ–¹æ³•é€šè¿‡å‰¯ä½œç”¨ä¿®æ”¹å®ä¾‹å±æ€§ã€‚

        ç¤ºä¾‹
        ----
        >>> args = Seq2SeqTrainingOverrideArguments(output_dir=None, learning_rate=None, eval_strategy=None)
        >>> args.train_type = 'full'
        >>> args.__post_init__(); assert args.learning_rate == 1e-5
        """
        self._init_output_dir()  # è§„èŒƒåŒ–ä¸å‡†å¤‡è¾“å‡ºç›®å½•
        self._init_metric_for_best_model()  # è®¾ç½®ç”¨äºé€‰æ‹©æœ€ä½³æ¨¡å‹çš„æŒ‡æ ‡
        if self.greater_is_better is None and self.metric_for_best_model is not None:  # è‹¥æœªæŒ‡å®šæ¯”è¾ƒæ–¹å‘ä½†å·²æœ‰æŒ‡æ ‡
            self.greater_is_better = 'loss' not in self.metric_for_best_model  # é loss æŒ‡æ ‡é€šå¸¸æ˜¯è¶Šå¤§è¶Šå¥½

        if self.learning_rate is None:  # è‹¥æœªæ‰‹åŠ¨è®¾ç½®å­¦ä¹ ç‡
            if self.train_type == 'full':  # å…¨é‡è®­ç»ƒé»˜è®¤æ›´å°å­¦ä¹ ç‡
                self.learning_rate = 1e-5  # è®¾å®šå…¨é‡è®­ç»ƒé»˜è®¤å­¦ä¹ ç‡
            else:  # å…¶ä»–ï¼ˆå¦‚ LoRA/Adapterï¼‰è®­ç»ƒå¯ç”¨ç¨å¤§å­¦ä¹ ç‡
                self.learning_rate = 1e-4  # è®¾å®šå¾®è°ƒé»˜è®¤å­¦ä¹ ç‡
        self._init_eval_strategy()  # æœ€ååŸºäºä¿å­˜ç­–ç•¥å®Œå–„è¯„ä¼°ç­–ç•¥


@dataclass  # ä½œä¸ºçº¯å‚æ•°æ‰¿è½½ä½“çš„æ•°æ®ç±»
class SwanlabArguments:
    """
    ç±»è¯´æ˜
    -----
    å°è£…ä¸ SwanLab é›†æˆæ‰€éœ€çš„å¯é€‰å‚æ•°ï¼ŒåŒ…æ‹¬é‰´æƒã€é¡¹ç›®/å·¥ä½œç©ºé—´ã€å®éªŒåã€ä»¥åŠå¯é€‰çš„é£ä¹¦é€šçŸ¥é…ç½®ã€‚

    ä¸»è¦å±æ€§
    -------
    - swanlab_token: ç™»å½• tokenï¼Œç”¨äºéäº¤äº’å¼é‰´æƒã€‚
    - swanlab_project: é¡¹ç›®åã€‚
    - swanlab_workspace: å·¥ä½œç©ºé—´ã€‚
    - swanlab_exp_name: å®éªŒåï¼Œé»˜è®¤å›é€€ä¸º `output_dir`ã€‚
    - swanlab_lark_webhook_url/swanlab_lark_secret: é£ä¹¦ç¾¤æœºå™¨äººé€šçŸ¥é…ç½®ã€‚
    - swanlab_mode: è¿è¡Œæ¨¡å¼ï¼š'cloud' æˆ– 'local'ã€‚

    ç¤ºä¾‹
    ---
    >>> args = SwanlabArguments(swanlab_project='demo', swanlab_mode='cloud')
    """

    swanlab_token: Optional[str] = None  # SwanLab ç™»å½• tokenï¼Œç”¨äºè‡ªåŠ¨åŒ–ç™»å½•
    swanlab_project: Optional[str] = None  # SwanLab é¡¹ç›®åç§°
    swanlab_workspace: Optional[str] = None  # SwanLab å·¥ä½œç©ºé—´
    swanlab_exp_name: Optional[str] = None  # å®éªŒåç§°ï¼›é»˜è®¤ä½¿ç”¨ output_dir
    swanlab_lark_webhook_url: Optional[str] = None  # é£ä¹¦é€šçŸ¥æœºå™¨äºº webhookï¼ˆå¯é€‰ï¼‰
    swanlab_lark_secret: Optional[str] = None  # é£ä¹¦æœºå™¨äººç­¾å secretï¼ˆå¯é€‰ï¼‰
    swanlab_mode: Literal['cloud', 'local'] = 'cloud'  # è¿è¡Œæ¨¡å¼ï¼Œé»˜è®¤ä¸ºäº‘ç«¯
    
    def _init_swanlab(self):
        """
        åˆå§‹åŒ– SwanLab é›†æˆï¼šæ£€æŸ¥å¯ç”¨æ€§ã€å‡†å¤‡å®éªŒåã€å®Œæˆç™»å½•ä¸å›è°ƒæ³¨å†Œã€‚

        å‚æ•°/è¿”å›
        --------
        æ— ï¼›è¯¥æ–¹æ³•é€šè¿‡å‰¯ä½œç”¨ä¸ç¬¬ä¸‰æ–¹åº“äº¤äº’ã€‚

        ç¤ºä¾‹
        ----
        >>> args.report_to = ['swanlab']
        >>> args._init_swanlab()  # å®Œæˆ SwanLab åˆå§‹åŒ–ï¼ˆè‹¥å·²å®‰è£…ï¼‰
        """
        if not is_swanlab_available():  # è‹¥æœªå®‰è£… SwanLab åŒ…åˆ™ç›´æ¥æŠ¥é”™æç¤ºå®‰è£…
            raise ValueError('You are using swanlab as `report_to`, please install swanlab by ' '`pip install swanlab`')  # æ˜ç¡®å®‰è£…æŒ‡å¼•
        if not self.swanlab_exp_name:  # è‹¥æœªæ˜¾å¼è®¾ç½®å®éªŒå
            self.swanlab_exp_name = self.output_dir  # é»˜è®¤ä½¿ç”¨è¾“å‡ºç›®å½•ä½œä¸ºå®éªŒå
        from importlib import import_module  # åŠ¨æ€å¯¼å…¥ï¼Œé¿å…é™æ€æ£€æŸ¥å¯¹å¯é€‰ä¾èµ–æŠ¥æœªè§£æè­¦å‘Š
        INTEGRATION_TO_CALLBACK = import_module('transformers.integrations').INTEGRATION_TO_CALLBACK  # åŠ¨æ€è·å–å›è°ƒæ³¨å†Œè¡¨
        swanlab = import_module('swanlab')  # åŠ¨æ€å¯¼å…¥ swanlab ä¸»åŒ…
        SwanLabCallback = import_module('swanlab.integration.transformers').SwanLabCallback  # åŠ¨æ€è·å– SwanLabCallback ç±»å‹
        if self.swanlab_token:  # æä¾›äº† token åˆ™è¿›è¡Œæ— å¤´ç™»å½•
            swanlab.login(self.swanlab_token)  # æ‰§è¡Œç™»å½•

        if self.swanlab_lark_webhook_url is not None:  # é…ç½®äº†é£ä¹¦é€šçŸ¥
            LarkCallback = import_module('swanlab.plugin.notification').LarkCallback  # åŠ¨æ€è·å–é£ä¹¦é€šçŸ¥å›è°ƒ
            lark_callback = LarkCallback(  # æ„é€ é£ä¹¦å›è°ƒå¯¹è±¡
                webhook_url=self.swanlab_lark_webhook_url,  # æŒ‡å®š webhook
                secret=self.swanlab_lark_secret,  # æŒ‡å®šç­¾å secretï¼ˆå¯é€‰ï¼‰
            )
            swanlab.register_callbacks([lark_callback])  # åœ¨ SwanLab ä¸­æ³¨å†Œè¯¥å›è°ƒ

        INTEGRATION_TO_CALLBACK['swanlab'] = SwanLabCallback(  # å°† 'swanlab' æ³¨å†Œä¸º transformers å¯è¯†åˆ«çš„å›è°ƒ
            project=self.swanlab_project,  # SwanLab é¡¹ç›®
            workspace=self.swanlab_workspace,  # SwanLab å·¥ä½œç©ºé—´
            experiment_name=self.swanlab_exp_name,  # å®éªŒåç§°
            config={'UPPERFRAME': 'ğŸ¦â€â¬›ms-swift'},  # é™„åŠ é…ç½®ï¼Œæ ‡æ³¨ä¸Šå±‚æ¡†æ¶æ¥æº
            mode=self.swanlab_mode,  # è¿è¡Œæ¨¡å¼ï¼ˆäº‘/æœ¬åœ°ï¼‰
        )


@dataclass  # æ±‡æ€»è®­ç»ƒæ‰€éœ€å…¨éƒ¨å‚æ•°çš„æ•°æ®ç±»
class TrainArguments(SwanlabArguments, TunerArguments, BaseArguments, Seq2SeqTrainingOverrideArguments):
    """
    ç±»è¯´æ˜
    -----
    æ±‡é›†åŸºç¡€å‚æ•°ã€è°ƒä¼˜å‚æ•°ä¸åºåˆ—åˆ°åºåˆ—è®­ç»ƒå‚æ•°çš„ç»Ÿä¸€å…¥å£ï¼Œå®Œæˆä»æ•°æ®ç±»åˆ°åº•å±‚è®­ç»ƒå‚æ•°çš„æ¡¥æ¥ï¼Œ
    å¹¶åœ¨åˆå§‹åŒ–é˜¶æ®µæ‰§è¡Œä¸€ç³»åˆ—ä¸å¹³å°ã€DeepSpeedã€è®¾å¤‡ã€è¯„ä¼°ç­–ç•¥ã€æ—¥å¿—ç›®å½•ç­‰ç›¸å…³çš„å‡†å¤‡å·¥ä½œã€‚

    ç»§æ‰¿å…³ç³»
    -------
    - SwanlabArguments: SwanLab é›†æˆå‚æ•°ã€‚
    - TunerArguments: å¾®è°ƒç›¸å…³å‚æ•°ã€‚
    - BaseArguments: é€šç”¨åŸºç¡€å‚æ•°ï¼ˆè®¾å¤‡ã€æ•°æ®é›†ç­‰ï¼‰ã€‚
    - Seq2SeqTrainingOverrideArguments: è¦†ç›–çš„ HF è®­ç»ƒå‚æ•°é»˜è®¤é€»è¾‘ã€‚

    å…³é”®å­—æ®µ
    -------
    - add_version: æ˜¯å¦åœ¨è¾“å‡ºç›®å½•è¿½åŠ ç‰ˆæœ¬æ ‡è¯†ï¼ˆæ—¶é—´æˆ³ç­‰ï¼‰ã€‚
    - loss_type/metric: æ’ä»¶ç”Ÿæ€ä¸­çš„æŸå¤±ä¸è¯„ä¼°æŒ‡æ ‡åç§°ã€‚
    - max_new_tokens/temperature: æ¨ç†ç›¸å…³çš„è¾…åŠ©å‚æ•°ï¼ˆåœ¨è®­ç»ƒè„šæœ¬ä¸­äº¦å¯ä¼ å…¥ä»¥ä¾¿ç»Ÿä¸€ï¼‰ã€‚
    - zero_hpz_partition_size/deepspeed_autotp_size: DeepSpeed ç›¸å…³åŠ¨æ€æ³¨å…¥é…ç½®ã€‚

    ç¤ºä¾‹
    ---
    >>> args = TrainArguments(dataset=['/path/to/ds'], cached_dataset=[], output_dir='output/run')
    >>> # args.__post_init__ å°†è‡ªåŠ¨å®Œæˆ DeepSpeed/è®¾å¤‡/è¯„ä¼°/æ—¥å¿—ç­‰å‡†å¤‡
    """
    add_version: bool = True  # æ˜¯å¦ç»™ output_dir åŠ¨æ€è¿½åŠ ç‰ˆæœ¬åç¼€
    create_checkpoint_symlink: bool = False  # æ˜¯å¦åˆ›å»º checkpoint çš„ç¬¦å·é“¾æ¥ï¼Œä¾¿äºå®šä½æœ€æ–°æƒé‡

    # plugin
    loss_type: Optional[str] = field(default=None, metadata={'help': f'loss_func choices: {list(LOSS_MAPPING.keys())}'})  # è‡ªå®šä¹‰æŸå¤±å‡½æ•°ç±»å‹åç§°
    metric: Optional[str] = None  # è¯„ä¼°æŒ‡æ ‡åç§°ï¼ˆæ’ä»¶ä¾§å®šä¹‰ï¼‰

    # extra
    max_new_tokens: int = 64  # æ¨ç†é˜¶æ®µé»˜è®¤ç”Ÿæˆçš„æœ€å¤§æ–°æ ‡è®°æ•°
    temperature: float = 0.  # é‡‡æ ·æ¸©åº¦ï¼Œé»˜è®¤ä¸ºè´ªå¿ƒï¼ˆ0ï¼‰
    load_args: bool = False  # æ˜¯å¦ä»ç£ç›˜åŠ è½½å†å²å‚æ•°ï¼ˆä¸Šå±‚å¯ç”¨ï¼‰

    max_new_tokens: int = 64
    temperature: float = 0.
    load_args: bool = False

    # zero++
    zero_hpz_partition_size: Optional[int] = None  # ZeRO++ åˆ†åŒºå¤§å°ï¼Œå­˜åœ¨æ—¶æ³¨å…¥åˆ° DeepSpeed é…ç½®

    # auto_tp
    deepspeed_autotp_size: Optional[int] = None  # è‡ªåŠ¨å¼ é‡å¹¶è¡Œå¤§å°ï¼ˆAutoTPï¼‰ï¼Œå­˜åœ¨æ—¶æ³¨å…¥åˆ° DeepSpeed é…ç½®

    def __post_init__(self) -> None:
        """
        dataclass åˆå§‹åŒ–åé’©å­ï¼šä¸²è”åŸºç¡€/è¦†ç›–/è°ƒä¼˜å‚æ•°çš„ååˆå§‹åŒ–æµç¨‹ï¼Œå¹¶å®Œæˆï¼š
        - åŠŸèƒ½æ£€æŸ¥ä¸å®‰å…¨çº¦æŸï¼›
        - è®¾å¤‡ã€å¹³å°ä¸ DeepSpeed é…ç½®ï¼›
        - è®­ç»ƒå‚æ•°ç”Ÿæˆä¸æ—¥å¿—ç›®å½•å‡†å¤‡ï¼›
        - SwanLab é›†æˆã€‚

        ç¤ºä¾‹
        ----
        >>> args = TrainArguments(dataset=['ds'], cached_dataset=[])
        >>> args.__post_init__()  # å†…éƒ¨å‰¯ä½œç”¨å¼åˆå§‹åŒ–
        """
        if self.padding_free or self.packing:  # è‹¥å¯ç”¨ padding_free æˆ–æ ·æœ¬æ‰“åŒ…
            if self.packing:  # packing ä¸ padding_free äº’æ–¥ï¼Œä¼˜å…ˆè®¾ç½® packing
                feature = 'packing'  # è®°å½•å¯ç”¨çš„åŠŸèƒ½å
                self.padding_free = False  # packing æ—¶æ˜¾å¼å…³é—­ padding_free
            else:
                feature = 'padding_free'  # ä»…å¯ç”¨ padding_free
            if self.attn_impl not in {'flash_attn', 'flash_attention_2', 'flash_attention_3'}:  # è¿™ä¸¤è€…éœ€è¦ FlashAttention æ”¯æŒ
                raise ValueError(f'The "{feature}" feature requires a flash attention implementation. '  # è‹¥æœªæ»¡è¶³åˆ™æŠ¥é”™æç¤º
                                 'Please use one of: "flash_attn", "flash_attention_2", "flash_attention_3".')
        if self.resume_from_checkpoint:  # é…ç½®äº†ä» checkpoint æ¢å¤
            self.resume_from_checkpoint = to_abspath(self.resume_from_checkpoint, True)  # è§„èŒƒåŒ–ä¸ºç»å¯¹è·¯å¾„ï¼ˆå¯ä¸å­˜åœ¨ï¼‰
            # The non-resume_only_model will have its weights loaded in the trainer.  # è¯´æ˜ï¼šé resume_only_model æ—¶æƒé‡ç”± trainer å¤„ç†
            if self.resume_only_model:  # è‹¥ä»…æ¢å¤æ¨¡å‹æƒé‡è€Œéè®­ç»ƒçŠ¶æ€
                if self.train_type == 'full':  # å…¨é‡è®­ç»ƒç›´æ¥å°†æ¨¡å‹è·¯å¾„èµ‹ç»™ model
                    self.model = self.resume_from_checkpoint  # è®¾ç½®å¾…åŠ è½½çš„åŸºç¡€æ¨¡å‹è·¯å¾„
                else:  # é€‚é…å‚æ•°åŒ–è®­ç»ƒï¼ˆå¦‚ LoRAï¼‰
                    self.adapters = [self.resume_from_checkpoint]  # ä»¥é€‚é…å™¨è·¯å¾„åˆ—è¡¨å½¢å¼ä¼ é€’
        BaseArguments.__post_init__(self)  # å…ˆåˆå§‹åŒ–åŸºç¡€å‚æ•°ï¼ˆè®¾å¤‡ã€seedã€æ•°æ®ç›¸å…³ç­‰ï¼‰
        Seq2SeqTrainingOverrideArguments.__post_init__(self)  # å†åˆå§‹åŒ–è¦†ç›–çš„ HF è®­ç»ƒå‚æ•°é€»è¾‘
        TunerArguments.__post_init__(self)  # æœ€ååˆå§‹åŒ–è°ƒä¼˜ç›¸å…³å‚æ•°

        if self.optimizer is None:  # è‹¥æœªæ˜¾å¼é€‰æ‹©ä¼˜åŒ–å™¨
            if self.lorap_lr_ratio:  # æŒ‡å®šäº† LoRA+ å­¦ä¹ ç‡æ¯”ä¾‹æ—¶ï¼Œä½¿ç”¨ lorap ä¼˜åŒ–å™¨
                self.optimizer = 'lorap'  # é€‰æ‹© lorap
            elif self.use_galore:  # å¯ç”¨ GaLore ä½ç§©ä¼˜åŒ–æ—¶
                self.optimizer = 'galore'  # é€‰æ‹© galore

        if len(self.dataset) == 0 and len(self.cached_dataset) == 0:  # æœªæä¾›ä»»ä½•è®­ç»ƒæ•°æ®
            raise ValueError(f'self.dataset: {self.dataset}, self.cached_dataset: {self.cached_dataset}. '  # ç›´æ¥æŠ¥é”™æç¤ºå¿…é¡»æä¾›æ•°æ®
                             'Please input the training dataset.')

        self._handle_pai_compat()  # é’ˆå¯¹ PAI è®­ç»ƒä½œä¸šåšå…¼å®¹ï¼ˆæ—¥å¿—ç›®å½•/ç‰ˆæœ¬æ§åˆ¶ï¼‰

        self._init_deepspeed()  # åˆå§‹åŒ–ä¸è§£æ DeepSpeed é…ç½®
        self._init_device()  # åˆå§‹åŒ–è®¾å¤‡/åˆ†å¸ƒå¼é…ç½®ï¼ˆç”±åŸºç±»æä¾›ï¼‰

        if getattr(self, 'accelerator_config', None) is None:  # è‹¥æœªé…ç½®åŠ é€Ÿå™¨å‚æ•°
            self.accelerator_config = {'dispatch_batches': False}  # è®¾ç½®é»˜è®¤åŠ é€Ÿå™¨è¡Œä¸ºï¼ˆä¸æ‹†æ‰¹è°ƒåº¦ï¼‰
        if self.split_dataset_ratio == 0 and not self.val_dataset and not self.eval_dataset:  # å®Œå…¨æ— éªŒè¯é›†
            self.eval_strategy = 'no'  # æ˜ç¡®ä¸è¿›è¡Œè¯„ä¼°
        self.training_args = TrainerFactory.get_training_args(self)  # é€šè¿‡å·¥å‚åŸºäºæœ¬å®ä¾‹ç”Ÿæˆ HF çš„ TrainingArguments
        self.training_args.remove_unused_columns = False  # ä¿ç•™æ•°æ®é›†ä¸­æœªè¢«ä½¿ç”¨çš„åˆ—ï¼Œä¾¿äºè‡ªå®šä¹‰ collator
        self._add_version()  # å¤„ç†è¾“å‡ºç›®å½•ç‰ˆæœ¬åç¼€ä¸æ—¥å¿—ç›®å½•

        if 'swanlab' in self.report_to:  # è‹¥ä¸Šå±‚è¦æ±‚ä¸ŠæŠ¥åˆ° SwanLab
            self._init_swanlab()  # å®Œæˆ SwanLab åˆå§‹åŒ–

    def _init_deepspeed(self):
        """
        åˆå§‹åŒ– DeepSpeedï¼š
        - æ”¯æŒç®€å†™åç§°åˆ°é¢„è®¾ JSON çš„æ˜ å°„ï¼›
        - å°†å­—ç¬¦ä¸²/è·¯å¾„é…ç½®è§£æä¸ºå­—å…¸ï¼›
        - åŠ¨æ€æ³¨å…¥ ZeRO++ ä¸ AutoTP é…ç½®é¡¹ï¼›
        - åšå¥½ä¸ device_map çš„äº’æ–¥æ ¡éªŒã€‚

        ç¤ºä¾‹
        ----
        >>> args.deepspeed = 'zero2'  # å°†æ˜ å°„åˆ°å†…ç½®é…ç½®æ–‡ä»¶
        >>> args._init_deepspeed()
        """
        if self.deepspeed:  # ä»…åœ¨å¯ç”¨ DeepSpeed æ—¶å¤„ç†
            require_version('deepspeed')  # ç¡®è®¤å·²å®‰è£… DeepSpeed åŒ…
            if is_mp():  # è‹¥å¯ç”¨äº† device_mapï¼ˆMPï¼‰åˆ™ä¸ DeepSpeed å†²çª
                raise ValueError('DeepSpeed is not compatible with `device_map`. '  # æŠ›å‡ºæ˜ç¡®é”™è¯¯ä¸ç¯å¢ƒä¿¡æ¯
                                 f'n_gpu: {get_device_count()}, '
                                 f'local_world_size: {self.local_world_size}.')

            ds_config_folder = os.path.abspath(os.path.join(os.path.dirname(__file__), '..', 'ds_config'))  # é¢„è®¾é…ç½®å­˜æ”¾ç›®å½•
            deepspeed_mapping = {  # å°†å‹å¥½åç§°æ˜ å°„ä¸º JSON æ–‡ä»¶å
                name: f'{name}.json'
                for name in ['zero0', 'zero1', 'zero2', 'zero3', 'zero2_offload', 'zero3_offload']
            }
            for ds_name, ds_config in deepspeed_mapping.items():  # è‹¥ç”¨æˆ·ä¼ å…¥çš„æ˜¯ä¸Šè¿°ç®€å†™åç§°
                if self.deepspeed == ds_name:  # å‘½ä¸­ç®€å†™
                    self.deepspeed = os.path.join(ds_config_folder, ds_config)  # æ‹¼æ¥å‡ºå®é™… JSON è·¯å¾„
                    break  # å®Œæˆæ˜ å°„

            self.deepspeed = json_parse_to_dict(self.deepspeed)  # å°†è·¯å¾„/å­—ç¬¦ä¸²è§£æä¸º dictï¼ˆæˆ–ä¿æŒåŸ dictï¼‰
            if self.zero_hpz_partition_size is not None:  # è‹¥æŒ‡å®š ZeRO++ åˆ†åŒºå¤§å°
                assert 'zero_optimization' in self.deepspeed  # æ–­è¨€é…ç½®åŒ…å« zero_optimization èŠ‚ç‚¹
                self.deepspeed['zero_optimization']['zero_hpz_partition_size'] = self.zero_hpz_partition_size  # æ³¨å…¥å‚æ•°
                logger.warn('If `zero_hpz_partition_size`(ZeRO++) causes grad_norm NaN, please'  # ç»™å‡ºæ½œåœ¨æ•°å€¼ä¸ç¨³å®šçš„æç¤º
                            ' try `--torch_dtype float16`')
            if self.deepspeed_autotp_size is not None:  # è‹¥å¯ç”¨ AutoTP
                assert self.deepspeed is not None, (  # éœ€è¦å·²ç»å¯ç”¨ DeepSpeed
                    'To use `deepspeed_autotp_size`, you need to additionally set the `--deepspeed` argument.')
                self.deepspeed['tensor_parallel'] = {'autotp_size': self.deepspeed_autotp_size}  # æ³¨å…¥å¼ é‡å¹¶è¡Œé…ç½®
                self.deepspeed['zero_optimization']['gather_16bit_weights_on_model_save'] = True  # ä¿å­˜æ—¶èšåˆ 16bit æƒé‡
            logger.info(f'Using deepspeed: {self.deepspeed}')  # è®°å½•æœ€ç»ˆä½¿ç”¨çš„ DeepSpeed é…ç½®

    def _handle_pai_compat(self) -> None:
        """
        å¤„ç†åœ¨é˜¿é‡Œäº‘ PAI è®­ç»ƒä½œä¸šç¯å¢ƒä¸‹çš„å…¼å®¹é€»è¾‘ï¼š
        - è‹¥æ£€æµ‹åˆ° PAI ç¯å¢ƒï¼Œä¸º logging_dir èµ‹é»˜è®¤çš„ PAI TensorBoard è·¯å¾„ï¼›
        - å…³é—­è¾“å‡ºç›®å½•ç‰ˆæœ¬è¿½åŠ ï¼Œé¿å…è·¯å¾„ç®¡ç†å¤æ‚åŒ–ã€‚

        ç¤ºä¾‹
        ----
        >>> if is_pai_training_job():
        ...     args._handle_pai_compat()
        """
        if not is_pai_training_job():  # é PAI ç¯å¢ƒåˆ™ç›´æ¥è¿”å›
            return  # ä¿æŒæœ¬åœ°/å…¶ä»–å¹³å°é»˜è®¤è¡Œä¸º

        logger.info('Handle pai compat...')  # è®°å½•å¼€å§‹å¤„ç† PAI å…¼å®¹
        pai_tensorboard_dir = get_pai_tensorboard_dir()  # è·å– PAI ç¯å¢ƒé»˜è®¤çš„ TensorBoard ç›®å½•
        if self.logging_dir is None and pai_tensorboard_dir is not None:  # æœªæŒ‡å®š logging_dir ä¸” PAI æä¾›äº†é»˜è®¤ç›®å½•
            self.logging_dir = pai_tensorboard_dir  # ä½¿ç”¨ PAI çš„è·¯å¾„
            logger.info(f'Setting args.logging_dir: {self.logging_dir}')  # è®°å½•å˜æ›´
        self.add_version = False  # PAI ç¯å¢ƒä¸‹é€šå¸¸ä¸è¿½åŠ ç‰ˆæœ¬åç¼€
        logger.info(f'Setting args.add_version: {self.add_version}')  # è®°å½•å˜æ›´

    def _add_version(self):
        """
        å‡†å¤‡è¾“å‡ºä¸æ—¥å¿—ç›®å½•ï¼š
        - éœ€è¦æ—¶ç»™ `output_dir` è¿½åŠ ç‰ˆæœ¬ä¿¡æ¯ï¼›
        - ç»Ÿä¸€è®¾ç½® `logging_dir`ï¼Œå¹¶ç¡®ä¿ç›®å½•åˆ›å»ºï¼›
        - å°†è·¯å¾„åŒæ­¥å› `training_args` ä¾› Trainer ä½¿ç”¨ã€‚

        ç¤ºä¾‹
        ----
        >>> args.output_dir = 'output/run'
        >>> args._add_version()  # æœ€ç»ˆä¼šåˆ›å»ºç›®å½•å¹¶åŒæ­¥åˆ° training_args
        """
        if self.add_version:  # å…è®¸ä¸ºè¾“å‡ºç›®å½•è¿½åŠ ç‰ˆæœ¬ï¼ˆæ—¶é—´æˆ³/å¢é‡å·ï¼‰
            self.output_dir = add_version_to_work_dir(self.output_dir)  # ç”Ÿæˆå¸¦ç‰ˆæœ¬çš„è¾“å‡ºç›®å½•
            logger.info(f'output_dir: {self.output_dir}')  # è®°å½•æœ€ç»ˆè¾“å‡ºç›®å½•

        if self.logging_dir is None:  # è‹¥æœªæŒ‡å®šæ—¥å¿—ç›®å½•
            self.logging_dir = f'{self.output_dir}/runs'  # é»˜è®¤æ”¾åœ¨è¾“å‡ºç›®å½•ä¸‹çš„ runs å­ç›®å½•

        self.logging_dir = to_abspath(self.logging_dir)  # è§„èŒƒåŒ–æ—¥å¿—ç›®å½•ä¸ºç»å¯¹è·¯å¾„
        if is_master():  # ä»…ä¸»è¿›ç¨‹åˆ›å»ºç›®å½•ï¼Œé¿å…å¹¶å‘å†²çª
            os.makedirs(self.output_dir, exist_ok=True)  # ç¡®ä¿è¾“å‡ºç›®å½•å­˜åœ¨

        if self.run_name is None:  # è‹¥æœªæŒ‡å®š run_name
            self.run_name = self.output_dir  # é»˜è®¤ä½¿ç”¨è¾“å‡ºç›®å½•ä½œä¸ºè¿è¡Œå

        self.training_args.output_dir = self.output_dir  # åŒæ­¥è¾“å‡ºç›®å½•åˆ° HF TrainingArguments
        self.training_args.run_name = self.run_name  # åŒæ­¥è¿è¡Œå
        self.training_args.logging_dir = self.logging_dir  # åŒæ­¥æ—¥å¿—ç›®å½•
