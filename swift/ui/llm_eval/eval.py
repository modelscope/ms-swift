# Copyright (c) Alibaba, Inc. and its affiliates.
from typing import Type

import gradio as gr

from swift.ui.base import BaseUI
from swift.utils import get_logger

logger = get_logger()


class Eval(BaseUI):

    group = 'llm_eval'

    locale_dict = {
        'eval_dataset': {
            'label': {
                'zh': 'è¯„æµ‹æ•°æ®é›†',
                'en': 'Evaluation dataset'
            },
            'info': {
                'zh': 'é€‰æ‹©è¯„æµ‹æ•°æ®é›†ï¼Œæ”¯æŒå¤šé€‰',
                'en': 'Select eval dataset, multiple datasets supported'
            }
        },
        'eval_limit': {
            'label': {
                'zh': 'è¯„æµ‹æ•°æ®ä¸ªæ•°',
                'en': 'Eval numbers for each dataset'
            },
            'info': {
                'zh': 'æ¯ä¸ªè¯„æµ‹é›†çš„å–æ ·æ•°',
                'en': 'Number of rows sampled from each dataset'
            }
        },
        'eval_output_dir': {
            'label': {
                'zh': 'è¯„æµ‹è¾“å‡ºç›®å½•',
                'en': 'Eval output dir'
            },
            'info': {
                'zh': 'è¯„æµ‹ç»“æœçš„è¾“å‡ºç›®å½•',
                'en': 'The dir to save the eval results'
            }
        },
        'custom_eval_config': {
            'label': {
                'zh': 'è‡ªå®šä¹‰æ•°æ®é›†è¯„æµ‹é…ç½®',
                'en': 'Custom eval config'
            },
            'info': {
                'zh': 'å¯ä»¥ä½¿ç”¨è¯¥é…ç½®è¯„æµ‹è‡ªå·±çš„æ•°æ®é›†ï¼Œè¯¦è§githubæ–‡æ¡£çš„è¯„æµ‹éƒ¨åˆ†',
                'en': 'Use this config to eval your own datasets, check the docs in github for details'
            }
        },
        'eval_url': {
            'label': {
                'zh': 'è¯„æµ‹é“¾æ¥',
                'en': 'The eval url'
            },
            'info': {
                'zh':
                'OpenAIæ ·å¼çš„è¯„æµ‹é“¾æ¥(å¦‚ï¼šhttp://localhost:8080/v1/chat/completions)ï¼Œç”¨äºè¯„æµ‹æ¥å£ï¼ˆæ¨¡å‹ç±»å‹è¾“å…¥ä¸ºå®é™…æ¨¡å‹ç±»å‹ï¼‰',
                'en':
                'The OpenAI style link(like: http://localhost:8080/v1/chat/completions) for '
                'evaluation(Input actual model type into model_type)'
            }
        },
        'api_key': {
            'label': {
                'zh': 'æ¥å£token',
                'en': 'The url token'
            },
            'info': {
                'zh': 'eval_urlçš„token',
                'en': 'The token used with eval_url'
            }
        },
        'infer_backend': {
            'label': {
                'zh': 'æ¨ç†æ¡†æ¶',
                'en': 'Infer backend'
            },
        }
    }

    @classmethod
    def do_build_ui(cls, base_tab: Type['BaseUI']):
        try:
            from evalscope.backend.opencompass import OpenCompassBackendManager
            from evalscope.backend.vlm_eval_kit import VLMEvalKitBackendManager
            eval_dataset_list = (
                OpenCompassBackendManager.list_datasets() + VLMEvalKitBackendManager.list_supported_datasets())
            logger.warn('If you encounter an error messageğŸ‘†ğŸ»ğŸ‘†ğŸ»ğŸ‘†ğŸ» of `.env` file, please ignore.')
        except Exception as e:
            logger.warn(e)
            logger.warn(
                ('The error message ğŸ‘†ğŸ»ğŸ‘†ğŸ»ğŸ‘†ğŸ»above will have no bad effects, '
                 'only means evalscope is not installed, and default eval datasets will be listed in the web-ui.'))
            eval_dataset_list = [
                'AX_b', 'cmb', 'winogrande', 'mmlu', 'afqmc', 'COPA', 'commonsenseqa', 'CMRC', 'lcsts', 'nq',
                'ocnli_fc', 'math', 'mbpp', 'DRCD', 'TheoremQA', 'CB', 'ReCoRD', 'lambada', 'tnews', 'flores',
                'humaneval', 'AX_g', 'ceval', 'bbh', 'BoolQ', 'MultiRC', 'piqa', 'csl', 'ARC_c', 'agieval', 'cmnli',
                'strategyqa', 'gsm8k', 'summedits', 'eprstmt', 'WiC', 'cluewsc', 'Xsum', 'ocnli', 'triviaqa',
                'hellaswag', 'race', 'bustm', 'RTE', 'C3', 'GaokaoBench', 'storycloze', 'ARC_e', 'siqa', 'obqa', 'WSC',
                'chid', 'COCO_VAL', 'MME', 'HallusionBench', 'POPE', 'MMBench_DEV_EN', 'MMBench_TEST_EN',
                'MMBench_DEV_CN', 'MMBench_TEST_CN', 'MMBench', 'MMBench_CN', 'MMBench_DEV_EN_V11',
                'MMBench_TEST_EN_V11', 'MMBench_DEV_CN_V11', 'MMBench_TEST_CN_V11', 'MMBench_V11', 'MMBench_CN_V11',
                'SEEDBench_IMG', 'SEEDBench2', 'SEEDBench2_Plus', 'ScienceQA_VAL', 'ScienceQA_TEST', 'MMT-Bench_ALL_MI',
                'MMT-Bench_ALL', 'MMT-Bench_VAL_MI', 'MMT-Bench_VAL', 'AesBench_VAL', 'AesBench_TEST', 'CCBench',
                'AI2D_TEST', 'MMStar', 'RealWorldQA', 'MLLMGuard_DS', 'BLINK', 'OCRVQA_TEST', 'OCRVQA_TESTCORE',
                'TextVQA_VAL', 'DocVQA_VAL', 'DocVQA_TEST', 'InfoVQA_VAL', 'InfoVQA_TEST', 'ChartQA_TEST', 'MathVision',
                'MathVision_MINI', 'MMMU_DEV_VAL', 'MMMU_TEST', 'OCRBench', 'MathVista_MINI', 'LLaVABench', 'MMVet',
                'MTVQA_TEST', 'MMLongBench_DOC', 'VCR_EN_EASY_500', 'VCR_EN_EASY_100', 'VCR_EN_EASY_ALL',
                'VCR_EN_HARD_500', 'VCR_EN_HARD_100', 'VCR_EN_HARD_ALL', 'VCR_ZH_EASY_500', 'VCR_ZH_EASY_100',
                'VCR_ZH_EASY_ALL', 'VCR_ZH_HARD_500', 'VCR_ZH_HARD_100', 'VCR_ZH_HARD_ALL', 'MMDU', 'MMBench-Video',
                'Video-MME', 'MMBench_DEV_EN', 'MMBench_TEST_EN', 'MMBench_DEV_CN', 'MMBench_TEST_CN', 'MMBench',
                'MMBench_CN', 'MMBench_DEV_EN_V11', 'MMBench_TEST_EN_V11', 'MMBench_DEV_CN_V11', 'MMBench_TEST_CN_V11',
                'MMBench_V11', 'MMBench_CN_V11', 'SEEDBench_IMG', 'SEEDBench2', 'SEEDBench2_Plus', 'ScienceQA_VAL',
                'ScienceQA_TEST', 'MMT-Bench_ALL_MI', 'MMT-Bench_ALL', 'MMT-Bench_VAL_MI', 'MMT-Bench_VAL',
                'AesBench_VAL', 'AesBench_TEST', 'CCBench', 'AI2D_TEST', 'MMStar', 'RealWorldQA', 'MLLMGuard_DS',
                'BLINK'
            ]

        with gr.Row():
            gr.Dropdown(
                elem_id='eval_dataset',
                is_list=True,
                choices=eval_dataset_list,
                multiselect=True,
                allow_custom_value=True,
                scale=20)
            gr.Textbox(elem_id='eval_limit', scale=20)
            gr.Dropdown(elem_id='infer_backend', scale=20)
        with gr.Row():
            gr.Textbox(elem_id='custom_eval_config', scale=20)
            gr.Textbox(elem_id='eval_output_dir', scale=20)
            gr.Textbox(elem_id='eval_url', scale=20)
            gr.Textbox(elem_id='api_key', scale=20)
