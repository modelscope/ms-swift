# SWIFT (Scalable lightWeight Infrastructure for Fine-Tuning)

<p align="center">
    <br>
    <img src="https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif" width="400"/>
    <br>
<p>

<p align="center">
<a href="https://modelscope.cn/home">é­”æ­ç¤¾åŒº</a>
<br>
        ä¸­æ–‡&nbsp ï½œ &nbsp<a href="README.md">English</a>
</p>

<p align="center">
<img src="https://img.shields.io/badge/python-%E2%89%A53.8-5be.svg">
<img src="https://img.shields.io/badge/pytorch-%E2%89%A51.12%20%7C%20%E2%89%A52.0-orange.svg">
<a href="https://github.com/modelscope/modelscope/"><img src="https://img.shields.io/badge/modelscope-%E2%89%A51.9.3-5D91D4.svg"></a>
<a href="https://github.com/modelscope/swift/"><img src="https://img.shields.io/badge/ms--swift-Build from source-6FEBB9.svg"></a>
</p>

##  ğŸ“– ç›®å½•
- [ç®€ä»‹](#-ç®€ä»‹)
- [æ–°é—»](#-æ–°é—»)
- [å¤§æ¨¡å‹è®­ç»ƒæ¨ç†](#-å¤§æ¨¡å‹è®­ç»ƒæ¨ç†)
- [å®‰è£…](#-å®‰è£…)
- [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [äº†è§£æ›´å¤š](#-äº†è§£æ›´å¤š)
- [License](#license)
- [è”ç³»æˆ‘ä»¬](#-è”ç³»æˆ‘ä»¬)

## ğŸ“ ç®€ä»‹
SWIFTï¼ˆScalable lightWeight Infrastructure for Fine-Tuningï¼‰æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„è½»é‡çº§ä¸€ç«™å¼è®­ç»ƒã€æ¨ç†æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚å®ƒé›†æˆäº†å„ç§é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œå¦‚LoRAã€QLoRAã€é˜¿é‡Œäº‘è‡ªç ”çš„ResTuning-Bypassç­‰ï¼Œä»¥åŠå¼€ç®±å³ç”¨çš„è®­ç»ƒæ¨ç†è„šæœ¬ï¼Œä½¿å¼€å‘è€…å¯ä»¥åœ¨å•å¼ å•†ä¸šçº§æ˜¾å¡ä¸Šå¾®è°ƒæ¨ç†LLM&AIGCæ¨¡å‹ã€‚æ­¤å¤–ï¼ŒSWIFTä¸[PEFT](https://github.com/huggingface/peft)å®Œå…¨å…¼å®¹ï¼Œä½¿å¼€å‘è€…å¯ä»¥åœ¨ModelScopeæ¨¡å‹ä½“ç³»ä¸­ä½¿ç”¨PEFTçš„èƒ½åŠ›ã€‚

ç›®å‰æ”¯æŒçš„æ–¹æ³•ï¼š

1. LoRAï¼š[LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2106.09685)
2. QA-LoRA:[Quantization-Aware Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2309.14717).
3. LongLoRA: [Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307)
4. Adapterï¼š[Parameter-Efficient Transfer Learning for NLP](http://arxiv.org/abs/1902.00751)
5. Prompt: [Visual Prompt Tuning](https://arxiv.org/abs/2203.12119)
6. Side: [Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks](https://arxiv.org/abs/1912.13503)
7. Res-Tuning: [Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone](https://arxiv.org/abs/2310.19859)  < [arXiv](https://arxiv.org/abs/2310.19859)  |  [Project Page](https://res-tuning.github.io/)  |  [Usage](docs/source/GetStarted/ResTuning.md) >
8. ROME: [Rank-One Editing of Encoder-Decoder Models](https://arxiv.org/abs/2211.13317)
9. NEFTune: [Noisy Embeddings Improve Instruction Finetuning](https://arxiv.org/abs/2310.05914)
10. æ‰€æœ‰åœ¨[PEFT](https://github.com/huggingface/peft)ä¸Šæä¾›çš„tuners

ä¸»è¦èƒ½åŠ›ï¼š
1. å¯ä»¥é€šè¿‡model-idä½¿SWIFTæˆ–PEFTçš„æ–¹æ³•ä½¿ç”¨ModelScope Hubä¸­çš„æ¨¡å‹
2. åœ¨å•æ¬¡è®­ç»ƒæˆ–æ¨ç†ä¸­å¯ä»¥ä½¿ç”¨å¤šä¸ªtuners
3. æ”¯æŒè°ƒç”¨`activate_adapter`æˆ–`deactivate_adapter`æˆ–`set_active_adapters`æ¥ä½¿éƒ¨åˆ†tuneræ¿€æ´»æˆ–å¤±æ´»ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¨ç†æ—¶åŒæ—¶åŠ è½½å¤šä¸ªç‹¬ç«‹çš„tunersåœ¨ä¸åŒçº¿ç¨‹ä¸­å¹¶è¡Œä½¿ç”¨ã€‚
4. æ”¯æŒé€šè¿‡è„šæœ¬æ–¹å¼å’Œå‘½ä»¤è¡Œæ–¹å¼å¼€å¯è®­ç»ƒå’Œæ¨ç†ï¼ŒåŒæ—¶æ”¯æŒWeb-UIæ–¹å¼è¿›è¡Œæ¨ç†ã€‚
5. æ”¯æŒæ¨¡å‹è®­ç»ƒåçš„éƒ¨ç½²é“¾è·¯(vllm/chatglm.cpp/xinference)ï¼Œè¯¦æƒ…å¯ä»¥æŸ¥çœ‹[å®˜æ–¹æ–‡æ¡£](./docs/source/GetStarted/éƒ¨ç½²æŒ‡å—.md)ã€‚

ç”¨æˆ·å¯ä»¥æŸ¥çœ‹ [SWIFTå®˜æ–¹æ–‡æ¡£](docs/source/GetStarted/å¿«é€Ÿä½¿ç”¨.md) æ¥äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚

## ğŸ‰ æ–°é—»
- 2023.12.5: æ”¯æŒæ¨¡å‹: zephyr-7b-beta-chat, openbuddy-zephyr-7b-chat. æ”¯æŒæ•°æ®é›†: hc3-zh, hc3-en.
- ğŸ”¥ 2023.12.2: [è‡ªæˆ‘è®¤çŸ¥å¾®è°ƒæœ€ä½³å®è·µ](https://github.com/modelscope/swift/blob/main/docs/source/LLM/è‡ªæˆ‘è®¤çŸ¥å¾®è°ƒæœ€ä½³å®è·µ.md), **10åˆ†é’Ÿå¯¹å¤§æ¨¡å‹è¿›è¡Œè‡ªæˆ‘è®¤çŸ¥å¾®è°ƒ**, åˆ›å»ºä¸“å±äºè‡ªå·±çš„å¤§æ¨¡å‹.
- ğŸ”¥ 2023.11.30: æ”¯æŒ**qwen-1_8b**, **qwen-72b**, **qwen-audio**ç³»åˆ—æ¨¡å‹çš„è®­ç»ƒçš„æ¨ç†. å¯¹åº”çš„shè„šæœ¬å¯ä»¥æŸ¥çœ‹[qwen_1_8b_chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_1_8b_chat), [qwen_72b_chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_72b_chat), [qwen_audio_chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_audio_chat)
- ğŸ”¥ 2023.11.29: æ”¯æŒ**AnimateDiff**çš„è®­ç»ƒå’Œæ¨ç†
- ğŸ”¥ 2023.11.24: æ”¯æŒ**yi-34b-chat**, **codefuse-codellama-34b-chat**æ¨¡å‹. å¯¹åº”çš„shè„šæœ¬å¯ä»¥æŸ¥çœ‹[yi_34b_chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/yi_34b_chat), [codefuse_codellama_34b_chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/codefuse_codellama_34b_chat).
- ğŸ”¥ 2023.11.18: æ”¯æŒ**tongyi-finance-14b**ç³»åˆ—æ¨¡å‹: tongyi-finance-14b, tongyi-finance-14b-chat, tongyi-finance-14b-chat-int4. å¯¹åº”çš„shè„šæœ¬å¯ä»¥æŸ¥çœ‹[tongyi_finance_14b_chat_int4](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/tongyi_finance_14b_chat_int4).
- 2023.11.16: æ”¯æŒæ›´å¤šæ¨¡å‹çš„**flash attn**æ”¯æŒ: qwenç³»åˆ—, qwen-vlç³»åˆ—, llamaç³»åˆ—, openbuddyç³»åˆ—, mistralç³»åˆ—, yiç³»åˆ—, ziyaç³»åˆ—. è¯·ä½¿ç”¨`use_flash_attn`å‚æ•°.
- ğŸ”¥ 2023.11.11: æ”¯æŒ**NEFTune**, ä½¿ç”¨`Swift.prepare_model(model, NEFTuneConfig())`å³å¯å¼€å¯.
- ğŸ”¥ 2023.11.11: æ”¯æŒ**å‘½ä»¤è¡Œ**è®­ç»ƒæ¨ç†å’Œ**Web-UI**æ¨ç†, è¯¦æƒ…å¯ä»¥æŸ¥çœ‹ä¸‹æ–¹çš„`ä½¿ç”¨Swift CLIè¿è¡Œ`ç« èŠ‚.
- ğŸ”¥ 2023.11.11: æ”¯æŒæ¨¡å‹è®­ç»ƒåçš„**éƒ¨ç½²**é“¾è·¯(vllm/chatglm.cpp/xinference)ï¼Œè¯¦æƒ…å¯ä»¥æŸ¥çœ‹[å®˜æ–¹æ–‡æ¡£](./docs/source/GetStarted/éƒ¨ç½²æŒ‡å—.md).
- ğŸ”¥ 2023.11.10: æ”¯æŒ**bluelm**ç³»åˆ—æ¨¡å‹: bluelm-7b, bluelm-7b-chat, bluelm-7b-32k, bluelm-7b-chat-32k. å¯¹åº”çš„shè„šæœ¬å¯ä»¥æŸ¥çœ‹[bluelm_7b_chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/bluelm_7b_chat).
- ğŸ”¥ 2023.11.08: æ”¯æŒ**xverse-65b**æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æµç¨‹ï¼Œè„šæœ¬åœ¨[xverse_65b](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/xverse_65b).
- ğŸ”¥ 2023.11.07: æ”¯æŒ**yi-6b**, **yi-34b**æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æµç¨‹ï¼Œè„šæœ¬åœ¨[yi_6b](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/yi_6b), [yi_34b](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/yi_34b).
<details><summary>å±•å¼€</summary>
- ğŸ”¥ 2023.10.30: æ”¯æŒ **QA-LoRA** å’Œ **LongLoRA**ä¸¤ç§æ–°çš„tuners.
- ğŸ”¥ 2023.10.30: æ”¯æŒä½¿ç”¨**ROME**(Rank One Model Editing)æ¥ç¼–è¾‘æ¨¡å‹ï¼Œåœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹å³å¯ç»™æ¨¡å‹çŒæ³¨æ–°çŸ¥è¯†ï¼
- 2023.10.30: æ”¯æŒ**skywork-13b**ç³»åˆ—æ¨¡å‹: skywork-13b, skywork-13b-chat. å¯¹åº”çš„shè„šæœ¬å¯ä»¥æŸ¥çœ‹[skywork_13b](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/skywork_13b).
- ğŸ”¥ 2023.10.27: æ”¯æŒ**chatglm3**ç³»åˆ—æ¨¡å‹: chatglm3-6b-base, chatglm3-6b, chatglm3-6b-32k. å¯¹åº”çš„shè„šæœ¬å¯ä»¥æŸ¥çœ‹[chatglm3_6b](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/chatglm3_6b).
- ğŸ”¥ 2023.10.17: æ”¯æŒ**int4**, **int8**æ¨¡å‹çš„SFT: qwen-7b-chat-int4, qwen-14b-chat-int4, qwen-vl-chat-int4, baichuan2-7b-chat-int4, baichuan2-13b-chat-int4, qwen-7b-chat-int8, qwen-14b-chat-int8.
- 2023.10.15: æ”¯æŒ**ziya2-13b**ç³»åˆ—æ¨¡å‹: ziya2-13b, ziya2-13b-chat.
- 2023.10.12: æ”¯æŒ**mistral-7b**ç³»åˆ—æ¨¡å‹: openbuddy-mistral-7b-chat, mistral-7b, mistral-7b-chat.
- ğŸ”¥ 2023.10.7: æ”¯æŒ**DeepSpeed ZeRO-2**, ä½¿å¾—lora(ä¸ä»…ä»…æ˜¯qlora)å¯ä»¥åœ¨åŒå¡A10ä¸Šè¿è¡ŒDDP.
- 2023.10.4: æ”¯æŒæ›´å¤šæ•°å­¦, æ³•å¾‹, SQL, ä»£ç é¢†åŸŸçš„æ•°æ®é›†: blossom-math-zh, school-math-zh, text2sql-en, sql-create-context-en, lawyer-llama-zh, tigerbot-law-zh, leetcode-python-en.
- ğŸ”¥ 2023.9.25: Supported **qwen-14b** model series: qwen-14b, qwen-14b-chat.
- 2023.9.18: Supported **internlm-20b** model series: internlm-20b, internlm-20b-chat.
- 2023.9.12: Supported training with **MP+DDP** to accelerate full-parameter fine-tuning speed.
- 2023.9.5: Supported **openbuddy-llama2-70b-chat** model.
- 2023.9.3: Supported **baichuan2** model series: baichuan2-7b, baichuan2-7b-chat, baichuan2-13b, baichuan2-13b-chat.
</details>


## âœ¨ å¤§æ¨¡å‹è®­ç»ƒæ¨ç†
### ç®€å•ä½¿ç”¨
- ã€å¿…è¯»ã€‘**10åˆ†é’Ÿ**å¯¹å¤§æ¨¡å‹è¿›è¡Œ**è‡ªæˆ‘è®¤çŸ¥å¾®è°ƒ**, åˆ›å»ºä¸“å±äºè‡ªå·±çš„å¤§æ¨¡å‹, å¯ä»¥æŸ¥çœ‹[è‡ªæˆ‘è®¤çŸ¥å¾®è°ƒæœ€ä½³å®è·µ](https://github.com/modelscope/swift/blob/main/docs/source/LLM/è‡ªæˆ‘è®¤çŸ¥å¾®è°ƒæœ€ä½³å®è·µ.md).
- å¿«é€Ÿå¯¹LLMè¿›è¡Œ**æ¨ç†**, æ­å»º**Web-UI**, å¯ä»¥æŸ¥çœ‹[LLMæ¨ç†æ–‡æ¡£](https://github.com/modelscope/swift/blob/main/docs/source/LLM/LLMæ¨ç†æ–‡æ¡£.md).
- å¿«é€Ÿå¯¹LLMè¿›è¡Œ**å¾®è°ƒ**, æ¨ç†å¹¶æ­å»ºWeb-UI. å¯ä»¥æŸ¥çœ‹[LLMå¾®è°ƒæ–‡æ¡£](https://github.com/modelscope/swift/blob/main/docs/source/LLM/LLMå¾®è°ƒæ–‡æ¡£.md).
- æŸ¥çœ‹swiftæ”¯æŒçš„æ¨¡å‹å’Œæ•°æ®é›†. å¯ä»¥æŸ¥çœ‹[æ”¯æŒçš„æ¨¡å‹å’Œæ•°æ®é›†](https://github.com/modelscope/swift/blob/main/docs/source/LLM/æ”¯æŒçš„æ¨¡å‹å’Œæ•°æ®é›†.md).
- å¯¹swiftä¸­çš„æ¨¡å‹, æ•°æ®é›†, å¯¹è¯æ¨¡æ¿è¿›è¡Œ**æ‹“å±•**, å¯ä»¥æŸ¥çœ‹[è‡ªå®šä¹‰ä¸æ‹“å±•](https://github.com/modelscope/swift/blob/main/docs/source/LLM/è‡ªå®šä¹‰ä¸æ‹“å±•.md).
- æŸ¥è¯¢å¾®è°ƒå’Œæ¨ç†çš„å‘½ä»¤è¡Œå‚æ•°, å¯ä»¥æŸ¥çœ‹[å‘½ä»¤è¡Œå‚æ•°](https://github.com/modelscope/swift/blob/main/docs/source/LLM/å‘½ä»¤è¡Œå‚æ•°.md).


### ç‰¹æ€§
- æ”¯æŒçš„SFTæ–¹æ³•: [lora](https://arxiv.org/abs/2106.09685), [qlora](https://arxiv.org/abs/2305.14314), å…¨å‚æ•°å¾®è°ƒ
- æ”¯æŒçš„ç‰¹æ€§: æ¨¡å‹é‡åŒ–, DDP, æ¨¡å‹å¹¶è¡Œ, gradient checkpointing, æ”¯æŒæ¨é€ModelScope Hub, è‡ªå®šä¹‰æ•°æ®é›†, å¤šæ¨¡æ€å’ŒAgent SFT, å¤šè½®å¯¹è¯, ...
- æ”¯æŒçš„æ¨¡å‹
  - å¤šæ¨¡æ€:
    - qwen-vl ç³»åˆ—: [qwen-vl](https://modelscope.cn/models/qwen/Qwen-VL/summary), [qwen-vl-chat](https://modelscope.cn/models/qwen/Qwen-VL-Chat/summary), [qwen-vl-chat-int4](https://modelscope.cn/models/qwen/Qwen-VL-Chat-Int4/summary)
    - qwen-audio ç³»åˆ—: [qwen-audio](https://modelscope.cn/models/qwen/Qwen-Audio/summary), [qwen-audio-chat](https://modelscope.cn/models/qwen/Qwen-Audio-Chat/summary)
  - é€šç”¨:
    - qwen ç³»åˆ—: [qwen-1_8b-chat](https://modelscope.cn/models/qwen/Qwen-1_8B/summary), [qwen-1_8b-chat-int4](https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int4/summary), [qwen-1_8b-chat-int8](https://modelscope.cn/models/qwen/Qwen-1_8B-Chat-Int8/summary), [qwen-7b](https://modelscope.cn/models/qwen/Qwen-7B/summary), [qwen-7b-chat](https://modelscope.cn/models/qwen/Qwen-7B-Chat/summary), [qwen-7b-chat-int4](https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int4/summary), [qwen-7b-chat-int8](https://modelscope.cn/models/qwen/Qwen-7B-Chat-Int8/summary), [qwen-14b](https://modelscope.cn/models/qwen/Qwen-14B/summary), [qwen-14b-chat](https://modelscope.cn/models/qwen/Qwen-14B-Chat/summary), [qwen-14b-chat-int4](https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int4/summary), [qwen-14b-chat-int8](https://modelscope.cn/models/qwen/Qwen-14B-Chat-Int8/summary), [qwen-72b](https://modelscope.cn/models/qwen/Qwen-72B/summary), [qwen-72b-chat](https://modelscope.cn/models/qwen/Qwen-72B-Chat/summary), [qwen-72b-chat-int4](https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int4/summary), [qwen-72b-chat-int8](https://modelscope.cn/models/qwen/Qwen-72B-Chat-Int8/summary)
    - chatglm ç³»åˆ—: [chatglm2-6b](https://modelscope.cn/models/ZhipuAI/chatglm2-6b/summary), [chatglm2-6b-32k](https://modelscope.cn/models/ZhipuAI/chatglm2-6b-32k/summary), [chatglm3-6b-base](https://modelscope.cn/models/ZhipuAI/chatglm3-6b-base/summary), [chatglm3-6b](https://modelscope.cn/models/ZhipuAI/chatglm3-6b/summary), [chatglm3-6b-32k](https://modelscope.cn/models/ZhipuAI/chatglm3-6b-32k/summary)
    - baichuan ç³»åˆ—: [baichuan-7b](https://modelscope.cn/models/baichuan-inc/baichuan-7B/summary), [baichuan-13b](https://modelscope.cn/models/baichuan-inc/Baichuan-13B-Base/summary), [baichuan-13b-chat](https://modelscope.cn/models/baichuan-inc/Baichuan-13B-Chat/summary), [baichuan2-7b](https://modelscope.cn/models/baichuan-inc/Baichuan2-7B-Base/summary), [baichuan2-7b-chat](https://modelscope.cn/models/baichuan-inc/Baichuan2-7B-Chat/summary), [baichuan2-13b](https://modelscope.cn/models/baichuan-inc/Baichuan2-13B-Base/summary), [baichuan2-13b-chat](https://modelscope.cn/models/baichuan-inc/Baichuan2-13B-Chat/summary), [baichuan2-7b-chat-int4](https://modelscope.cn/models/baichuan-inc/Baichuan2-7B-Chat-4bits/summary), [baichuan2-13b-chat-int4](https://modelscope.cn/models/baichuan-inc/Baichuan2-13B-Chat-4bits/summary)
    - llama ç³»åˆ—: [llama2-7b](https://modelscope.cn/models/modelscope/Llama-2-7b-ms/summary), [llama2-7b-chat](https://modelscope.cn/models/modelscope/Llama-2-7b-chat-ms/summary), [llama2-13b](https://modelscope.cn/models/modelscope/Llama-2-13b-ms/summary), [llama2-13b-chat](https://modelscope.cn/models/modelscope/Llama-2-13b-chat-ms/summary), [llama2-70b](https://modelscope.cn/models/modelscope/Llama-2-70b-ms/summary), [llama2-70b-chat](https://modelscope.cn/models/modelscope/Llama-2-70b-chat-ms/summary)
    - openbuddy ç³»åˆ—: [openbuddy-llama2-13b-chat](https://modelscope.cn/models/OpenBuddy/openbuddy-llama2-13b-v8.1-fp16/summary), [openbuddy-llama-65b-chat](https://modelscope.cn/models/OpenBuddy/openbuddy-llama-65b-v8-bf16/summary), [openbuddy-llama2-70b-chat](https://modelscope.cn/models/OpenBuddy/openbuddy-llama2-70b-v10.1-bf16/summary), [openbuddy-mistral-7b-chat](https://modelscope.cn/models/OpenBuddy/openbuddy-mistral-7b-v13.1/summary), [openbuddy-zephyr-7b-chat](https://modelscope.cn/models/OpenBuddy/openbuddy-zephyr-7b-v14.1/summary)
    - internlm ç³»åˆ—: [internlm-7b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-7b/summary), [internlm-7b-chat](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-chat-7b-v1_1/summary), [internlm-7b-chat-8k](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-chat-7b-8k/summary), [internlm-20b](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-20b/summary), [internlm-20b-chat](https://modelscope.cn/models/Shanghai_AI_Laboratory/internlm-chat-20b/summary)
    - xverse ç³»åˆ—: [xverse-7b](https://modelscope.cn/models/xverse/XVERSE-7B/summary), [xverse-7b-chat](https://modelscope.cn/models/xverse/XVERSE-7B-Chat/summary), [xverse-13b](https://modelscope.cn/models/xverse/XVERSE-13B/summary), [xverse-13b-chat](https://modelscope.cn/models/xverse/XVERSE-13B-Chat/summary), [xverse-65b](https://modelscope.cn/models/xverse/XVERSE-65B/summary)
    - bluelm ç³»åˆ—: [bluelm-7b](https://modelscope.cn/models/vivo-ai/BlueLM-7B-Base/summary), [bluelm-7b-chat](https://modelscope.cn/models/vivo-ai/BlueLM-7B-Chat/summary), [bluelm-7b-32k](https://modelscope.cn/models/vivo-ai/BlueLM-7B-Base-32K/summary), [bluelm-7b-chat-32k](https://modelscope.cn/models/vivo-ai/BlueLM-7B-Chat-32K/summary)
    - mistral ç³»åˆ—: [mistral-7b](https://modelscope.cn/models/AI-ModelScope/Mistral-7B-v0.1/summary), [mistral-7b-chat](https://modelscope.cn/models/AI-ModelScope/Mistral-7B-Instruct-v0.1/summary)
    - yi ç³»åˆ—: [yi-6b](https://modelscope.cn/models/01ai/Yi-6B/summary), [yi-34b](https://modelscope.cn/models/01ai/Yi-34B/summary), [yi-34b-chat](https://modelscope.cn/models/01ai/Yi-34B-Chat/summary)
    - zephyr ç³»åˆ—: zephyr-7b-beta-chat(https://modelscope.cn/models/modelscope/zephyr-7b-beta/summary)
    - ziya ç³»åˆ—: [ziya2-13b](https://modelscope.cn/models/Fengshenbang/Ziya2-13B-Base/summary), [ziya2-13b-chat](https://modelscope.cn/models/Fengshenbang/Ziya2-13B-Chat/summary)
    - skywork ç³»åˆ—: [skywork-13b](https://modelscope.cn/models/skywork/Skywork-13B-base/summary), [skywork-13b-chat](https://modelscope.cn/models/skywork/Skywork-13B-chat/summary)
    - other: [polylm-13b](https://modelscope.cn/models/damo/nlp_polylm_13b_text_generation/summary), [seqgpt-560m](https://modelscope.cn/models/damo/nlp_seqgpt-560m/summary)
  - é‡‘è:
    - tongyi-finance ç³»åˆ—: [tongyi-finance-14b](https://modelscope.cn/models/TongyiFinance/Tongyi-Finance-14B/summary), [tongyi-finance-14b-chat](https://modelscope.cn/models/TongyiFinance/Tongyi-Finance-14B-Chat/summary), [tongyi-finance-14b-chat-int4](https://modelscope.cn/models/TongyiFinance/Tongyi-Finance-14B-Chat-Int4/summary)
  - ä»£ç :
    - codefuse series: [codefuse-codellama-34b-chat](https://modelscope.cn/models/codefuse-ai/CodeFuse-CodeLlama-34B/summary)
- æ”¯æŒçš„æ•°æ®é›†:
  - NLP:
    - é€šç”¨: ğŸ”¥[alpaca-en](https://modelscope.cn/datasets/AI-ModelScope/alpaca-gpt4-data-en/summary)(gpt4), ğŸ”¥[alpaca-zh](https://modelscope.cn/datasets/AI-ModelScope/alpaca-gpt4-data-zh/summary)(gpt4), [multi-alpaca-all](https://www.modelscope.cn/datasets/damo/nlp_polylm_multialpaca_sft/summary), [instinwild-en](https://www.modelscope.cn/datasets/wyj123456/instinwild/summary), [instinwild-zh](https://www.modelscope.cn/datasets/wyj123456/instinwild/summary), [cot-en](https://www.modelscope.cn/datasets/YorickHe/CoT/summary), [cot-zh](https://www.modelscope.cn/datasets/YorickHe/CoT/summary), [firefly-all-zh](https://www.modelscope.cn/datasets/wyj123456/firefly/summary), [instruct-en](https://www.modelscope.cn/datasets/wyj123456/instruct/summary), [gpt4all-en](https://www.modelscope.cn/datasets/wyj123456/GPT4all/summary), [sharegpt-en](https://www.modelscope.cn/datasets/huangjintao/sharegpt/summary), [sharegpt-zh](https://www.modelscope.cn/datasets/huangjintao/sharegpt/summary)
    - Agent: [damo-agent-zh](https://modelscope.cn/datasets/damo/MSAgent-Bench/summary), ğŸ”¥[damo-agent-mini-zh](https://modelscope.cn/datasets/damo/MSAgent-Bench/summary), ğŸ”¥[agent-instruct-all-en](https://modelscope.cn/datasets/ZhipuAI/AgentInstruct/summary)
    - ä»£ç : [code-alpaca-en](https://www.modelscope.cn/datasets/wyj123456/code_alpaca_en/summary), ğŸ”¥[leetcode-python-en](https://modelscope.cn/datasets/AI-ModelScope/leetcode-solutions-python/summary), ğŸ”¥[codefuse-python-en](https://modelscope.cn/datasets/codefuse-ai/CodeExercise-Python-27k/summary), ğŸ”¥[codefuse-evol-instruction-zh](https://modelscope.cn/datasets/codefuse-ai/Evol-instruction-66k/summary)
    - åŒ»ç–—: [medical-en](https://www.modelscope.cn/datasets/huangjintao/medical_zh/summary), [medical-zh](https://www.modelscope.cn/datasets/huangjintao/medical_zh/summary), [medical-mini-zh](https://www.modelscope.cn/datasets/huangjintao/medical_zh/summary)
    - æ³•å¾‹: ğŸ”¥[lawyer-llama-zh](https://modelscope.cn/datasets/AI-ModelScope/lawyer_llama_data/summary), [tigerbot-law-zh](https://modelscope.cn/datasets/AI-ModelScope/tigerbot-law-plugin/summary)
    - æ•°å­¦: ğŸ”¥[blossom-math-zh](https://modelscope.cn/datasets/AI-ModelScope/blossom-math-v2/summary), [school-math-zh](https://modelscope.cn/datasets/AI-ModelScope/school_math_0.25M/summary)
    - SQL: [text2sql-en](https://modelscope.cn/datasets/AI-ModelScope/texttosqlv2_25000_v2/summary), ğŸ”¥[sql-create-context-en](https://modelscope.cn/datasets/AI-ModelScope/sql-create-context/summary)
    - æ–‡æœ¬ç”Ÿæˆ: ğŸ”¥[advertise-gen-zh](https://modelscope.cn/datasets/lvjianjin/AdvertiseGen/summary), ğŸ”¥[dureader-robust-zh](https://modelscope.cn/datasets/modelscope/DuReader_robust-QG/summary)
    - åˆ†ç±»: [cmnli-zh](https://www.modelscope.cn/datasets/modelscope/clue/summary), ğŸ”¥[cmnli-mini-zh](https://www.modelscope.cn/datasets/modelscope/clue/summary), ğŸ”¥[jd-sentiment-zh](https://modelscope.cn/datasets/DAMO_NLP/jd/summary), [ğŸ”¥hc3-zh](https://modelscope.cn/datasets/simpleai/HC3-Chinese/summary), [ğŸ”¥hc3-en](https://modelscope.cn/datasets/simpleai/HC3/summary)
    - å…¶ä»–: [finance-en](https://www.modelscope.cn/datasets/wyj123456/finance_en/summary), [poetry-zh](https://www.modelscope.cn/datasets/modelscope/chinese-poetry-collection/summary), [cls-fudan-news-zh](https://modelscope.cn/datasets/damo/zh_cls_fudan-news/summary), [ner-jave-zh](https://modelscope.cn/datasets/damo/zh_ner-JAVE/summary)
  - å¤šæ¨¡æ€:
    - è§†è§‰: [coco-en](https://modelscope.cn/datasets/modelscope/coco_2014_caption/summary), ğŸ”¥[coco-mini-en](https://modelscope.cn/datasets/modelscope/coco_2014_caption/summary)
    - éŸ³é¢‘: [aishell1-zh](https://modelscope.cn/datasets/speech_asr/speech_asr_aishell1_trainsets/summary), ğŸ”¥[aishell1-mini-zh](https://modelscope.cn/datasets/speech_asr/speech_asr_aishell1_trainsets/summary)
  - è‡ªå®šä¹‰æ•°æ®é›†
- æ”¯æŒçš„å¯¹è¯æ¨¡æ¿:
  - æ–‡æœ¬ç”Ÿæˆ: default-generation, default-generation-bos, chatglm-generation
  - å¯¹è¯: default, chatml(qwen), baichuan, chatglm2, chatglm3, llama, openbuddy, internlm, xverse, ziya, skywork, bluelm, yi, zephyr


## ğŸ› ï¸ å®‰è£…

SWIFTåœ¨Pythonç¯å¢ƒä¸­è¿è¡Œã€‚è¯·ç¡®ä¿æ‚¨çš„Pythonç‰ˆæœ¬é«˜äº3.8ã€‚

- æ–¹æ³•1ï¼šä½¿ç”¨pipå‘½ä»¤å®‰è£…SWIFTï¼š

```shell
# å…¨é‡èƒ½åŠ›
pip install ms-swift[all] -U
# ä»…ä½¿ç”¨LLM
pip install ms-swift[llm] -U
# ä»…ä½¿ç”¨AIGC
pip install ms-swift[aigc] -U
# ä»…ä½¿ç”¨adapters
pip install ms-swift -U
```

- æ–¹æ³•2ï¼šé€šè¿‡æºä»£ç å®‰è£…SWIFTï¼ˆæ–¹ä¾¿è¿è¡Œè®­ç»ƒæ¨ç†è„šæœ¬ï¼‰ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```shell
git clone https://github.com/modelscope/swift.git
cd swift
pip install -e .[llm]
```

SWIFTä¾èµ–torch>=1.13ã€‚

- æ–¹æ³•3ï¼šåœ¨æˆ‘ä»¬çš„Dockeré•œåƒä¸­ä½¿ç”¨SWIFT

```shell
docker pull registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.8.0-py38-torch2.0.1-tf2.13.0-1.9.1
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

SWIFTæ”¯æŒå¤šä¸ªtunersï¼ŒåŒ…æ‹¬ç”±[PEFT](https://github.com/huggingface/peft)æä¾›çš„tunersã€‚è¦ä½¿ç”¨è¿™äº›tunersï¼Œåªéœ€è°ƒç”¨:
```python
from swift import Swift, LoRAConfig
config = LoRAConfig(...)
model = Swift.prepare_model(model, config, extra_state_keys=['...'])
```
ä¸Šé¢çš„ä»£ç ç‰‡æ®µéšæœºåˆå§‹åŒ–äº†tunerã€‚è¾“å…¥modelæ˜¯torch.nn.Moduleçš„ä¸€ä¸ªå®ä¾‹ï¼Œconfigæ˜¯SwiftConfigæˆ–PeftConfigçš„å­ç±»å®ä¾‹ã€‚extra_state_keysæ˜¯è¦è®­ç»ƒå¹¶å­˜å‚¨åœ¨è¾“å‡ºç›®å½•ä¸­çš„é¢å¤–æ¨¡å—æƒé‡ï¼ˆå¦‚linear headï¼‰ã€‚

æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ç»„åˆå¤šä¸ªtunersï¼š
```python
from swift import Swift, LoRAConfig, PromptConfig
model = Swift.prepare_model(model, {'lora': LoRAConfig(...), 'prompt': PromptConfig(...)})
```

åœ¨å¾®è°ƒä¹‹åï¼Œæ‚¨å¯ä»¥è°ƒç”¨save_pretrainedå’Œpush_to_hubæ–¹æ³•ï¼š

```python
from swift import push_to_hub
model.save_pretrained('some-output-folder')
push_to_hub('my-group/some-repo-id-modelscope', 'some-output-folder', token='some-ms-token')
```
å‡è®¾`my-group/some-repo-id-modelscope`æ˜¯Hubä¸­çš„model-idï¼Œè€Œ`some-ms-token`æ˜¯ç”¨äºä¸Šä¼ çš„ä»¤ç‰Œã€‚

ä½¿ç”¨model-idè¿›è¡Œåç»­æ¨ç†ï¼š

```python
from swift import Swift
model = Swift.from_pretrained(model, 'my-group/some-repo-id-modelscope')
```

ä¸‹é¢æ˜¯ä¸€ä¸ªå¯è¿è¡Œçš„ç¤ºä¾‹ï¼š

```python
import os
import tempfile

# è¯·é€šè¿‡`pip install modelscope`å®‰è£…modelscope
from modelscope import Model

from swift import LoRAConfig, SwiftModel, Swift, push_to_hub

tmp_dir = tempfile.TemporaryDirectory().name
if not os.path.exists(tmp_dir):
    os.makedirs(tmp_dir)


model = Model.from_pretrained('modelscope/Llama-2-7b-ms', device_map='auto')
lora_config = LoRAConfig(target_modules=['q_proj', 'k_proj', 'v_proj'])
model: SwiftModel = Swift.prepare_model(model, lora_config)
# åœ¨è¿™é‡Œè¿›è¡Œä¸€äº›å¾®è°ƒæ“ä½œ
model.save_pretrained(tmp_dir)

push_to_hub('my-group/swift_llama2', output_dir=tmp_dir)
model = Model.from_pretrained('modelscope/Llama-2-7b-ms', device_map='auto')
model = SwiftModel.from_pretrained(model, 'my-group/swift_llama2', device_map='auto')
```

è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨transformersåº“å®ä¾‹åŒ–æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨SWIFTè¿›è¡Œé«˜æ•ˆå¾®è°ƒçš„ç¤ºä¾‹ã€‚

```python
from swift import Swift, LoRAConfig, AdapterConfig, PromptConfig
from transformers import AutoModelForImageClassification

# åˆå§‹vitæ¨¡å‹
model = AutoModelForImageClassification.from_pretrained("google/vit-base-patch16-224")

# åˆå§‹åŒ–LoRA tuneré…ç½®
lora_config = LoRAConfig(
    r=10,  # LoRAæ¨¡å—çš„rank
    target_modules=['query', 'key', 'value'],  # å°†è¦è¢«æ›¿æ¢çš„æ¨¡å—çš„æ¨¡å—ååç¼€
    merge_weights=False  # æ˜¯å¦åˆå¹¶æƒé‡
)

# åˆå§‹åŒ–adapter tuneré…ç½®
adapter_config = AdapterConfig(
    dim=768,  # hidden statesçš„ç»´åº¦
    hidden_pos=0,  # è¦ä¼ é€’åˆ°adapterçš„hidden stateçš„ä½ç½®
    target_modules=r'.*attention.output.dense$',  # è¦ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢çš„æ¨¡å—
    adapter_length=10  # adapteré•¿åº¦
)

# åˆå§‹åŒ–prompt tuneré…ç½®
prompt_config = PromptConfig(
    dim=768,  # hidden statesçš„ç»´åº¦
    target_modules=r'.*layer\.\d+$',  # è¦ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢çš„æ¨¡å—
    embedding_pos=0,    # embeddingå¼ é‡çš„ä½ç½®
    prompt_length=10,   # æç¤ºç¬¦tokençš„é•¿åº¦
    attach_front=False  # æ˜¯å¦å°†æç¤ºç¬¦é™„åŠ åœ¨embeddingå‰é¢
)

# ä½¿ç”¨swiftåˆ›å»ºæ¨¡å‹ã€‚åœ¨å®è·µä¸­ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å…¶ä¸­ä»»ä½•ä¸€ä¸ªè°ƒè°å™¨æˆ–å®ƒä»¬çš„ç»„åˆã€‚
model = Swift.prepare_model(model, {"lora_tuner": lora_config, "adapter_tuner": adapter_config, "prompt_tuner": prompt_config})

# è·å–æ¨¡å‹çš„å¯è®­ç»ƒå‚æ•°ã€‚
model.get_trainable_parameters()
# 'trainable params: 838,776 || all params: 87,406,432 || trainable%: 0.9596273189597764'
```

å¯ä»¥åœ¨SWIFTä¸­ä½¿ç”¨PEFTæä¾›çš„åŠŸèƒ½ï¼š

```python
from swift import LoraConfig, Swift
from peft import TaskType
lora_config = LoraConfig(target_modules=['query', 'key', 'value'], task_type=TaskType.CAUSAL_LM)
model_wrapped = Swift.prepare_model(model, lora_config)

# æˆ–è€…ä½¿ç”¨from_pretrainedä»modelscope hubä¸­åŠ è½½æƒé‡ã€‚
model_wrapped = Swift.from_pretrained(model, 'some-id-in-the-modelscope-modelhub')
```

Swift tunerså’ŒPeft tunersä¹‹é—´çš„ä¿å­˜ç­–ç•¥ç•¥æœ‰ä¸åŒã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¸ºSwift tunerså‘½åï¼š

```python
model = Swift.prepare_model(model, {'default': LoRAConfig(...)})
model.save_pretrained('./output')
```

åœ¨outputç›®å½•ä¸­å°†ä¼šå¾—åˆ°ä»¥ä¸‹ç±»ä¼¼çš„ç›®å½•ç»“æ„ï¼š

```text
output
    |-- default
        |-- adapter_config.json
        |-- adapter_model.bin
    |-- adapter_config.json
    |-- adapter_model.bin
```

å­˜å‚¨åœ¨outputç›®å½•ä¸­çš„config/weightsæ˜¯extra_state_keysçš„é…ç½®å’Œæƒé‡ã€‚è¿™ä¸Peftä¸åŒï¼ŒPeftå­˜å‚¨äº†`default` tunerçš„config/weightsã€‚

## ğŸ” äº†è§£æ›´å¤š

- [ModelScopeåº“](https://github.com/modelscope/modelscope/)

  ModelScopeåº“æ˜¯ModelScopeé¡¹ç›®çš„æ¨¡å‹åº“ï¼ŒåŒ…å«äº†å„æ¨¡æ€çƒ­é—¨çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚

- [å°†è‡ªå·±çš„æ¨¡å‹è´¡çŒ®ç»™ModelScope](https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)

## License

æœ¬é¡¹ç›®ä½¿ç”¨[Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE)è¿›è¡Œè®¸å¯ã€‚

## â˜ è”ç³»æˆ‘ä»¬

æ‚¨å¯ä»¥é€šè¿‡åŠ æˆ‘ä»¬çš„å¾®ä¿¡ç¾¤, æ¥å’Œæˆ‘ä»¬è”ç³»å’Œäº¤æµ:

<p align="left">
<img src="asset/wechat.png" width="250" style="display: inline-block;">
</p>
