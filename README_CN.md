# SWIFT (Scalable lightWeight Infrastructure for Fine-Tuning)

<p align="center">
    <br>
    <img src="https://modelscope.oss-cn-beijing.aliyuncs.com/modelscope.gif" width="400"/>
    <br>
<p>

<p align="center">
<a href="https://modelscope.cn/home">é­”æ­ç¤¾åŒº</a>
<br>
        ä¸­æ–‡&nbsp ï½œ &nbsp<a href="README.md">English</a>&nbsp ï½œ &nbsp<a href="https://github.com/modelscope/swift/blob/main/docs/source/GetStarted/%E5%BF%AB%E9%80%9F%E4%BD%BF%E7%94%A8.md">æ–‡æ¡£</a>
</p>

<p align="center">
<img src="https://img.shields.io/badge/python-%E2%89%A53.8-5be.svg">
<img src="https://img.shields.io/badge/pytorch-%E2%89%A51.12%20%7C%20%E2%89%A52.0-orange.svg">
<a href="https://github.com/modelscope/modelscope/"><img src="https://img.shields.io/badge/modelscope-%E2%89%A51.9.5-5D91D4.svg"></a>
<a href="https://pypi.org/project/ms-swift/"><img src="https://badge.fury.io/py/ms-swift.svg"></a>
<a href="https://github.com/modelscope/swift/blob/main/LICENSE"><img src="https://img.shields.io/github/license/modelscope/swift"></a>
<a href="https://pepy.tech/project/ms-swift"><img src="https://pepy.tech/badge/ms-swift"></a>
<a href="https://github.com/modelscope/swift/pulls"><img src="https://img.shields.io/badge/PR-welcome-55EB99.svg"></a>
</p>

##  ğŸ“– ç›®å½•
- [ç®€ä»‹](#-ç®€ä»‹)
- [æ–°é—»](#-æ–°é—»)
- ğŸ”¥[å¤§æ¨¡å‹è®­ç»ƒæ¨ç†](#-å¤§æ¨¡å‹è®­ç»ƒæ¨ç†)
- ğŸ”¥[SCEdit](#-SCEdit)
- [å®‰è£…](#-å®‰è£…)
- [å¿«é€Ÿå¼€å§‹](#-å¿«é€Ÿå¼€å§‹)
- [äº†è§£æ›´å¤š](#-äº†è§£æ›´å¤š)
- [License](#license)
- [è”ç³»æˆ‘ä»¬](#-è”ç³»æˆ‘ä»¬)

## ğŸ“ ç®€ä»‹
SWIFTï¼ˆScalable lightWeight Infrastructure for Fine-Tuningï¼‰æ˜¯ä¸€ä¸ªå¯æ‰©å±•çš„è½»é‡çº§ä¸€ç«™å¼è®­ç»ƒã€æ¨ç†æ·±åº¦å­¦ä¹ æ¡†æ¶ã€‚å®ƒé›†æˆäº†å„ç§é«˜æ•ˆçš„å¾®è°ƒæ–¹æ³•ï¼Œå¦‚LoRAã€QLoRAã€é˜¿é‡Œäº‘è‡ªç ”çš„ResTuning-Bypassç­‰ï¼Œä»¥åŠå¼€ç®±å³ç”¨çš„è®­ç»ƒæ¨ç†è„šæœ¬ï¼Œä½¿å¼€å‘è€…å¯ä»¥åœ¨å•å¼ å•†ä¸šçº§æ˜¾å¡ä¸Šå¾®è°ƒæ¨ç†LLM&AIGCæ¨¡å‹ã€‚æ­¤å¤–ï¼ŒSWIFTä¸[PEFT](https://github.com/huggingface/peft)å®Œå…¨å…¼å®¹ï¼Œä½¿å¼€å‘è€…å¯ä»¥åœ¨ModelScopeæ¨¡å‹ä½“ç³»ä¸­ä½¿ç”¨PEFTçš„èƒ½åŠ›ã€‚

ç›®å‰æ”¯æŒçš„æ–¹æ³•ï¼š

1. ğŸ”¥LoRA: [LORA: LOW-RANK ADAPTATION OF LARGE LANGUAGE MODELS](https://arxiv.org/abs/2106.09685)
2. ğŸ”¥LoRA+: [LoRA+: Efficient Low Rank Adaptation of Large Models](https://arxiv.org/pdf/2402.12354.pdf)
3. ğŸ”¥LLaMA PRO: [LLAMA PRO: Progressive LLaMA with Block Expansion](https://arxiv.org/pdf/2401.02415.pdf)
4. ğŸ”¥SCEdit: [SCEdit: Efficient and Controllable Image Diffusion Generation via Skip Connection Editing](https://arxiv.org/abs/2312.11392)  < [arXiv](https://arxiv.org/abs/2312.11392)  |  [Project Page](https://scedit.github.io/) >
5. ğŸ”¥NEFTune: [Noisy Embeddings Improve Instruction Finetuning](https://arxiv.org/abs/2310.05914)
6. QA-LoRA:[Quantization-Aware Low-Rank Adaptation of Large Language Models](https://arxiv.org/abs/2309.14717).
7. LongLoRA: [Efficient Fine-tuning of Long-Context Large Language Models](https://arxiv.org/abs/2309.12307)
8. ROME: [Rank-One Editing of Encoder-Decoder Models](https://arxiv.org/abs/2211.13317)
9. Adapter: [Parameter-Efficient Transfer Learning for NLP](http://arxiv.org/abs/1902.00751)
10. Prompt Tuning: [Visual Prompt Tuning](https://arxiv.org/abs/2203.12119)
11. Side: [Side-Tuning: A Baseline for Network Adaptation via Additive Side Networks](https://arxiv.org/abs/1912.13503)
12. Res-Tuning: [Res-Tuning: A Flexible and Efficient Tuning Paradigm via Unbinding Tuner from Backbone](https://arxiv.org/abs/2310.19859)  < [arXiv](https://arxiv.org/abs/2310.19859)  |  [Project Page](https://res-tuning.github.io/)  |  [Usage](docs/source/GetStarted/ResTuning.md) >
13. [PEFT](https://github.com/huggingface/peft)æä¾›çš„tuners, å¦‚IA3, AdaLoRAç­‰

ä¸»è¦èƒ½åŠ›ï¼š
1. å¯ä»¥é€šè¿‡model-idä½¿SWIFTæˆ–PEFTçš„æ–¹æ³•ä½¿ç”¨ModelScope Hubä¸­çš„æ¨¡å‹
2. åœ¨å•æ¬¡è®­ç»ƒæˆ–æ¨ç†ä¸­å¯ä»¥ä½¿ç”¨å¤šä¸ªtuners
3. æ”¯æŒè°ƒç”¨`activate_adapter`æˆ–`deactivate_adapter`æˆ–`set_active_adapters`æ¥ä½¿éƒ¨åˆ†tuneræ¿€æ´»æˆ–å¤±æ´»ï¼Œç”¨æˆ·å¯ä»¥åœ¨æ¨ç†æ—¶åŒæ—¶åŠ è½½å¤šä¸ªç‹¬ç«‹çš„tunersåœ¨ä¸åŒçº¿ç¨‹ä¸­å¹¶è¡Œä½¿ç”¨ã€‚
4. æ”¯æŒé€šè¿‡è„šæœ¬æ–¹å¼å’Œå‘½ä»¤è¡Œæ–¹å¼å¼€å¯è®­ç»ƒå’Œæ¨ç†ï¼ŒåŒæ—¶æ”¯æŒWeb-UIæ–¹å¼è¿›è¡Œæ¨ç†ã€‚
5. æ”¯æŒæ¨¡å‹è®­ç»ƒåçš„éƒ¨ç½²é“¾è·¯(vllm/chatglm.cpp/xinference)ï¼Œè¯¦æƒ…å¯ä»¥æŸ¥çœ‹[å®˜æ–¹æ–‡æ¡£](./docs/source/GetStarted/éƒ¨ç½²æŒ‡å—.md)ã€‚

ç”¨æˆ·å¯ä»¥æŸ¥çœ‹ [SWIFTå®˜æ–¹æ–‡æ¡£](docs/source/GetStarted/å¿«é€Ÿä½¿ç”¨.md) æ¥äº†è§£è¯¦ç»†ä¿¡æ¯ã€‚

## ğŸ‰ æ–°é—»
- ğŸ”¥2024.02.29: æ”¯æŒ[LLaMA PRO](https://arxiv.org/pdf/2401.02415.pdf), ä½¿ç”¨[è¿™ä¸ªè„šæœ¬](https://github.com/modelscope/swift/blob/main/examples/pytorch/llm/scripts/yi_6b_chat/llamapro/sft.sh)å³å¯å¼€å§‹è®­ç»ƒ.
- ğŸ”¥2024.02.29: æ”¯æŒ[LoRA+](https://arxiv.org/pdf/2402.12354.pdf), ä½¿ç”¨[è¿™ä¸ªè„šæœ¬](https://github.com/modelscope/swift/blob/main/examples/pytorch/llm/scripts/yi_6b_chat/lorap/sft.sh)å³å¯å¼€å§‹è®­ç»ƒ.
- 2024.02.25: æ”¯æŒ`swift export`, å¯¹æ¨¡å‹è¿›è¡ŒAWQé‡åŒ–å¯¼å‡º, ä»¥åŠæ¨é€ModelScope Hub. å…·ä½“å¯ä»¥æŸ¥çœ‹æ–‡æ¡£: [LLMé‡åŒ–æ–‡æ¡£](https://github.com/modelscope/swift/blob/main/docs/source/LLM/LLM%E9%87%8F%E5%8C%96%E6%96%87%E6%A1%A3.md).
- 2024.02.22: æ”¯æŒgemmaç³»åˆ—: gemma-2b, [gemma-2b-instruct](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/gemma_2b_instruct), gemma-7b, gemma-7b-instruct.
- 2024.02.16: æ”¯æŒdeepseek-mathç³»åˆ—: deepseek-math-7b, deepseek-math-7b-instruct, deepseek-math-7b-chat.
- ğŸ”¥2024.02.05: æ”¯æŒ**Qwen1.5**ç³»åˆ—æ¨¡å‹, æ”¯æŒçš„æ‰€æœ‰Qwen1.5ç³»åˆ—æ¨¡å‹è¯·æŸ¥çœ‹[æ¨¡å‹åˆ—è¡¨](https://github.com/modelscope/swift/blob/main/docs/source/LLM/%E6%94%AF%E6%8C%81%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86.md#%E6%A8%A1%E5%9E%8B). æä¾›äº†[qwen1half-7b-chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen1half_7b_chat), [qwen1half-7b-chat-int8](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen1half_7b_chat_int8)å¾®è°ƒçš„è„šæœ¬.
- 2024.02.05: æ”¯æŒæ‰©æ•£æ¨¡å‹å¦‚**SDXL**, **SD**, **ControlNet**çš„è®­ç»ƒ, åŒæ—¶ä¹Ÿæ”¯æŒ**DreamBooth**çš„è®­ç»ƒ, è¯¦æƒ…å¯ä»¥æŸ¥çœ‹å¯¹åº”çš„[è®­ç»ƒè„šæœ¬](https://github.com/modelscope/swift/tree/main/examples/pytorch/sdxl/scripts).
- 2024.02.01: æ”¯æŒopenbmb-minicpmç³»åˆ—: [openbmb-minicpm-2b-sft-chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/openbmb_minicpm_2b_sft_chat), openbmb-minicpm-2b-chat.
- ğŸ”¥2024.02.01: æ”¯æŒæ•°æ®é›†æ‰“æ··æ¥å‡å°‘ **ç¾éš¾æ€§é—å¿˜é—®é¢˜**. ä½¿ç”¨`--train_dataset_mix_ratio 2.0`å¼€å¯è®­ç»ƒï¼åŒæ—¶æˆ‘ä»¬ä¹Ÿå¼€æºäº†é€šç”¨çŸ¥è¯†æ•°æ®é›† [ms-bench](https://www.modelscope.cn/datasets/iic/ms_bench/summary).
- ğŸ”¥2024.02.01: æ”¯æŒAgentè®­ç»ƒï¼Agentè®­ç»ƒç®—æ³•æºè‡ªè¿™ç¯‡[è®ºæ–‡](https://arxiv.org/pdf/2309.00986.pdf). æˆ‘ä»¬ä¹Ÿå¢åŠ äº†[ms-agent](https://www.modelscope.cn/datasets/iic/ms_agent/summary)è¿™ä¸ªä¼˜è´¨çš„agentæ•°æ®é›†. ä½¿ç”¨[è¿™ä¸ªè„šæœ¬](https://github.com/modelscope/swift/blob/main/examples/pytorch/llm/scripts/qwen_7b_chat/lora/sft.sh)å¼€å¯Agentè®­ç»ƒ!
- ğŸ”¥2024.02.01: æ”¯æŒåœ¨DPOè®­ç»ƒä¸­å¢åŠ SFT lossæ¥å‡å°‘KLæ•£åº¦lossé€ æˆçš„ç”Ÿæˆé‡å¤é—®é¢˜.
- 2024.02.01: æ”¯æŒåœ¨è®­ç»ƒä¸­ä½¿ç”¨AdaLoRAå’ŒIA3ä¸¤ä¸ªadapter.
- 2024.02.01: æ”¯æŒåœ¨AnimateDiffè®­ç»ƒä¸­ä½¿ç”¨`--merge_lora`å‚æ•°.
<details><summary>æ›´å¤š</summary>

- 2024.01.30: æ”¯æŒ[internlm-xcomposer2-7b-chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/internlm_xcomposer2_7b_chat).
- ğŸ”¥2024.01.30: æ”¯æŒ[ZeRO-3](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_14b_chat/full_ddp_zero3/), åªéœ€è¦æŒ‡å®š`--deepspeed default-zero3`å³å¯.
- 2024.01.29: æ”¯æŒinternlm2-mathç³»åˆ—: internlm2-math-7b, internlm2-math-7b-chat, internlm2-math-20b, internlm2-math-20b-chat.
- ğŸ”¥2024.01.26: æ”¯æŒ[yi-vl-6b-chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/yi_vl_6b_chat), yi-vl-34b-chat.
- 2024.01.24: æ”¯æŒcodefuse-codegeex2-6b-chat, codefuse-qwen-14b-chat.
- 2024.01.23: æ”¯æŒorionç³»åˆ—: orion-14b, [orion-14b-chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/orion_14b_chat).
- 2024.01.20: æ”¯æŒ[xverse-13b-256k](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/xverse_13b_256k), xverse-65b-v2, xverse-65b-chat.
- ğŸ”¥2024.01.17: æ”¯æŒinternlm2ç³»åˆ—: internlm2-7b-base, internlm2-7b, [internlm2-7b-sft-chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/internlm2_7b_sft_chat), internlm2-7b-chat, internlm2-20b-base, internlm2-20b, internlm2-20b-sft-chat, internlm2-20b-chat.
- 2024.01.15: æ”¯æŒyuanç³»åˆ—: yuan2-2b-instruct, [yuan2-2b-janus-instruct](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/yuan2_2b_janus_instruct), yuan2-51b-instruct, yuan2-102b-instruct.
- ğŸ”¥2024.01.12: æ”¯æŒ**deepseek-moe**ç³»åˆ—: deepseek-moe-16b, [deepseek-moe-16b-chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/deepseek_moe_16b_chat).
- ğŸ”¥2024.01.04: æ”¯æŒ**VLLMéƒ¨ç½²**, å…¼å®¹**OpenAI API**æ ·å¼, å…·ä½“å¯ä»¥æŸ¥çœ‹[VLLMæ¨ç†åŠ é€Ÿä¸éƒ¨ç½²](https://github.com/modelscope/swift/blob/main/docs/source/LLM/VLLMæ¨ç†åŠ é€Ÿä¸éƒ¨ç½².md#éƒ¨ç½²).
- 2024.01.04: æ›´æ–°[Benchmark](https://github.com/modelscope/swift/blob/main/docs/source/LLM/Benchmark.md), æ–¹ä¾¿æŸ¥çœ‹ä¸åŒæ¨¡å‹è®­ç»ƒçš„é€Ÿåº¦å’Œæ‰€éœ€æ˜¾å­˜.
- ğŸ”¥ 2023.12.29: æ”¯æŒweb-uiè¿›è¡Œsftè®­ç»ƒå’Œæ¨ç†ï¼Œå®‰è£…ms-swiftåä½¿ç”¨`swift web-ui`å¼€å¯
- ğŸ”¥ 2023.12.29: æ”¯æŒ DPO RLHF(Reinforcement Learning from Human Feedback) å’Œä¸‰ä¸ªç”¨äºæ­¤ä»»åŠ¡çš„æ•°æ®é›†: AI-ModelScope/stack-exchange-paired ä»¥åŠ AI-ModelScope/hh-rlhf ä»¥åŠ AI-ModelScope/hh_rlhf_cn. æŸ¥çœ‹[æ–‡æ¡£](https://github.com/modelscope/swift/blob/main/docs/source/LLM/LLM%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E8%AE%AD%E7%BB%83%E6%96%87%E6%A1%A3.md)å¼€å¯è®­ç»ƒï¼
- ğŸ”¥ 2023.12.28: æ”¯æŒSCEdit! è¯¥tunerå¯æ˜¾è‘—é™ä½U-Netä¸­çš„æ˜¾å­˜å ç”¨ï¼Œå¹¶æ”¯æŒä½æ˜¾å­˜å¯æ§å›¾åƒç”Ÿæˆï¼ˆå–ä»£ControlNetï¼‰ï¼Œé˜…è¯»ä¸‹é¢çš„ç« èŠ‚æ¥äº†è§£è¯¦ç»†ä¿¡æ¯
- 2023.12.23: æ”¯æŒ[codegeex2-6b](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/codegeex2_6b).
- 2023.12.19: æ”¯æŒ[phi2-3b](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/phi2_3b).
- 2023.12.18: æ”¯æŒVLLMè¿›è¡Œæ¨ç†åŠ é€Ÿ.
- 2023.12.15: æ”¯æŒdeepseek, deepseek-coderç³»åˆ—: deepseek-7b, deepseek-7b-chat, deepseek-67b, deepseek-67b-chat, openbuddy-deepseek-67b-chat, deepseek-coder-1_3b, deepseek-coder-1_3b-instruct, deepseek-coder-6_7b, deepseek-coder-6_7b-instruct, deepseek-coder-33b, deepseek-coder-33b-instruct.
- 2023.12.13: æ”¯æŒmistral-7b-instruct-v2, [mixtral-moe-7b](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/mixtral_7b_moe), [mixtral-moe-7b-instruct](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/mixtral_7b_moe_instruct).
- 2023.12.09: æ”¯æŒ`freeze_parameters`å‚æ•°, ä½œä¸ºloraå’Œå…¨å‚æ•°è®­ç»ƒçš„æŠ˜ä¸­æ–¹æ¡ˆ. å¯¹åº”çš„shå¯ä»¥æŸ¥çœ‹[full_freeze_ddp](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_7b_chat/full_freeze_ddp). æ”¯æŒ`disable_tqdm`, `lazy_tokenize`, `preprocess_num_proc`å‚æ•°, å…·ä½“å¯ä»¥æŸ¥çœ‹[å‘½ä»¤è¡Œå‚æ•°](https://github.com/modelscope/swift/blob/main/docs/source/LLM/å‘½ä»¤è¡Œå‚æ•°.md).
- 2023.12.08: æ”¯æŒ[sus-34b-chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/sus_34b_chat), æ”¯æŒyi-6b-200k, yi-34b-200k.
- 2023.12.07: æ”¯æŒ[Multi-Node DDPè®­ç»ƒ](https://github.com/modelscope/swift/blob/main/docs/source/LLM/LLM%E5%BE%AE%E8%B0%83%E6%96%87%E6%A1%A3.md#%E4%BD%BF%E7%94%A8cli).
- 2023.12.05: æ”¯æŒæ¨¡å‹: zephyr-7b-beta-chat, openbuddy-zephyr-7b-chat. æ”¯æŒæ•°æ®é›†: hc3-zh, hc3-en.
- ğŸ”¥ 2023.12.02: [è‡ªæˆ‘è®¤çŸ¥å¾®è°ƒæœ€ä½³å®è·µ](https://github.com/modelscope/swift/blob/main/docs/source/LLM/è‡ªæˆ‘è®¤çŸ¥å¾®è°ƒæœ€ä½³å®è·µ.md), **10åˆ†é’Ÿå¯¹å¤§æ¨¡å‹è¿›è¡Œè‡ªæˆ‘è®¤çŸ¥å¾®è°ƒ**, åˆ›å»ºä¸“å±äºè‡ªå·±çš„å¤§æ¨¡å‹.
- ğŸ”¥ 2023.11.30: æ”¯æŒ**qwen-1_8b**, **qwen-72b**, **qwen-audio**ç³»åˆ—æ¨¡å‹çš„è®­ç»ƒçš„æ¨ç†. å¯¹åº”çš„shè„šæœ¬å¯ä»¥æŸ¥çœ‹[qwen_1_8b_chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_1_8b_chat), [qwen_72b_chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_72b_chat), [qwen_audio_chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_audio_chat)
- ğŸ”¥ 2023.11.29: æ”¯æŒ**AnimateDiff**çš„è®­ç»ƒå’Œæ¨ç†
- ğŸ”¥ 2023.11.24: æ”¯æŒ**yi-34b-chat**, **codefuse-codellama-34b-chat**æ¨¡å‹. å¯¹åº”çš„shè„šæœ¬å¯ä»¥æŸ¥çœ‹[yi_34b_chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/yi_34b_chat), [codefuse_codellama_34b_chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/codefuse_codellama_34b_chat).
- ğŸ”¥ 2023.11.18: æ”¯æŒ**tongyi-finance-14b**ç³»åˆ—æ¨¡å‹: tongyi-finance-14b, tongyi-finance-14b-chat, tongyi-finance-14b-chat-int4. å¯¹åº”çš„shè„šæœ¬å¯ä»¥æŸ¥çœ‹[tongyi_finance_14b_chat_int4](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/tongyi_finance_14b_chat_int4).
- 2023.11.16: æ”¯æŒæ›´å¤šæ¨¡å‹çš„**flash attn**æ”¯æŒ: qwenç³»åˆ—, qwen-vlç³»åˆ—, llamaç³»åˆ—, openbuddyç³»åˆ—, mistralç³»åˆ—, yiç³»åˆ—, ziyaç³»åˆ—. è¯·ä½¿ç”¨`use_flash_attn`å‚æ•°.
- ğŸ”¥ 2023.11.11: æ”¯æŒ**NEFTune**, ä½¿ç”¨`Swift.prepare_model(model, NEFTuneConfig())`å³å¯å¼€å¯.
- ğŸ”¥ 2023.11.11: æ”¯æŒ**å‘½ä»¤è¡Œ**è®­ç»ƒæ¨ç†å’Œ**Web-UI**æ¨ç†, è¯¦æƒ…å¯ä»¥æŸ¥çœ‹ä¸‹æ–¹çš„`ä½¿ç”¨Swift CLIè¿è¡Œ`ç« èŠ‚.
- ğŸ”¥ 2023.11.11: æ”¯æŒæ¨¡å‹è®­ç»ƒåçš„**éƒ¨ç½²**é“¾è·¯(vllm/chatglm.cpp/xinference)ï¼Œè¯¦æƒ…å¯ä»¥æŸ¥çœ‹[å®˜æ–¹æ–‡æ¡£](./docs/source/GetStarted/éƒ¨ç½²æŒ‡å—.md).
- ğŸ”¥ 2023.11.10: æ”¯æŒ**bluelm**ç³»åˆ—æ¨¡å‹: bluelm-7b, bluelm-7b-chat, bluelm-7b-32k, bluelm-7b-chat-32k. å¯¹åº”çš„shè„šæœ¬å¯ä»¥æŸ¥çœ‹[bluelm_7b_chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/bluelm_7b_chat).
- ğŸ”¥ 2023.11.08: æ”¯æŒ**xverse-65b**æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æµç¨‹ï¼Œè„šæœ¬åœ¨[xverse_65b](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/xverse_65b).
- ğŸ”¥ 2023.11.07: æ”¯æŒ**yi-6b**, **yi-34b**æ¨¡å‹çš„è®­ç»ƒå’Œæ¨ç†æµç¨‹ï¼Œè„šæœ¬åœ¨[yi_6b](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/yi_6b), [yi_34b](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/yi_34b).
- ğŸ”¥ 2023.10.30: æ”¯æŒ **QA-LoRA** å’Œ **LongLoRA**ä¸¤ç§æ–°çš„tuners.
- ğŸ”¥ 2023.10.30: æ”¯æŒä½¿ç”¨**ROME**(Rank One Model Editing)æ¥ç¼–è¾‘æ¨¡å‹ï¼Œåœ¨æ— éœ€è®­ç»ƒçš„æƒ…å†µä¸‹å³å¯ç»™æ¨¡å‹çŒæ³¨æ–°çŸ¥è¯†ï¼
- 2023.10.30: æ”¯æŒ**skywork-13b**ç³»åˆ—æ¨¡å‹: skywork-13b, skywork-13b-chat. å¯¹åº”çš„shè„šæœ¬å¯ä»¥æŸ¥çœ‹[skywork_13b](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/skywork_13b).
- ğŸ”¥ 2023.10.27: æ”¯æŒ**chatglm3**ç³»åˆ—æ¨¡å‹: chatglm3-6b-base, chatglm3-6b, chatglm3-6b-32k. å¯¹åº”çš„shè„šæœ¬å¯ä»¥æŸ¥çœ‹[chatglm3_6b](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/chatglm3_6b).
- ğŸ”¥ 2023.10.17: æ”¯æŒ**int4**, **int8**æ¨¡å‹çš„SFT: qwen-7b-chat-int4, qwen-14b-chat-int4, qwen-vl-chat-int4, baichuan2-7b-chat-int4, baichuan2-13b-chat-int4, qwen-7b-chat-int8, qwen-14b-chat-int8.
- 2023.10.15: æ”¯æŒ**ziya2-13b**ç³»åˆ—æ¨¡å‹: ziya2-13b, ziya2-13b-chat.
- 2023.10.12: æ”¯æŒ**mistral-7b**ç³»åˆ—æ¨¡å‹: openbuddy-mistral-7b-chat, mistral-7b, mistral-7b-instruct.
- ğŸ”¥ 2023.10.07: æ”¯æŒ**DeepSpeed ZeRO-2**, ä½¿å¾—lora(ä¸ä»…ä»…æ˜¯qlora)å¯ä»¥åœ¨åŒå¡A10ä¸Šè¿è¡ŒDDP.
- 2023.10.04: æ”¯æŒæ›´å¤šæ•°å­¦, æ³•å¾‹, SQL, ä»£ç é¢†åŸŸçš„æ•°æ®é›†: blossom-math-zh, school-math-zh, text2sql-en, sql-create-context-en, lawyer-llama-zh, tigerbot-law-zh, leetcode-python-en.
- ğŸ”¥ 2023.09.25: æ”¯æŒ**qwen-14b**ç³»åˆ—: qwen-14b, qwen-14b-chat.
- 2023.09.18: æ”¯æŒ**internlm-20b**ç³»åˆ—: internlm-20b, internlm-20b-chat.
- 2023.09.12: æ”¯æŒ**MP+DDP**å¯¹å…¨å‚æ•°è®­ç»ƒè¿›è¡ŒåŠ é€Ÿ.
- 2023.09.05: æ”¯æŒ**openbuddy-llama2-70b-chat**.
- 2023.09.03: æ”¯æŒ**baichuan2**ç³»åˆ—: baichuan2-7b, baichuan2-7b-chat, baichuan2-13b, baichuan2-13b-chat.
</details>


## âœ¨ å¤§æ¨¡å‹è®­ç»ƒæ¨ç†
### WEB UIè®­ç»ƒæ¨ç†

å®‰è£…SWIFTä¹‹åï¼Œç”¨å¦‚ä¸‹æ–¹å¼å¯åŠ¨ç•Œé¢è®­ç»ƒæ¨ç†ï¼š

```shell
swift web-ui
```

> æ”¯æŒçš„ç¯å¢ƒå˜é‡ï¼š
>
> WEBUI_SHARE=1 æ§åˆ¶gradioæ˜¯å¦æ˜¯shareçŠ¶æ€
> SWIFT_UI_LANG=en/zh æ§åˆ¶web-uiç•Œé¢è¯­è¨€
> WEBUI_SERVER server_nameå‚æ•°ï¼Œ web-ui host ipï¼Œ0.0.0.0ä»£è¡¨æ‰€æœ‰ipå‡å¯è®¿é—®ï¼Œ127.0.0.1ä»£è¡¨åªå…è®¸æœ¬æœºè®¿é—®
> WEBUI_PORT web-uiçš„ç«¯å£å·

ä¸‹é¢æ˜¯ä¸€ä¸ªweb-uiçš„ç®€å•è§†é¢‘ä»‹ç»ï¼š



[![Watch the video](docs/source/cources/resources/20240119160942.jpg)](https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/SWIFT%E8%A7%86%E9%A2%91_%E8%B0%83%E6%95%B4%E5%B0%81%E9%9D%A2.mp4)

### ç®€å•ä½¿ç”¨

- **10åˆ†é’Ÿ**å¯¹å¤§æ¨¡å‹è¿›è¡Œ**è‡ªæˆ‘è®¤çŸ¥å¾®è°ƒ**, åˆ›å»ºä¸“å±äºè‡ªå·±çš„å¤§æ¨¡å‹, å¯ä»¥æŸ¥çœ‹[è‡ªæˆ‘è®¤çŸ¥å¾®è°ƒæœ€ä½³å®è·µ](https://github.com/modelscope/swift/blob/main/docs/source/LLM/è‡ªæˆ‘è®¤çŸ¥å¾®è°ƒæœ€ä½³å®è·µ.md).
- å¿«é€Ÿå¯¹LLMè¿›è¡Œ**æ¨ç†**, æ­å»º**Web-UI**, å¯ä»¥æŸ¥çœ‹[LLMæ¨ç†æ–‡æ¡£](https://github.com/modelscope/swift/blob/main/docs/source/LLM/LLMæ¨ç†æ–‡æ¡£.md).
- å¿«é€Ÿå¯¹LLMè¿›è¡Œ**å¾®è°ƒ**, æ¨ç†å¹¶æ­å»ºWeb-UI, å¯ä»¥æŸ¥çœ‹[LLMå¾®è°ƒæ–‡æ¡£](https://github.com/modelscope/swift/blob/main/docs/source/LLM/LLMå¾®è°ƒæ–‡æ¡£.md).
- ä½¿ç”¨**ç•Œé¢**æ–¹å¼è¿›è¡Œå¾®è°ƒå’Œæ¨ç†, å¯ä»¥æŸ¥çœ‹[WEB-UIæ–‡æ¡£](https://github.com/modelscope/swift/blob/main/docs/source/GetStarted/%E7%95%8C%E9%9D%A2%E8%AE%AD%E7%BB%83%E6%8E%A8%E7%90%86.md).
- æ”¯æŒ**DPOè®­ç»ƒ**, å¯ä»¥æŸ¥çœ‹[DPOæ–‡æ¡£](https://github.com/modelscope/swift/blob/main/docs/source/LLM/LLM%E4%BA%BA%E7%B1%BB%E5%AF%B9%E9%BD%90%E8%AE%AD%E7%BB%83%E6%96%87%E6%A1%A3.md).
- å¯¹å¾®è°ƒçš„æ¨¡å‹è¿›è¡Œå¯¼å‡º, åŒ…æ‹¬: merge-lora, AWQé‡åŒ–, æ¨é€ModelScope Hub, å¯ä»¥æŸ¥çœ‹[LLMé‡åŒ–æ–‡æ¡£](https://github.com/modelscope/swift/blob/main/docs/source/LLM/LLM%E9%87%8F%E5%8C%96%E6%A8%A1%E5%9E%8B.md).
- ä½¿ç”¨VLLMè¿›è¡Œ**æ¨ç†åŠ é€Ÿ**å’Œ**éƒ¨ç½²(OpenAI API)**. å¯ä»¥æŸ¥çœ‹[VLLMæ¨ç†åŠ é€Ÿä¸éƒ¨ç½²](https://github.com/modelscope/swift/blob/main/docs/source/LLM/VLLMæ¨ç†åŠ é€Ÿä¸éƒ¨ç½².md).
- æŸ¥çœ‹swiftæ”¯æŒçš„æ¨¡å‹å’Œæ•°æ®é›†. å¯ä»¥æŸ¥çœ‹[æ”¯æŒçš„æ¨¡å‹å’Œæ•°æ®é›†](https://github.com/modelscope/swift/blob/main/docs/source/LLM/æ”¯æŒçš„æ¨¡å‹å’Œæ•°æ®é›†.md).
- å¯¹swiftä¸­çš„æ¨¡å‹, æ•°æ®é›†, å¯¹è¯æ¨¡æ¿è¿›è¡Œ**æ‹“å±•**, å¯ä»¥æŸ¥çœ‹[è‡ªå®šä¹‰ä¸æ‹“å±•](https://github.com/modelscope/swift/blob/main/docs/source/LLM/è‡ªå®šä¹‰ä¸æ‹“å±•.md).
- æŸ¥è¯¢å¾®è°ƒå’Œæ¨ç†çš„å‘½ä»¤è¡Œå‚æ•°, å¯ä»¥æŸ¥çœ‹[å‘½ä»¤è¡Œå‚æ•°](https://github.com/modelscope/swift/blob/main/docs/source/LLM/å‘½ä»¤è¡Œå‚æ•°.md).
- æŸ¥çœ‹ä¸åŒå‚æ•°ä¸‹çš„è®­ç»ƒæ—¶é—´å’Œè®­ç»ƒæ˜¾å­˜å¯¹æ¯”, å¯ä»¥æŸ¥çœ‹[Benchmark](https://github.com/modelscope/swift/blob/main/docs/source/LLM/Benchmark.md).


### å¿«é€Ÿå¼€å§‹
ä½ å¯ä»¥é€šè¿‡è¿è¡Œä»¥ä¸‹ä»£ç æ¥æµ‹è¯•ç¯å¢ƒæ˜¯å¦å®‰è£…æ­£ç¡®.
```python
# pip install ms-swift[llm] -U

# Experimental environment: A10, 3090, V100, ...
# 8GB GPU memory
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

import torch

from swift.llm import (
    DatasetName, InferArguments, ModelType, SftArguments,
    infer_main, sft_main, app_ui_main, merge_lora
)

model_type = ModelType.qwen1half_0_5b
sft_args = SftArguments(
    model_type=model_type,
    train_dataset_sample=2000,
    dataset=[DatasetName.jd_sentiment_zh],
    output_dir='output')
result = sft_main(sft_args)
best_model_checkpoint = result['best_model_checkpoint']
print(f'best_model_checkpoint: {best_model_checkpoint}')
torch.cuda.empty_cache()

infer_args = InferArguments(
    ckpt_dir=best_model_checkpoint,
    load_dataset_config=True,
    val_dataset_sample=10)
merge_lora(infer_args, device_map='cpu')
result = infer_main(infer_args)
torch.cuda.empty_cache()

app_ui_main(infer_args)
```

### è®­ç»ƒè„šæœ¬
ä½ å¯ä»¥å‚è€ƒä»¥ä¸‹è„šæœ¬æ¥è‡ªå®šä¹‰å±äºä½ çš„è®­ç»ƒè„šæœ¬.

- full: [qwen1half-7b-chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen1half_7b_chat/full) (A100)
- full+ddp+zero2: [qwen-7b-chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_7b_chat/full_ddp_zero2) (4\*A100)
- full+ddp+zero3: [qwen-14b-chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_14b_chat/full_ddp_zero3) (4\*A100)
- lora: [chatglm3-6b](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/chatglm3_6b/lora) (3090), [yi-34b-chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/yi_34b_chat/lora) (A100)
- lora+ddp: [chatglm3-6b](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/chatglm3_6b/lora_ddp) (2\*3090)
- lora+ddp+zero3: [qwen-14b-chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_14b_chat/lora_ddp_zero3) (4\*3090), [qwen-72b-chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_72b_chat/lora_ddp_zero3) (4\*A100)
- qlora(gptq-int4): [qwen-7b-chat-int4](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_7b_chat_int4/qlora) (3090)
- qlora(gptq-int8): [qwen1half-7b-chat-int8](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen1half_7b_chat_int8/qlora) (3090)
- qlora(bnb-int4): [qwen-7b-chat](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/qwen_7b_chat/qlora) (3090)


### ç‰¹æ€§
- æ”¯æŒçš„SFTæ–¹æ³•: [lora](https://arxiv.org/abs/2106.09685), [qlora](https://arxiv.org/abs/2305.14314), [longlora](https://arxiv.org/abs/2309.12307), [qalora](https://arxiv.org/abs/2309.14717), å…¨å‚æ•°å¾®è°ƒ, éƒ¨åˆ†å‚æ•°å¾®è°ƒ.
- æ”¯æŒçš„ç‰¹æ€§: æ¨¡å‹é‡åŒ–, DDP, æ¨¡å‹å¹¶è¡Œ, gradient checkpointing, æ”¯æŒæ¨é€ModelScope Hub, è‡ªå®šä¹‰æ•°æ®é›†, å¤šæ¨¡æ€å’ŒAgent SFT, å¤šè½®å¯¹è¯, DPO, è‡ªæˆ‘è®¤çŸ¥å¾®è°ƒ, ...
- æ”¯æŒçš„æ¨¡å‹: [[è¯¦ç»†ä¿¡æ¯]](https://github.com/modelscope/swift/blob/main/docs/source/LLM/%E6%94%AF%E6%8C%81%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86.md#%E6%A8%A1%E5%9E%8B)
  - å¤šæ¨¡æ€:
    - [qwen-vl](https://github.com/QwenLM/Qwen-VL) ç³»åˆ—: qwen-vl, qwen-vl-chat, qwen-vl-chat-int4.
    - [qwen-audio](https://github.com/QwenLM/Qwen-Audio) ç³»åˆ—: qwen-audio, qwen-audio-chat.
    - [yi-vl](https://github.com/01-ai/Yi) ç³»åˆ—: yi-vl-6b-chat, yi-vl-34b-chat.
    - [cogagent](https://github.com/THUDM/CogVLM) ç³»åˆ—: cogagent-18b-chat, cogagent-18b-instruct.
    - [internlm-xcomposer2](https://github.com/InternLM/InternLM-XComposer) ç³»åˆ—: internlm-xcomposer2-7b-chat.
  - é€šç”¨:
    - [qwen](https://github.com/QwenLM/Qwen) ç³»åˆ—:
      - qwen-1_8b, qwen-1_8b-chat, qwen-1_8b-chat-int4, qwen-1_8b-chat-int8.
      - qwen-7b, qwen-7b-chat, qwen-7b-chat-int4, qwen-7b-chat-int8.
      - qwen-14b, qwen-14b-chat, qwen-14b-chat-int4, qwen-14b-chat-int8.
      - qwen-72b, qwen-72b-chat, qwen-72b-chat-int4, qwen-72b-chat-int8.
    - [qwen1.5](https://github.com/QwenLM/Qwen1.5) ç³»åˆ—:
      - qwen1half-0_5b, qwen1half-0_5b-chat, qwen1half-0_5b-chat-int4, qwen1half-0_5b-chat-int8.
      - qwen1half-1_8b, qwen1half-1_8b-chat, qwen1half-1_8b-chat-int4, qwen1half-1_8b-chat-int8.
      - qwen1half-4b, qwen1half-4b-chat, qwen1half-4b-chat-int4, qwen1half-4b-chat-int8.
      - qwen1half-7b, qwen1half-7b-chat, qwen1half-7b-chat-int4, qwen1half-7b-chat-int8.
      - qwen1half-14b, qwen1half-14b-chat, qwen1half-14b-chat-int4, qwen1half-14b-chat-int8.
      - qwen1half-72b, qwen1half-72b-chat, qwen1half-72b-chat-int4, qwen1half-72b-chat-int8.
    - [chatglm](https://github.com/THUDM/ChatGLM-6B) ç³»åˆ—: chatglm2-6b, chatglm2-6b-32k, chatglm3-6b-base, chatglm3-6b, chatglm3-6b-32k.
    - [llama](https://github.com/facebookresearch/llama) ç³»åˆ—: llama2-7b, llama2-7b-chat, llama2-13b, llama2-13b-chat, llama2-70b, llama2-70b-chat.
    - [yi](https://github.com/01-ai/Yi) ç³»åˆ—: yi-6b, yi-6b-200k, yi-6b-chat, yi-34b, yi-34b-200k, yi-34b-chat.
    - [internlm](https://github.com/InternLM/InternLM) ç³»åˆ—:
      - internlm-7b, internlm-7b-chat, internlm-7b-chat-8k, internlm-20b, internlm-20b-chat.
      - internlm2-1_8b, internlm2-1_8b-sft-chat, internlm2-1_8b-chat, internlm2-7b-base, internlm2-7b, internlm2-7b-sft-chat, internlm2-7b-chat, internlm2-20b-base, internlm2-20b, internlm2-20b-sft-chat, internlm2-20b-chat.
    - [deepseek](https://github.com/deepseek-ai/deepseek-LLM) ç³»åˆ—: deepseek-7b, deepseek-7b-chat, deepseek-67b, deepseek-67b-chat, deepseek-moe-16b, deepseek-moe-16b-chat.
    - [gemma](https://github.com/google/gemma_pytorch) ç³»åˆ—: gemma-2b, gemma-2b-instruct, gemma-7b, gemma-7b-instruct.
    - [openbmb-minicpm](https://github.com/OpenBMB/mlc-MiniCPM) ç³»åˆ—: openbmb-minicpm-2b-sft-chat, openbmb-minicpm-2b-chat.
    - [openbuddy](https://github.com/OpenBuddy/OpenBuddy) ç³»åˆ—: openbuddy-llama2-13b-chat, openbuddy-llama-65b-chat, openbuddy-llama2-70b-chat, openbuddy-mistral-7b-chat, openbuddy-zephyr-7b-chat, openbuddy-deepseek-67b-chat, openbuddy-mixtral-moe-7b-chat.
    - [mistral](https://github.com/mistralai/mistral-src) ç³»åˆ—: mistral-7b, mistral-7b-instruct, mistral-7b-instruct-v2.
    - [mixtral](https://github.com/mistralai/mistral-src) ç³»åˆ—: mixtral-moe-7b, mixtral-moe-7b-instruct.
    - [baichuan](https://github.com/baichuan-inc/Baichuan2) ç³»åˆ—: baichuan-7b, baichuan-13b, baichuan-13b-chat, baichuan2-7b, baichuan2-7b-chat, baichuan2-13b, baichuan2-13b-chat, baichuan2-7b-chat-int4, baichuan2-13b-chat-int4.
    - [yuan](https://github.com/IEIT-Yuan/Yuan-2.0) ç³»åˆ—: yuan2-2b-instruct, yuan2-2b-janus-instruct, yuan2-51b-instruct, yuan2-102b-instruct.
    - [xverse](https://github.com/xverse-ai/XVERSE-13B) ç³»åˆ—: xverse-7b, xverse-7b-chat, xverse-13b, xverse-13b-chat, xverse-65b, xverse-65b-v2, xverse-65b-chat, xverse-13b-256k.
    - [orion](https://github.com/OrionStarAI/OrionStar-Yi-34B-Chat) ç³»åˆ—: orion-14b, orion-14b-chat.
    - [bluelm](https://github.com/vivo-ai-lab/BlueLM) ç³»åˆ—: bluelm-7b, bluelm-7b-chat, bluelm-7b-32k, bluelm-7b-chat-32k.
    - [zephyr](https://github.com/huggingface/alignment-handbook) ç³»åˆ—: zephyr-7b-beta-chat.
    - [ziya](https://github.com/IDEA-CCNL/Fengshenbang-LM) ç³»åˆ—: ziya2-13b, ziya2-13b-chat.
    - [skywork](https://github.com/SkyworkAI/Skywork) ç³»åˆ—: skywork-13b, skywork-13b-chat.
    - other: [polylm-13b](https://github.com/DAMO-NLP-MT/PolyLM), [seqgpt-560m](https://github.com/Alibaba-NLP/SeqGPT), [sus-34b-chat](https://github.com/SUSTech-IDEA/SUS-Chat).
  - é‡‘è:
    - [tongyi-finance](https://github.com/QwenLM/Qwen) ç³»åˆ—: tongyi-finance-14b, tongyi-finance-14b-chat, tongyi-finance-14b-chat-int4.
  - ä»£ç :
    - [codefuse](https://github.com/codefuse-ai) ç³»åˆ—: codefuse-codellama-34b-chat, codefuse-codegeex2-6b-chat, codefuse-qwen-14b-chat.
    - [deepseek-coder](https://github.com/deepseek-ai/DeepSeek-Coder) ç³»åˆ—: deepseek-coder-1_3b, deepseek-coder-1_3b-instruct, deepseek-coder-6_7b, deepseek-coder-6_7b-instruct, deepseek-coder-33b, deepseek-coder-33b-instruct.
    - [codegeex2](https://github.com/THUDM/CodeGeeX2) ç³»åˆ—: codegeex2-6b.
    - [phi](https://www.microsoft.com/en-us/research/blog/phi-2-the-surprising-power-of-small-language-models/) ç³»åˆ—: phi2-3b.
  - æ•°å­¦:
    - [internlm2-math](https://github.com/InternLM/InternLM-Math) ç³»åˆ—: internlm2-math-7b, internlm2-math-7b-chat, internlm2-math-20b, internlm2-math-20b-chat.
    - [deepseek-math](https://github.com/deepseek-ai/DeepSeek-Math) ç³»åˆ—: deepseek-math-7b, deepseek-math-7b-instruct, deepseek-math-7b-chat.
- æ”¯æŒçš„æ•°æ®é›†: [[è¯¦ç»†ä¿¡æ¯]](https://github.com/modelscope/swift/blob/main/docs/source/LLM/%E6%94%AF%E6%8C%81%E7%9A%84%E6%A8%A1%E5%9E%8B%E5%92%8C%E6%95%B0%E6%8D%AE%E9%9B%86.md#%E6%95%B0%E6%8D%AE%E9%9B%86)
  - NLP:
    - é€šç”¨: ğŸ”¥ms-bench, ğŸ”¥ms-bench-mini, ğŸ”¥alpaca-en(gpt4), ğŸ”¥alpaca-zh(gpt4), multi-alpaca-all, instinwild-en, instinwild-zh, cot-en, cot-zh, firefly-all-zh, instruct-en, gpt4all-en, sharegpt-en, sharegpt-zh, tulu-v2-sft-mixture, wikipedia-zh, open-orca, open-orca-gpt4, sharegpt-gpt4, ğŸ”¥sharegpt-gpt4-mini.
    - Agent: ğŸ”¥ms-agent, damo-mini-agent-zh, damo-agent-zh, agent-instruct-all-en.
    - RLHF: ğŸ”¥hh-rlhf-cn, stack-exchange-paired, hh-rlhf-harmless-base, hh-rlhf-helpful-base, hh-rlhf-helpful-online, hh-rlhf-helpful-rejection-sampled, hh-rlhf-red-team-attempts, hh-rlhf-cn-harmless-base-cn, hh-rlhf-cn-helpful-base-cn, hh-rlhf-cn-harmless-base-en, hh-rlhf-cn-helpful-base-en.
    - ä»£ç : code-alpaca-en, ğŸ”¥leetcode-python-en, ğŸ”¥codefuse-python-en, ğŸ”¥codefuse-evol-instruction-zh.
    - åŒ»ç–—: medical-en, medical-zh, medical-mini-zh, ğŸ”¥disc-med-sft-zh.
    - æ³•å¾‹: lawyer-llama-zh, tigerbot-law-zh, ğŸ”¥disc-law-sft-zh.
    - æ•°å­¦: ğŸ”¥blossom-math-zh, school-math-zh, open-platypus-en.
    - SQL: text2sql-en, ğŸ”¥sql-create-context-en.
    - æ–‡æœ¬ç”Ÿæˆ: ğŸ”¥advertise-gen-zh, ğŸ”¥dureader-robust-zh.
    - åˆ†ç±»: cmnli-zh, ğŸ”¥cmnli-mini-zh, ğŸ”¥jd-sentiment-zh, ğŸ”¥hc3-zh, ğŸ”¥hc3-en.
    - AWQ: pileval.
    - å…¶ä»–: finance-en, poetry-zh, webnovel-zh, generated-chat-zh, cls-fudan-news-zh, ner-jave-zh.
  - å¤šæ¨¡æ€:
    - è§†è§‰: coco-en, ğŸ”¥coco-mini-en, coco-mini-en-2, capcha-images.
    - éŸ³é¢‘: aishell1-zh, ğŸ”¥aishell1-mini-zh.
  - è‡ªå®šä¹‰æ•°æ®é›†
- æ”¯æŒçš„å¯¹è¯æ¨¡æ¿:
  - æ–‡æœ¬ç”Ÿæˆ: default-generation, default-generation-bos, chatglm-generation, qwen-audio-generation.
  - å¯¹è¯: default, qwen, qwen-audio, baichuan, chatglm2, chatglm3, llama, openbuddy, internlm, internlm2, internlm-xcomposer2, yi, yi-vl, yuan, xverse, ziya, skywork, bluelm, zephyr, sus, deepseek, deepseek-coder, codefuse-codellama, codefuse, cogagent-chat, cogagent-instruct, orion, openbmb, chatml.


## ğŸ”¥SCEdit

SCEditç”±é˜¿é‡Œå·´å·´é€šä¹‰å®éªŒå®¤è§†è§‰æ™ºèƒ½å›¢é˜Ÿ(Alibaba TongYi Vision Intelligence Lab)æ‰€æå‡ºï¼Œæ˜¯ä¸€ä¸ªé«˜æ•ˆçš„ç”Ÿæˆå¼å¾®è°ƒæ¡†æ¶ã€‚è¯¥æ¡†æ¶ä¸ä»…æ”¯æŒæ–‡ç”Ÿå›¾ä¸‹æ¸¸ä»»åŠ¡çš„å¾®è°ƒèƒ½åŠ›ï¼Œ**ç›¸æ¯”LoRAèŠ‚çœ30%-50%çš„è®­ç»ƒæ˜¾å­˜å¼€é”€**ï¼Œå®ç°å¿«é€Ÿè¿ç§»åˆ°ç‰¹å®šçš„ç”Ÿæˆåœºæ™¯ä¸­ï¼›è€Œä¸”è¿˜å¯ä»¥**ç›´æ¥æ‰©å±•åˆ°å¯æ§å›¾åƒç”Ÿæˆä»»åŠ¡ä¸­ï¼Œä»…éœ€ControlNetæ¡ä»¶ç”Ÿæˆ7.9%çš„å‚æ•°é‡å¹¶èŠ‚çœ30%çš„æ˜¾å­˜å¼€é”€**ï¼Œæ”¯æŒè¾¹ç¼˜å›¾ã€æ·±åº¦å›¾ã€åˆ†å‰²å›¾ã€å§¿æ€ã€é¢œè‰²å›¾ã€å›¾åƒè¡¥å…¨ç­‰æ¡ä»¶ç”Ÿæˆä»»åŠ¡ã€‚

æˆ‘ä»¬ä½¿ç”¨äº†[é£æ ¼è¿ç§»æ•°æ®é›†](https://modelscope.cn/datasets/damo/style_custom_dataset/dataPeview)ä¸­çš„3Dé£æ ¼æ•°æ®è¿›è¡Œäº†è®­ç»ƒï¼Œå¹¶ä½¿ç”¨ç›¸åŒçš„`Prompt: A boy in a camouflage jacket with a scarf`è¿›è¡Œæµ‹è¯•ï¼Œå…·ä½“çš„å®šæ€§å’Œå®šé‡çš„ç»“æœå¦‚ä¸‹ï¼š

| Method    | bs   | ep   | Target Module | Param. (M)    | Mem. (MiB) | 3D style                                                     |
| --------- | ---- | ---- | ------------- | ------------- | ---------- | ------------------------------------------------------------ |
| LoRA/r=64 | 1    | 50   | q/k/v/out/mlp | 23.94 (2.20%) | 8440MiB    | <img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/167218/1703665229562-0f33bbb0-c492-41b4-9f37-3ae720dca80d.png" alt="img" style="zoom:20%;" /> |
| SCEdit    | 1    | 50   | up_blocks     | 19.68 (1.81%) | 7556MiB    | <img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/167218/1703665933913-74b98741-3b57-46a4-9871-539df3a0112c.png" alt="img" style="zoom:20%;" /> |
| LoRA/r=64 | 10   | 100  | q/k/v/out/mlp | 23.94 (2.20%) | 26300MiB   | <img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/167218/1703750608529-de20d0e7-bf9c-4928-8e59-73cc54f2c8d7.png" alt="img" style="zoom:20%;" /> |
| SCEdit    | 10   | 100  | up_blocks     | 19.68 (1.81%) | 18634MiB   | <img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/167218/1703663033092-94492e44-341f-4259-9df4-13c168e3b5d6.png" alt="img" style="zoom:20%;" /> |
| LoRA/r=64 | 30   | 200  | q/k/v/out/mlp | 23.94 (2.20%) | 69554MiB   | <img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/167218/1703750626635-2e368d7b-5e99-4a06-b189-8615f302bcd7.png" alt="img" style="zoom:20%;" /> |
| SCEdit    | 30   | 200  | up_blocks     | 19.68 (1.81%) | 43350MiB   | <img src="https://intranetproxy.alipay.com/skylark/lark/0/2023/png/167218/1703662246942-1102b1f4-93ab-4653-b943-3302f2a5259e.png" alt="img" style="zoom:20%;" /> |

ä½¿ç”¨SCEditæ‰§è¡Œè®­ç»ƒä»»åŠ¡å¹¶å¤ç°ä¸Šè¿°ç»“æœï¼š

```shell
# å…ˆæ‰§è¡Œä¸‹é¢ç« èŠ‚çš„å®‰è£…æ­¥éª¤
cd examples/pytorch/multi_modal/notebook
python text_to_image_synthesis.py
```

## ğŸ› ï¸ å®‰è£…

SWIFTåœ¨Pythonç¯å¢ƒä¸­è¿è¡Œã€‚è¯·ç¡®ä¿æ‚¨çš„Pythonç‰ˆæœ¬é«˜äº3.8ã€‚

- æ–¹æ³•1ï¼šä½¿ç”¨pipå‘½ä»¤å®‰è£…SWIFTï¼š

```shell
# å…¨é‡èƒ½åŠ›
pip install ms-swift[all] -U
# ä»…ä½¿ç”¨LLM
pip install ms-swift[llm] -U
# ä»…ä½¿ç”¨AIGC
pip install ms-swift[aigc] -U
# ä»…ä½¿ç”¨adapters
pip install ms-swift -U
```

- æ–¹æ³•2ï¼šé€šè¿‡æºä»£ç å®‰è£…SWIFTï¼ˆæ–¹ä¾¿è¿è¡Œè®­ç»ƒæ¨ç†è„šæœ¬ï¼‰ï¼Œè¯·è¿è¡Œä»¥ä¸‹å‘½ä»¤ï¼š

```shell
git clone https://github.com/modelscope/swift.git
cd swift
pip install -e .[llm]
```

SWIFTä¾èµ–torch>=1.13ã€‚

- æ–¹æ³•3ï¼šåœ¨æˆ‘ä»¬çš„Dockeré•œåƒä¸­ä½¿ç”¨SWIFT

```shell
docker pull registry.cn-hangzhou.aliyuncs.com/modelscope-repo/modelscope:ubuntu20.04-cuda11.8.0-py38-torch2.0.1-tf2.13.0-1.9.1
```

## ğŸš€ å¿«é€Ÿå¼€å§‹

SWIFTæ”¯æŒå¤šä¸ªtunersï¼ŒåŒ…æ‹¬ç”±[PEFT](https://github.com/huggingface/peft)æä¾›çš„tunersã€‚è¦ä½¿ç”¨è¿™äº›tunersï¼Œåªéœ€è°ƒç”¨:
```python
from swift import Swift, LoRAConfig
config = LoRAConfig(...)
model = Swift.prepare_model(model, config, extra_state_keys=['...'])
```
ä¸Šé¢çš„ä»£ç ç‰‡æ®µéšæœºåˆå§‹åŒ–äº†tunerã€‚è¾“å…¥modelæ˜¯torch.nn.Moduleçš„ä¸€ä¸ªå®ä¾‹ï¼Œconfigæ˜¯SwiftConfigæˆ–PeftConfigçš„å­ç±»å®ä¾‹ã€‚extra_state_keysæ˜¯è¦è®­ç»ƒå¹¶å­˜å‚¨åœ¨è¾“å‡ºç›®å½•ä¸­çš„é¢å¤–æ¨¡å—æƒé‡ï¼ˆå¦‚linear headï¼‰ã€‚

æ‚¨å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ç»„åˆå¤šä¸ªtunersï¼š
```python
from swift import Swift, LoRAConfig, PromptConfig
model = Swift.prepare_model(model, {'lora': LoRAConfig(...), 'prompt': PromptConfig(...)})
```

åœ¨å¾®è°ƒä¹‹åï¼Œæ‚¨å¯ä»¥è°ƒç”¨save_pretrainedå’Œpush_to_hubæ–¹æ³•ï¼š

```python
from swift import push_to_hub
model.save_pretrained('some-output-folder')
push_to_hub('my-group/some-repo-id-modelscope', 'some-output-folder', token='some-ms-token')
```
å‡è®¾`my-group/some-repo-id-modelscope`æ˜¯Hubä¸­çš„model-idï¼Œè€Œ`some-ms-token`æ˜¯ç”¨äºä¸Šä¼ çš„ä»¤ç‰Œã€‚

ä½¿ç”¨model-idè¿›è¡Œåç»­æ¨ç†ï¼š

```python
from swift import Swift
model = Swift.from_pretrained(model, 'my-group/some-repo-id-modelscope')
```

ä¸‹é¢æ˜¯ä¸€ä¸ªå¯è¿è¡Œçš„ç¤ºä¾‹ï¼š

```python
import os
import tempfile

# è¯·é€šè¿‡`pip install modelscope`å®‰è£…modelscope
from modelscope import Model

from swift import LoRAConfig, SwiftModel, Swift, push_to_hub

tmp_dir = tempfile.TemporaryDirectory().name
if not os.path.exists(tmp_dir):
    os.makedirs(tmp_dir)


model = Model.from_pretrained('modelscope/Llama-2-7b-ms', device_map='auto')
lora_config = LoRAConfig(target_modules=['q_proj', 'k_proj', 'v_proj'])
model: SwiftModel = Swift.prepare_model(model, lora_config)
# åœ¨è¿™é‡Œè¿›è¡Œä¸€äº›å¾®è°ƒæ“ä½œ
model.save_pretrained(tmp_dir)

push_to_hub('my-group/swift_llama2', output_dir=tmp_dir)
model = Model.from_pretrained('modelscope/Llama-2-7b-ms', device_map='auto')
model = SwiftModel.from_pretrained(model, 'my-group/swift_llama2', device_map='auto')
```

è¿™æ˜¯ä¸€ä¸ªä½¿ç”¨transformersåº“å®ä¾‹åŒ–æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨SWIFTè¿›è¡Œé«˜æ•ˆå¾®è°ƒçš„ç¤ºä¾‹ã€‚

```python
from swift import Swift, LoRAConfig, AdapterConfig, PromptConfig
from transformers import AutoModelForImageClassification

# åˆå§‹vitæ¨¡å‹
model = AutoModelForImageClassification.from_pretrained("google/vit-base-patch16-224")

# åˆå§‹åŒ–LoRA tuneré…ç½®
lora_config = LoRAConfig(
    r=10,  # LoRAæ¨¡å—çš„rank
    target_modules=['query', 'key', 'value'],  # å°†è¦è¢«æ›¿æ¢çš„æ¨¡å—çš„æ¨¡å—ååç¼€
    merge_weights=False  # æ˜¯å¦åˆå¹¶æƒé‡
)

# åˆå§‹åŒ–adapter tuneré…ç½®
adapter_config = AdapterConfig(
    dim=768,  # hidden statesçš„ç»´åº¦
    hidden_pos=0,  # è¦ä¼ é€’åˆ°adapterçš„hidden stateçš„ä½ç½®
    target_modules=r'.*attention.output.dense$',  # è¦ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢çš„æ¨¡å—
    adapter_length=10  # adapteré•¿åº¦
)

# åˆå§‹åŒ–prompt tuneré…ç½®
prompt_config = PromptConfig(
    dim=768,  # hidden statesçš„ç»´åº¦
    target_modules=r'.*layer\.\d+$',  # è¦ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼æ›¿æ¢çš„æ¨¡å—
    embedding_pos=0,    # embeddingå¼ é‡çš„ä½ç½®
    prompt_length=10,   # æç¤ºç¬¦tokençš„é•¿åº¦
    attach_front=False  # æ˜¯å¦å°†æç¤ºç¬¦é™„åŠ åœ¨embeddingå‰é¢
)

# ä½¿ç”¨swiftåˆ›å»ºæ¨¡å‹ã€‚åœ¨å®è·µä¸­ï¼Œæ‚¨å¯ä»¥ä½¿ç”¨å…¶ä¸­ä»»ä½•ä¸€ä¸ªè°ƒè°å™¨æˆ–å®ƒä»¬çš„ç»„åˆã€‚
model = Swift.prepare_model(model, {"lora_tuner": lora_config, "adapter_tuner": adapter_config, "prompt_tuner": prompt_config})

# è·å–æ¨¡å‹çš„å¯è®­ç»ƒå‚æ•°ã€‚
model.get_trainable_parameters()
# 'trainable params: 838,776 || all params: 87,406,432 || trainable%: 0.9596273189597764'
```

å¯ä»¥åœ¨SWIFTä¸­ä½¿ç”¨PEFTæä¾›çš„åŠŸèƒ½ï¼š

```python
from swift import LoraConfig, Swift
from peft import TaskType
lora_config = LoraConfig(target_modules=['query', 'key', 'value'], task_type=TaskType.CAUSAL_LM)
model_wrapped = Swift.prepare_model(model, lora_config)

# æˆ–è€…ä½¿ç”¨from_pretrainedä»modelscope hubä¸­åŠ è½½æƒé‡ã€‚
model_wrapped = Swift.from_pretrained(model, 'some-id-in-the-modelscope-modelhub')
```

Swift tunerså’ŒPeft tunersä¹‹é—´çš„ä¿å­˜ç­–ç•¥ç•¥æœ‰ä¸åŒã€‚å¯ä»¥é€šè¿‡ä»¥ä¸‹æ–¹å¼ä¸ºSwift tunerså‘½åï¼š

```python
model = Swift.prepare_model(model, {'default': LoRAConfig(...)})
model.save_pretrained('./output')
```

åœ¨outputç›®å½•ä¸­å°†ä¼šå¾—åˆ°ä»¥ä¸‹ç±»ä¼¼çš„ç›®å½•ç»“æ„ï¼š

```text
output
    |-- default
        |-- adapter_config.json
        |-- adapter_model.bin
    |-- adapter_config.json
    |-- adapter_model.bin
```

å­˜å‚¨åœ¨outputç›®å½•ä¸­çš„config/weightsæ˜¯extra_state_keysçš„é…ç½®å’Œæƒé‡ã€‚è¿™ä¸Peftä¸åŒï¼ŒPeftå­˜å‚¨äº†`default` tunerçš„config/weightsã€‚

## ğŸ” äº†è§£æ›´å¤š

- [ModelScopeåº“](https://github.com/modelscope/modelscope/)

  ModelScopeåº“æ˜¯ModelScopeé¡¹ç›®çš„æ¨¡å‹åº“ï¼ŒåŒ…å«äº†å„æ¨¡æ€çƒ­é—¨çš„æ·±åº¦å­¦ä¹ æ¨¡å‹ã€‚

- [å°†è‡ªå·±çš„æ¨¡å‹è´¡çŒ®ç»™ModelScope](https://modelscope.cn/docs/ModelScope%E6%A8%A1%E5%9E%8B%E6%8E%A5%E5%85%A5%E6%B5%81%E7%A8%8B%E6%A6%82%E8%A7%88)

## License

æœ¬é¡¹ç›®ä½¿ç”¨[Apache License (Version 2.0)](https://github.com/modelscope/modelscope/blob/master/LICENSE)è¿›è¡Œè®¸å¯ã€‚


## â˜ è”ç³»æˆ‘ä»¬

æ‚¨å¯ä»¥é€šè¿‡åŠ æˆ‘ä»¬çš„å¾®ä¿¡ç¾¤, æ¥å’Œæˆ‘ä»¬è”ç³»å’Œäº¤æµ:

<p align="left">
<img src="asset/wechat.png" width="250" style="display: inline-block;">
</p>


## Star History

[![Star History Chart](https://api.star-history.com/svg?repos=modelscope/swift&type=Date)](https://star-history.com/#modelscope/swift&Date)
