{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Omega17Exp LoRA SFT Fine-tuning with MS-SWIFT\n",
        "\n",
        "This notebook demonstrates how to fine-tune the custom **Omega17ExpForCausalLM** model using LoRA (Low-Rank Adaptation) for Supervised Fine-Tuning (SFT).\n",
        "\n",
        "## Model Specifications\n",
        "- **Architecture**: Omega17ExpForCausalLM (MoE - Mixture of Experts)\n",
        "- **Hidden Size**: 2048\n",
        "- **Layers**: 48\n",
        "- **Experts**: 128 total, 8 per token\n",
        "- **Context Length**: 262,144 tokens\n",
        "- **Vocab Size**: 151,936\n",
        "\n",
        "## Requirements\n",
        "- RunPod with GPU (A100 40GB+ recommended for this MoE model)\n",
        "- Custom transformers fork: `transformers-usf-om-vl-exp-v0`"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 1. Environment Setup"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Install the custom transformers fork (REQUIRED for Omega17Exp model)\n",
        "!pip install transformers-usf-om-vl-exp-v0 -q\n",
        "\n",
        "# Install MS-SWIFT and dependencies\n",
        "!pip install ms-swift[llm] -q\n",
        "\n",
        "# Additional dependencies\n",
        "!pip install accelerate bitsandbytes peft datasets -q\n",
        "\n",
        "# For DeepSpeed (optional but recommended for large models)\n",
        "!pip install deepspeed -q"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Verify GPU availability\n",
        "import torch\n",
        "print(f\"PyTorch version: {torch.__version__}\")\n",
        "print(f\"CUDA available: {torch.cuda.is_available()}\")\n",
        "if torch.cuda.is_available():\n",
        "    print(f\"GPU: {torch.cuda.get_device_name(0)}\")\n",
        "    print(f\"GPU Memory: {torch.cuda.get_device_properties(0).total_memory / 1e9:.2f} GB\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Register Omega17Exp model with MS-SWIFT\n",
        "# This imports the omega17 module which handles all registration automatically\n",
        "\n",
        "from swift.llm.model.register import MODEL_MAPPING\n",
        "\n",
        "# Check if already registered\n",
        "if 'omega17_exp' not in MODEL_MAPPING:\n",
        "    # Import omega17 module to trigger registration\n",
        "    from swift.llm.model.model import omega17\n",
        "    print(\"✅ Omega17Exp model registered successfully!\")\n",
        "else:\n",
        "    print(\"✅ Omega17Exp model already registered\")\n",
        "\n",
        "# Verify registration\n",
        "print(f\"\\nModel type 'omega17_exp' in MODEL_MAPPING: {'omega17_exp' in MODEL_MAPPING}\")\n",
        "\n",
        "# Show model metadata\n",
        "if 'omega17_exp' in MODEL_MAPPING:\n",
        "    meta = MODEL_MAPPING['omega17_exp']\n",
        "    print(f\"Template: {meta.template}\")\n",
        "    print(f\"Architectures: {meta.architectures}\")\n",
        "    print(f\"Model arch: {meta.model_arch}\")\n",
        "    print(f\"Additional saved files: {meta.additional_saved_files}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from swift.llm.model.register import (\n",
        "    Model, ModelGroup, ModelMeta, register_model,\n",
        "    get_model_tokenizer_with_flash_attn\n",
        ")\n",
        "from swift.llm.model.constant import LLMModelType\n",
        "from swift.llm.model.model_arch import ModelArch\n",
        "from swift.llm.model.patcher import patch_output_to_input_device\n",
        "from swift.llm import TemplateType\n",
        "\n",
        "# Add custom model type\n",
        "if not hasattr(LLMModelType, 'omega17_exp'):\n",
        "    LLMModelType.omega17_exp = 'omega17_exp'\n",
        "\n",
        "\n",
        "def get_model_tokenizer_omega17_exp(model_dir, model_info, model_kwargs, load_model=True, **kwargs):\n",
        "    \"\"\"\n",
        "    Custom get_model_tokenizer function for Omega17Exp MoE model.\n",
        "    \"\"\"\n",
        "    model, tokenizer = get_model_tokenizer_with_flash_attn(\n",
        "        model_dir, model_info, model_kwargs, load_model, **kwargs\n",
        "    )\n",
        "    \n",
        "    if model is not None:\n",
        "        # Fix dtype for MoE layers\n",
        "        try:\n",
        "            mlp_cls = model.model.layers[1].mlp.__class__\n",
        "            for module in model.modules():\n",
        "                if isinstance(module, mlp_cls):\n",
        "                    patch_output_to_input_device(module)\n",
        "        except (AttributeError, IndexError):\n",
        "            pass\n",
        "    \n",
        "    return model, tokenizer\n",
        "\n",
        "\n",
        "# Register the model\n",
        "register_model(\n",
        "    ModelMeta(\n",
        "        LLMModelType.omega17_exp,\n",
        "        model_groups=[ModelGroup([])],\n",
        "        template=TemplateType.chatml,\n",
        "        get_function=get_model_tokenizer_omega17_exp,\n",
        "        architectures=['Omega17ExpForCausalLM'],\n",
        "        model_arch=ModelArch.llama,\n",
        "        requires=['transformers-usf-om-vl-exp-v0'],\n",
        "    ),\n",
        "    exist_ok=True\n",
        ")\n",
        "\n",
        "print(\"✅ Omega17Exp model registered successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - MODIFY THESE VALUES\n",
        "# ============================================================\n",
        "\n",
        "# Model path (local path or HuggingFace model ID)\n",
        "MODEL_PATH = \"/path/to/your/omega17-exp-model\"  # <-- CHANGE THIS\n",
        "\n",
        "# Dataset configuration\n",
        "# Option 1: Use a HuggingFace dataset\n",
        "DATASET = \"alpaca-en\"  # MS-SWIFT built-in dataset\n",
        "# Option 2: Use your custom dataset (JSONL format)\n",
        "# DATASET = \"/path/to/your/dataset.jsonl\"\n",
        "\n",
        "# Output directory for checkpoints\n",
        "OUTPUT_DIR = \"./output/omega17_lora_sft\"\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 1  # Per device batch size (reduce if OOM)\n",
        "GRADIENT_ACCUMULATION_STEPS = 16  # Effective batch size = BATCH_SIZE * GRAD_ACCUM\n",
        "MAX_LENGTH = 2048  # Maximum sequence length\n",
        "NUM_EPOCHS = 3\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "# LoRA configuration\n",
        "LORA_RANK = 64  # LoRA rank (higher = more parameters)\n",
        "LORA_ALPHA = 128  # LoRA alpha (typically 2x rank)\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# Target modules for LoRA (Omega17Exp architecture)\n",
        "LORA_TARGET_MODULES = [\n",
        "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n",
        "    \"gate_proj\", \"up_proj\", \"down_proj\"  # MLP\n",
        "]\n",
        "\n",
        "# Quantization (for memory efficiency)\n",
        "USE_QLORA = False  # Set to True for 4-bit quantization\n",
        "QUANT_BITS = 4  # 4 or 8 bit quantization\n",
        "\n",
        "print(f\"Model: {MODEL_PATH}\")\n",
        "print(f\"Dataset: {DATASET}\")\n",
        "print(f\"Output: {OUTPUT_DIR}\")\n",
        "print(f\"LoRA Rank: {LORA_RANK}, Alpha: {LORA_ALPHA}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# ============================================================\n",
        "# CONFIGURATION - MODIFY THESE VALUES\n",
        "# ============================================================\n",
        "\n",
        "# Model path (local path or HuggingFace model ID)\n",
        "MODEL_PATH = \"/path/to/your/omega17-exp-model\"  # <-- CHANGE THIS\n",
        "\n",
        "# Dataset configuration\n",
        "# Option 1: Use a HuggingFace dataset\n",
        "DATASET = \"alpaca-en\"  # MS-SWIFT built-in dataset\n",
        "# Option 2: Use your custom dataset (JSONL format)\n",
        "# DATASET = \"/path/to/your/dataset.jsonl\"\n",
        "\n",
        "# Output directory for checkpoints\n",
        "OUTPUT_DIR = \"./output/omega17_lora_sft\"\n",
        "\n",
        "# Training hyperparameters\n",
        "BATCH_SIZE = 1  # Per device batch size (reduce if OOM)\n",
        "GRADIENT_ACCUMULATION_STEPS = 16  # Effective batch size = BATCH_SIZE * GRAD_ACCUM\n",
        "MAX_LENGTH = 2048  # Maximum sequence length\n",
        "NUM_EPOCHS = 3\n",
        "LEARNING_RATE = 1e-4\n",
        "\n",
        "# LoRA configuration\n",
        "LORA_RANK = 64  # LoRA rank (higher = more parameters)\n",
        "LORA_ALPHA = 128  # LoRA alpha (typically 2x rank)\n",
        "LORA_DROPOUT = 0.05\n",
        "\n",
        "# Target modules for LoRA\n",
        "# For Omega17Exp (llama-style architecture), target attention and MLP\n",
        "LORA_TARGET_MODULES = [\n",
        "    \"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",  # Attention\n",
        "    \"gate_proj\", \"up_proj\", \"down_proj\"  # MLP (if not targeting experts)\n",
        "]\n",
        "\n",
        "# Quantization (for memory efficiency)\n",
        "USE_QLORA = False  # Set to True for 4-bit quantization\n",
        "QUANT_BITS = 4  # 4 or 8 bit quantization\n",
        "\n",
        "print(f\"Model: {MODEL_PATH}\")\n",
        "print(f\"Dataset: {DATASET}\")\n",
        "print(f\"Output: {OUTPUT_DIR}\")\n",
        "print(f\"LoRA Rank: {LORA_RANK}, Alpha: {LORA_ALPHA}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 4. Prepare Dataset\n",
        "\n",
        "MS-SWIFT supports multiple dataset formats. Here's how to prepare your data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Example: Create a custom dataset in JSONL format\n",
        "# Each line should be a JSON object with the following format:\n",
        "\n",
        "import json\n",
        "import os\n",
        "\n",
        "# Example dataset structure for SFT\n",
        "example_data = [\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "            {\"role\": \"user\", \"content\": \"What is machine learning?\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"Machine learning is a subset of artificial intelligence...\"}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"messages\": [\n",
        "            {\"role\": \"user\", \"content\": \"Explain quantum computing.\"},\n",
        "            {\"role\": \"assistant\", \"content\": \"Quantum computing uses quantum mechanics principles...\"}\n",
        "        ]\n",
        "    }\n",
        "]\n",
        "\n",
        "# Alternative format (query/response)\n",
        "example_data_simple = [\n",
        "    {\n",
        "        \"query\": \"What is the capital of France?\",\n",
        "        \"response\": \"The capital of France is Paris.\"\n",
        "    },\n",
        "    {\n",
        "        \"query\": \"Write a Python function to calculate factorial.\",\n",
        "        \"response\": \"def factorial(n):\\n    if n <= 1:\\n        return 1\\n    return n * factorial(n-1)\"\n",
        "    }\n",
        "]\n",
        "\n",
        "# Save example dataset\n",
        "os.makedirs(\"./data\", exist_ok=True)\n",
        "with open(\"./data/example_train.jsonl\", \"w\") as f:\n",
        "    for item in example_data:\n",
        "        f.write(json.dumps(item) + \"\\n\")\n",
        "\n",
        "print(\"Example dataset created at ./data/example_train.jsonl\")\n",
        "print(\"\\nSupported dataset formats:\")\n",
        "print(\"1. messages format: [{role: 'user', content: '...'}, {role: 'assistant', content: '...'}]\")\n",
        "print(\"2. query/response format: {query: '...', response: '...'}\")\n",
        "print(\"3. instruction/input/output format: {instruction: '...', input: '...', output: '...'}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 5. Method A: Train Using MS-SWIFT Python API"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from swift.llm import sft_main, SftArguments, TrainArguments\n",
        "from swift.llm.model.register import MODEL_MAPPING\n",
        "\n",
        "# Verify model is registered\n",
        "print(f\"Registered model types: {list(MODEL_MAPPING.keys())[-10:]}...\")  # Show last 10\n",
        "print(f\"omega17_exp registered: {'omega17_exp' in MODEL_MAPPING}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define training arguments\n",
        "sft_args = SftArguments(\n",
        "    # Model configuration\n",
        "    model=MODEL_PATH,\n",
        "    model_type='omega17_exp',  # Use registered model type\n",
        "    \n",
        "    # Dataset configuration\n",
        "    dataset=[DATASET],\n",
        "    \n",
        "    # Training type\n",
        "    train_type='lora',  # LoRA fine-tuning\n",
        "    \n",
        "    # LoRA configuration\n",
        "    lora_rank=LORA_RANK,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    lora_target_modules=LORA_TARGET_MODULES,\n",
        "    \n",
        "    # Quantization (optional)\n",
        "    quant_bits=QUANT_BITS if USE_QLORA else None,\n",
        "    \n",
        "    # Training parameters\n",
        "    output_dir=OUTPUT_DIR,\n",
        "    max_length=MAX_LENGTH,\n",
        "    num_train_epochs=NUM_EPOCHS,\n",
        "    per_device_train_batch_size=BATCH_SIZE,\n",
        "    gradient_accumulation_steps=GRADIENT_ACCUMULATION_STEPS,\n",
        "    learning_rate=LEARNING_RATE,\n",
        "    \n",
        "    # Optimizer and scheduler\n",
        "    optim='adamw_torch',\n",
        "    lr_scheduler_type='cosine',\n",
        "    warmup_ratio=0.03,\n",
        "    \n",
        "    # Precision\n",
        "    torch_dtype='bfloat16',\n",
        "    \n",
        "    # Logging and saving\n",
        "    logging_steps=10,\n",
        "    save_steps=500,\n",
        "    save_total_limit=3,\n",
        "    \n",
        "    # Gradient checkpointing (save memory)\n",
        "    gradient_checkpointing=True,\n",
        "    \n",
        "    # Evaluation\n",
        "    eval_steps=500,\n",
        "    \n",
        "    # Misc\n",
        "    seed=42,\n",
        "    report_to=['tensorboard'],\n",
        ")\n",
        "\n",
        "print(\"Training arguments configured!\")\n",
        "print(f\"\\nKey settings:\")\n",
        "print(f\"  - Train type: {sft_args.train_type}\")\n",
        "print(f\"  - LoRA rank: {sft_args.lora_rank}\")\n",
        "print(f\"  - Max length: {sft_args.max_length}\")\n",
        "print(f\"  - Batch size: {sft_args.per_device_train_batch_size}\")\n",
        "print(f\"  - Gradient accumulation: {sft_args.gradient_accumulation_steps}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Start training\n",
        "# Uncomment the line below to run training\n",
        "# result = sft_main(sft_args)\n",
        "\n",
        "print(\"To start training, uncomment and run: result = sft_main(sft_args)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 6. Method B: Train Using CLI (Recommended for Production)\n",
        "\n",
        "For production training, using the CLI is often more stable and easier to manage."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Generate the CLI command\n",
        "cli_command = f\"\"\"\n",
        "CUDA_VISIBLE_DEVICES=0 swift sft \\\\\n",
        "    --model {MODEL_PATH} \\\\\n",
        "    --model_type omega17_exp \\\\\n",
        "    --dataset {DATASET} \\\\\n",
        "    --train_type lora \\\\\n",
        "    --lora_rank {LORA_RANK} \\\\\n",
        "    --lora_alpha {LORA_ALPHA} \\\\\n",
        "    --lora_dropout {LORA_DROPOUT} \\\\\n",
        "    --lora_target_modules {' '.join(LORA_TARGET_MODULES)} \\\\\n",
        "    --output_dir {OUTPUT_DIR} \\\\\n",
        "    --max_length {MAX_LENGTH} \\\\\n",
        "    --num_train_epochs {NUM_EPOCHS} \\\\\n",
        "    --per_device_train_batch_size {BATCH_SIZE} \\\\\n",
        "    --gradient_accumulation_steps {GRADIENT_ACCUMULATION_STEPS} \\\\\n",
        "    --learning_rate {LEARNING_RATE} \\\\\n",
        "    --torch_dtype bfloat16 \\\\\n",
        "    --gradient_checkpointing true \\\\\n",
        "    --logging_steps 10 \\\\\n",
        "    --save_steps 500 \\\\\n",
        "    --save_total_limit 3\n",
        "\"\"\"\n",
        "\n",
        "print(\"CLI Command for training:\")\n",
        "print(\"=\" * 60)\n",
        "print(cli_command)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save CLI command to a shell script\n",
        "with open(\"train_omega17_lora.sh\", \"w\") as f:\n",
        "    f.write(\"#!/bin/bash\\n\")\n",
        "    f.write(\"# Omega17Exp LoRA SFT Training Script\\n\\n\")\n",
        "    f.write(\"# Install dependencies first:\\n\")\n",
        "    f.write(\"# pip install transformers-usf-om-vl-exp-v0\\n\")\n",
        "    f.write(\"# pip install ms-swift[llm]\\n\\n\")\n",
        "    f.write(cli_command.strip())\n",
        "\n",
        "print(\"Training script saved to: train_omega17_lora.sh\")\n",
        "print(\"Run with: bash train_omega17_lora.sh\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 7. Multi-GPU Training with DeepSpeed"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# DeepSpeed ZeRO-2 configuration for multi-GPU training\n",
        "deepspeed_config = {\n",
        "    \"train_batch_size\": \"auto\",\n",
        "    \"train_micro_batch_size_per_gpu\": \"auto\",\n",
        "    \"gradient_accumulation_steps\": \"auto\",\n",
        "    \"gradient_clipping\": 1.0,\n",
        "    \"zero_optimization\": {\n",
        "        \"stage\": 2,\n",
        "        \"offload_optimizer\": {\n",
        "            \"device\": \"cpu\",\n",
        "            \"pin_memory\": True\n",
        "        },\n",
        "        \"allgather_partitions\": True,\n",
        "        \"allgather_bucket_size\": 2e8,\n",
        "        \"reduce_scatter\": True,\n",
        "        \"reduce_bucket_size\": 2e8,\n",
        "        \"overlap_comm\": True,\n",
        "        \"contiguous_gradients\": True\n",
        "    },\n",
        "    \"bf16\": {\n",
        "        \"enabled\": True\n",
        "    }\n",
        "}\n",
        "\n",
        "# Save DeepSpeed config\n",
        "import json\n",
        "with open(\"ds_config_zero2.json\", \"w\") as f:\n",
        "    json.dump(deepspeed_config, f, indent=2)\n",
        "\n",
        "print(\"DeepSpeed config saved to: ds_config_zero2.json\")\n",
        "\n",
        "# Multi-GPU training command\n",
        "multi_gpu_command = f\"\"\"\n",
        "# Multi-GPU training with DeepSpeed ZeRO-2\n",
        "CUDA_VISIBLE_DEVICES=0,1,2,3 swift sft \\\\\n",
        "    --model {MODEL_PATH} \\\\\n",
        "    --model_type omega17_exp \\\\\n",
        "    --dataset {DATASET} \\\\\n",
        "    --train_type lora \\\\\n",
        "    --lora_rank {LORA_RANK} \\\\\n",
        "    --lora_alpha {LORA_ALPHA} \\\\\n",
        "    --output_dir {OUTPUT_DIR} \\\\\n",
        "    --max_length {MAX_LENGTH} \\\\\n",
        "    --num_train_epochs {NUM_EPOCHS} \\\\\n",
        "    --per_device_train_batch_size {BATCH_SIZE} \\\\\n",
        "    --gradient_accumulation_steps {GRADIENT_ACCUMULATION_STEPS} \\\\\n",
        "    --learning_rate {LEARNING_RATE} \\\\\n",
        "    --deepspeed ds_config_zero2.json \\\\\n",
        "    --torch_dtype bfloat16 \\\\\n",
        "    --gradient_checkpointing true\n",
        "\"\"\"\n",
        "\n",
        "print(\"\\nMulti-GPU Command:\")\n",
        "print(\"=\" * 60)\n",
        "print(multi_gpu_command)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 8. Inference with Trained LoRA Adapter"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# After training, load the model with LoRA adapter for inference\n",
        "from swift.llm import InferArguments, infer_main, get_model_tokenizer, get_template\n",
        "\n",
        "# Path to your trained LoRA adapter\n",
        "ADAPTER_PATH = f\"{OUTPUT_DIR}/checkpoint-xxx\"  # Replace xxx with actual checkpoint\n",
        "\n",
        "# Inference configuration\n",
        "infer_args = InferArguments(\n",
        "    model=MODEL_PATH,\n",
        "    model_type='omega17_exp',\n",
        "    adapters=[ADAPTER_PATH],  # Load LoRA adapter\n",
        "    torch_dtype='bfloat16',\n",
        "    max_new_tokens=512,\n",
        "    temperature=0.7,\n",
        "    top_p=0.9,\n",
        ")\n",
        "\n",
        "print(\"Inference configuration ready!\")\n",
        "print(f\"Model: {MODEL_PATH}\")\n",
        "print(f\"Adapter: {ADAPTER_PATH}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLI inference command\n",
        "infer_cli = f\"\"\"\n",
        "swift infer \\\\\n",
        "    --model {MODEL_PATH} \\\\\n",
        "    --model_type omega17_exp \\\\\n",
        "    --adapters {ADAPTER_PATH} \\\\\n",
        "    --torch_dtype bfloat16 \\\\\n",
        "    --max_new_tokens 512 \\\\\n",
        "    --temperature 0.7 \\\\\n",
        "    --stream true\n",
        "\"\"\"\n",
        "\n",
        "print(\"CLI Inference Command:\")\n",
        "print(\"=\" * 60)\n",
        "print(infer_cli)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 9. Merge LoRA Weights (Optional)\n",
        "\n",
        "Merge LoRA adapter into the base model for faster inference without adapter loading overhead."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# CLI command to merge LoRA weights\n",
        "merge_command = f\"\"\"\n",
        "swift export \\\\\n",
        "    --model {MODEL_PATH} \\\\\n",
        "    --model_type omega17_exp \\\\\n",
        "    --adapters {ADAPTER_PATH} \\\\\n",
        "    --merge_lora true \\\\\n",
        "    --output_dir ./merged_model\n",
        "\"\"\"\n",
        "\n",
        "print(\"Merge LoRA Command:\")\n",
        "print(\"=\" * 60)\n",
        "print(merge_command)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## 10. Troubleshooting\n",
        "\n",
        "### Common Issues and Solutions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "troubleshooting_guide = \"\"\"\n",
        "============================================================\n",
        "TROUBLESHOOTING GUIDE\n",
        "============================================================\n",
        "\n",
        "1. MODEL NOT FOUND ERROR\n",
        "   - Ensure transformers-usf-om-vl-exp-v0 is installed\n",
        "   - Check that model path is correct and accessible\n",
        "   - Verify model files include: config.json, modeling_omega17_exp.py, etc.\n",
        "\n",
        "2. OUT OF MEMORY (OOM)\n",
        "   - Reduce batch_size (try 1)\n",
        "   - Reduce max_length (try 1024 or 512)\n",
        "   - Enable gradient_checkpointing\n",
        "   - Use QLoRA with quant_bits=4\n",
        "   - Use DeepSpeed ZeRO-2 or ZeRO-3\n",
        "\n",
        "3. SLOW TRAINING\n",
        "   - Use flash attention (usually auto-enabled)\n",
        "   - Increase batch size if memory allows\n",
        "   - Use bf16 instead of fp16/fp32\n",
        "   - Use multi-GPU with DeepSpeed\n",
        "\n",
        "4. LORA NOT WORKING\n",
        "   - Check target_modules match your model architecture\n",
        "   - For MoE models, you may need to target router layers\n",
        "   - Try: [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\"]\n",
        "\n",
        "5. DATASET FORMAT ERRORS\n",
        "   - Use JSONL format with one JSON object per line\n",
        "   - Supported formats:\n",
        "     * messages: [{\"role\": \"user\", \"content\": \"...\"}, ...]\n",
        "     * query/response: {\"query\": \"...\", \"response\": \"...\"}\n",
        "     * instruction/output: {\"instruction\": \"...\", \"output\": \"...\"}\n",
        "\n",
        "6. CUSTOM TRANSFORMERS FORK ISSUES\n",
        "   - Uninstall regular transformers first:\n",
        "     pip uninstall transformers\n",
        "   - Then install the custom fork:\n",
        "     pip install transformers-usf-om-vl-exp-v0\n",
        "\n",
        "============================================================\n",
        "\"\"\"\n",
        "\n",
        "print(troubleshooting_guide)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Summary\n",
        "\n",
        "This notebook provides a complete workflow for LoRA SFT fine-tuning of the Omega17Exp model:\n",
        "\n",
        "1. **Environment Setup**: Install custom transformers fork and MS-SWIFT\n",
        "2. **Model Registration**: Register Omega17Exp with MS-SWIFT\n",
        "3. **Configuration**: Set model path, dataset, and training parameters\n",
        "4. **Training**: Use Python API or CLI for training\n",
        "5. **Multi-GPU**: DeepSpeed configuration for distributed training\n",
        "6. **Inference**: Load and use trained adapter\n",
        "7. **Export**: Merge LoRA weights into base model\n",
        "\n",
        "For questions or issues, check the MS-SWIFT documentation: https://swift.readthedocs.io/"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 4
}
