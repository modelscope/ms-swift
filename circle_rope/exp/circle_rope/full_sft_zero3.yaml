# Circle-RoPE Full SFT Training with DeepSpeed ZeRO-3
# Optimized for Qwen2.5-VL multi-GPU full parameter training with very large models
# Recommended for: 8+ GPUs or large models (32B+) that don't fit in ZeRO-2

# ==================== Model Configuration ====================
# Replace with your actual model path
model: /path/to/Qwen2.5-VL-32B-Instruct  # Or 72B
# Use the registered Circle-RoPE model type
model_type: qwen2_5_vl_circle_rope

# ==================== Training Type ====================
train_type: full  # Full parameter fine-tuning
torch_dtype: bfloat16  # Use bfloat16 for better numerical stability
attn_impl: flash_attn  # Flash Attention 2 for speed and memory efficiency

# ==================== Circle-RoPE Configuration ====================
# Optional: Override default Circle-RoPE settings
model_config_override: |
  {
    "circle_rope": {
      "alpha": 0.5,
      "radius": 10,
      "method": "circle",
      "AGE_mode": "strategy_4",
      "move_to_origin": true
    }
  }

# ==================== Dataset Configuration ====================
dataset: your-dataset-name  # Replace with your dataset
split_dataset_ratio: 0.05  # 5% for validation
max_length: 8192  # Maximum sequence length
truncation_strategy: delete

# ==================== Training Hyperparameters ====================
num_train_epochs: 3
per_device_train_batch_size: 1  # Batch size per GPU
per_device_eval_batch_size: 1
gradient_accumulation_steps: 32  # Larger accumulation for ZeRO-3

learning_rate: 1e-5  # Lower LR for very large models
weight_decay: 0.01
max_grad_norm: 1.0

# ==================== Learning Rate Schedule ====================
lr_scheduler_type: cosine
warmup_ratio: 0.03

# ==================== Vision Transformer Specific ====================
vit_lr: 5e-6  # Even lower LR for vision tower in large models
freeze_vit: false
freeze_aligner: false

# ==================== Optimizer Configuration ====================
optim: adamw_torch
adam_beta1: 0.9
adam_beta2: 0.999
adam_epsilon: 1e-8

# ==================== Memory Optimization ====================
gradient_checkpointing: true
use_reentrant: false

# ==================== DeepSpeed ZeRO-3 Configuration ====================
deepspeed: zero3
# ZeRO-3: Optimizer state + gradient + parameter partitioning
# Maximum memory savings, allows training very large models
# Has higher communication overhead than ZeRO-2
# Recommended for 8+ GPUs or models that don't fit in ZeRO-2

# ZeRO-3 Offload (Optional - for extreme memory constraints)
# Uncomment if you need to offload to CPU
# deepspeed_config_dict: |
#   {
#     "zero_optimization": {
#       "stage": 3,
#       "offload_optimizer": {
#         "device": "cpu",
#         "pin_memory": true
#       },
#       "offload_param": {
#         "device": "cpu",
#         "pin_memory": true
#       }
#     }
#   }

# ==================== Evaluation Configuration ====================
eval_strategy: steps
eval_steps: 500
save_strategy: steps
save_steps: 500
save_total_limit: 3  # Keep fewer checkpoints with ZeRO-3 (they're larger)
save_only_model: false

# ==================== Logging ====================
logging_steps: 10
logging_first_step: true
report_to: tensorboard

# ==================== Data Loading ====================
dataloader_num_workers: 4
dataset_num_proc: 16
dataloader_pin_memory: true
remove_unused_columns: false

# ==================== Mixed Precision Training ====================
fp16: false
bf16: true
bf16_full_eval: true

# ==================== Checkpointing ====================
save_safetensors: true
output_dir: output/qwen2_5_vl_circle_rope_full_zero3
resume_from_checkpoint: null

# ==================== Advanced Settings ====================
# For ZeRO-3, these settings are important
ddp_find_unused_parameters: false
gradient_checkpointing_kwargs: {"use_reentrant": false}

# Metrics
metric_for_best_model: eval_loss
greater_is_better: false
load_best_model_at_end: true

# ==================== Custom Settings ====================
model_name: qwen2.5-vl-circle-rope
model_author: your-name

# ==================== Usage ====================
# Run with deepspeed launcher (recommended for ZeRO-3):
# deepspeed --num_gpus=8 --master_port=29500 \
# $(which swift) sft --config examples/circle_rope/full_sft_zero3.yaml

# Or with torchrun:
# CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
# NPROC_PER_NODE=8 \
# swift sft --config examples/circle_rope/full_sft_zero3.yaml

# ==================== Performance Tips ====================
# 1. ZeRO-3 has higher communication overhead - ensure high-bandwidth interconnect (NVLink, InfiniBand)
# 2. Larger gradient_accumulation_steps helps amortize communication cost
# 3. For very large models (72B+), consider CPU offloading (uncomment deepspeed_config_dict above)
# 4. Monitor GPU memory usage - ZeRO-3 should allow much larger batch sizes than ZeRO-2
