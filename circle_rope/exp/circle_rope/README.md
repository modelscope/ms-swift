# Circle-RoPE for Qwen2.5-VL

Circle-RoPE (Circular Rotary Position Embedding) implementation for Qwen2.5-VL models, integrated with ms-swift for easy training and inference.

## üìã Table of Contents

- [Overview](#overview)
- [Features](#features)
- [Installation](#installation)
- [Quick Start](#quick-start)
- [Training Configurations](#training-configurations)
- [Advanced Usage](#advanced-usage)
- [Configuration Reference](#configuration-reference)
- [Troubleshooting](#troubleshooting)

## üéØ Overview

Circle-RoPE is a novel position embedding technique that maps positional encodings onto a circular manifold, providing better handling of long sequences and spatial relationships in vision-language models.

This implementation:
- ‚úÖ **Zero modification to original HF weights** - All changes applied dynamically at load time
- ‚úÖ **Fully integrated with ms-swift** - Use standard swift commands for training
- ‚úÖ **Multiple training modes** - LoRA, full fine-tuning with DeepSpeed ZeRO-2/3
- ‚úÖ **Multi-GPU optimized** - Flash Attention 2, gradient checkpointing, mixed precision
- ‚úÖ **Easy to use** - Just copy folder and use `model_type: qwen2_5_vl_circle_rope`

## ‚ú® Features

### Circle-RoPE Implementation
- **Configurable projection methods**: Circle, no-circle, or hybrid
- **AGE (Adaptive Group Embedding) modes**: Different layer-wise strategies
- **Auto-scaling radius**: Automatic or manual radius configuration
- **Coordinate transformations**: Origin centering, positive axis shifting

### Training Optimizations
- **DeepSpeed ZeRO-2/3**: Efficient distributed training for large models
- **Flash Attention 2**: 2-4x speedup and reduced memory usage
- **Gradient Checkpointing**: Trade computation for memory
- **Mixed Precision (BF16)**: Faster training with better stability than FP16
- **Vision Tower Tuning**: Separate learning rates for vision encoder
- **LoRA Support**: Parameter-efficient fine-tuning

## üöÄ Installation

### Prerequisites

```bash
# Install ms-swift (if not already installed)
pip install ms-swift -U

# Install required dependencies for Qwen2.5-VL
pip install transformers>=4.49 qwen_vl_utils>=0.0.6 decord
pip install flash-attn --no-build-isolation  # Optional but recommended
```

### Setup Circle-RoPE

1. **Copy the circle_rope folder to your model directory:**

```bash
# Navigate to your ms-swift installation
cd /path/to/ms-swift

# Copy circle_rope to your model directory
cp -r circle_rope /path/to/Qwen2.5-VL-7B-Instruct/
```

2. **Verify installation:**

```bash
python -c "import sys; sys.path.insert(0, '/path/to/Qwen2.5-VL-7B-Instruct'); from circle_rope import register_circle_rope; register_circle_rope()"
```

If no errors, you're ready to go!

## üé¨ Quick Start

### Option 1: Using the Launcher Script (Recommended)

```bash
# LoRA training (single GPU)
./examples/circle_rope/train.sh \
  --num_gpus 1 \
  --config examples/circle_rope/sft.yaml \
  --model /path/to/Qwen2.5-VL-7B-Instruct \
  --dataset your-dataset

# Full training with ZeRO-2 (4 GPUs)
./examples/circle_rope/train.sh \
  --num_gpus 4 \
  --config examples/circle_rope/full_sft_zero2.yaml \
  --model /path/to/Qwen2.5-VL-7B-Instruct

# Full training with ZeRO-3 (8 GPUs, large model)
./examples/circle_rope/train.sh \
  --num_gpus 8 \
  --config examples/circle_rope/full_sft_zero3.yaml \
  --model /path/to/Qwen2.5-VL-32B-Instruct
```

### Option 2: Direct Swift Command

```bash
# Single GPU
swift sft --config examples/circle_rope/sft.yaml

# Multi-GPU (automatic DDP)
CUDA_VISIBLE_DEVICES=0,1,2,3 swift sft --config examples/circle_rope/sft.yaml

# Multi-GPU with DeepSpeed
deepspeed --num_gpus=4 $(which swift) sft --config examples/circle_rope/full_sft_zero2.yaml
```

### Option 3: Python API

```python
import sys
sys.path.insert(0, '/path/to/Qwen2.5-VL-7B-Instruct')
from circle_rope import register_circle_rope

# Register the model type
register_circle_rope()

# Now use swift normally
from swift import sft_main, SftArguments

args = SftArguments(
    model='/path/to/Qwen2.5-VL-7B-Instruct',
    model_type='qwen2_5_vl_circle_rope',
    dataset='your-dataset',
    train_type='lora',
    # ... other arguments
)
sft_main(args)
```

## üìö Training Configurations

We provide three optimized configurations for different scenarios:

### 1. LoRA Fine-tuning (`sft.yaml`)
**Best for:** Quick experimentation, limited GPU memory

- **GPUs**: 1-4 GPUs
- **Memory**: ~24GB per GPU
- **Training type**: LoRA
- **Features**:
  - Flash Attention 2
  - Gradient accumulation
  - BF16 mixed precision

```bash
swift sft --config examples/circle_rope/sft.yaml
```

### 2. Full Fine-tuning with ZeRO-2 (`full_sft_zero2.yaml`)
**Best for:** Medium models (7B-13B), 4-8 GPUs

- **GPUs**: 4-8 GPUs with 40GB+ VRAM
- **Memory**: ~40GB per GPU (A100/A800)
- **Training type**: Full parameter fine-tuning
- **DeepSpeed**: ZeRO-2 (optimizer + gradient partitioning)
- **Features**:
  - Flash Attention 2
  - Gradient checkpointing
  - Vision tower differential LR
  - BF16 mixed precision

```bash
./examples/circle_rope/train.sh --num_gpus 4 --config examples/circle_rope/full_sft_zero2.yaml
```

**Recommended settings:**
- Batch size: 1-2 per GPU
- Gradient accumulation: 16-32
- Learning rate: 1e-5 to 5e-5
- Warmup: 3-5% of total steps

### 3. Full Fine-tuning with ZeRO-3 (`full_sft_zero3.yaml`)
**Best for:** Large models (32B+), 8+ GPUs

- **GPUs**: 8+ GPUs
- **Memory**: ~40GB per GPU
- **Training type**: Full parameter fine-tuning
- **DeepSpeed**: ZeRO-3 (optimizer + gradient + parameter partitioning)
- **Features**:
  - Maximum memory efficiency
  - CPU offloading support
  - Suitable for 32B/72B models

```bash
./examples/circle_rope/train.sh --num_gpus 8 --config examples/circle_rope/full_sft_zero3.yaml
```

**Recommended settings:**
- Batch size: 1 per GPU
- Gradient accumulation: 32-64
- Learning rate: 5e-6 to 2e-5
- May require CPU offloading for 72B models

## üîß Advanced Usage

### Custom Circle-RoPE Configuration

Override Circle-RoPE parameters in your config:

```yaml
model_config_override: |
  {
    "circle_rope": {
      "alpha": 0.7,              # Nonlinear coefficient (0-1, higher = more based on original angles)
      "radius": "auto-1.5",      # Auto-scale with 1.5x multiplier, or fixed value like 10
      "method": "circle",         # Projection method: "circle" or "no_circle"
      "AGE_mode": "strategy_4",   # Layer-wise strategy (see below)
      "move_to_origin": true,     # Center coordinates at origin
      "dff_rate": 0.0             # Differential rate for hybrid approach (0.0 = full circle)
    }
  }
```

### AGE Strategies

Different layer-wise application strategies:

- **`strategy_2`**: First 18 layers use Circle-RoPE, last 18 layers use original RoPE
- **`strategy_3`**: First 18 layers use original RoPE, last 18 layers use Circle-RoPE
- **`strategy_4`** (recommended): Alternate between Circle-RoPE and original RoPE per layer

### Vision Tower Settings

Control vision encoder training:

```yaml
# Freeze vision tower (only train LLM)
freeze_vit: true
freeze_aligner: false

# Or use different learning rates
vit_lr: 1e-6        # Much lower LR for vision tower
learning_rate: 2e-5  # Normal LR for LLM
```

### Memory Optimization Tips

For limited GPU memory:

```yaml
# Reduce batch size and sequence length
per_device_train_batch_size: 1
max_length: 4096

# Increase gradient accumulation
gradient_accumulation_steps: 32

# Enable CPU offloading (ZeRO-3 only)
deepspeed_config_dict: |
  {
    "zero_optimization": {
      "stage": 3,
      "offload_optimizer": {"device": "cpu", "pin_memory": true},
      "offload_param": {"device": "cpu", "pin_memory": true}
    }
  }
```

### Multi-Node Training

For training across multiple machines:

```bash
# On each node, run:
deepspeed \
  --num_gpus=8 \
  --num_nodes=4 \
  --node_rank=$NODE_RANK \
  --master_addr=$MASTER_ADDR \
  --master_port=29500 \
  $(which swift) sft --config examples/circle_rope/full_sft_zero3.yaml
```

## üìñ Configuration Reference

### Circle-RoPE Parameters

| Parameter | Type | Default | Description |
|-----------|------|---------|-------------|
| `alpha` | float | 0.5 | Balance between original angles and uniform distribution (0-1) |
| `radius` | float/str | 10 | Circle radius. Use number or "auto" for automatic scaling |
| `method` | str | "circle" | Projection method: "circle" or "no_circle" |
| `AGE_mode` | str | "strategy_4" | Layer-wise application strategy |
| `move_to_origin` | bool | true | Center coordinates at origin before projection |
| `move_to_positive` | float/str | - | Shift coordinates to positive quadrant |
| `dff_rate` | float | 0.0 | Differential rate for hybrid projection (0.0-1.0) |

### Training Parameters

| Parameter | ZeRO-2 | ZeRO-3 | Description |
|-----------|--------|--------|-------------|
| `per_device_train_batch_size` | 1-2 | 1 | Batch size per GPU |
| `gradient_accumulation_steps` | 16-32 | 32-64 | Steps to accumulate gradients |
| `learning_rate` | 2e-5 | 1e-5 | Main learning rate |
| `vit_lr` | 1e-5 | 5e-6 | Vision tower learning rate |
| `max_length` | 8192 | 8192 | Maximum sequence length |
| `gradient_checkpointing` | true | true | Enable gradient checkpointing |

## üêõ Troubleshooting

### Common Issues

#### 1. `ModuleNotFoundError: No module named 'modular_qwen2_5_vl_circle_rope'`

**Solution:** Ensure circle_rope folder is in your model directory:
```bash
cp -r circle_rope /path/to/your/model/
```

#### 2. `ImportError: cannot import name 'register_circle_rope'`

**Solution:** Check Python path and module structure:
```python
import sys
sys.path.insert(0, '/path/to/your/model')
from circle_rope import register_circle_rope
```

#### 3. Out of Memory (OOM) Errors

**Solutions:**
- Reduce `per_device_train_batch_size` to 1
- Reduce `max_length` to 4096 or lower
- Increase `gradient_accumulation_steps`
- Use ZeRO-3 instead of ZeRO-2
- Enable CPU offloading in ZeRO-3
- Freeze vision tower: `freeze_vit: true`

#### 4. Slow Training Speed

**Solutions:**
- Ensure Flash Attention 2 is installed: `pip install flash-attn --no-build-isolation`
- Check `attn_impl: flash_attn` is set in config
- Verify NVLink/high-bandwidth interconnect for multi-GPU
- Reduce `dataloader_num_workers` if CPU bottleneck
- Use fewer gradient accumulation steps

#### 5. Config Override Not Applied

**Solutions:**
- Verify JSON syntax in `model_config_override`
- Check logs for "Applying model_config_override" message
- Ensure parameter names match model config structure
- Try using `model_type: qwen2_5_vl_circle_rope` instead

### Performance Benchmarks

**Qwen2.5-VL-7B on 4x A100 (40GB)**

| Config | GPUs | Batch Size | Grad Accum | Throughput | Memory/GPU |
|--------|------|------------|------------|------------|------------|
| LoRA | 1 | 2 | 8 | ~2.5 samples/sec | ~24GB |
| LoRA | 4 | 2 | 8 | ~8 samples/sec | ~24GB |
| Full (ZeRO-2) | 4 | 1 | 16 | ~1.2 samples/sec | ~38GB |
| Full (ZeRO-3) | 4 | 1 | 32 | ~0.8 samples/sec | ~32GB |

*Note: Throughput varies based on sequence length and dataset*

## üìö Additional Resources

- **Circle-RoPE Paper**: [Link if available]
- **ms-swift Documentation**: https://github.com/modelscope/swift
- **Qwen2.5-VL**: https://huggingface.co/Qwen/Qwen2.5-VL-7B-Instruct
- **DeepSpeed**: https://www.deepspeed.ai/
- **Flash Attention**: https://github.com/Dao-AILab/flash-attention

## üìù Citation

If you use Circle-RoPE in your research, please cite:

```bibtex
@article{circle-rope,
  title={Circle-RoPE: Circular Rotary Position Embedding for Vision-Language Models},
  author={Your Name},
  journal={arXiv preprint arXiv:XXXX.XXXXX},
  year={2024}
}
```

## ü§ù Contributing

Contributions are welcome! Please feel free to:
- Report bugs or issues
- Suggest improvements
- Share your training results
- Add support for other models

## üìÑ License

This implementation follows the same license as ms-swift and Qwen2.5-VL.
