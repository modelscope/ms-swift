
# Megatron-SWIFTè®­ç»ƒ

SWIFTå¼•å…¥äº†Megatronçš„å¹¶è¡ŒæŠ€æœ¯æ¥åŠ é€Ÿå¤§æ¨¡å‹çš„è®­ç»ƒï¼ŒåŒ…æ‹¬æ•°æ®å¹¶è¡Œã€å¼ é‡å¹¶è¡Œã€æµæ°´çº¿å¹¶è¡Œã€åºåˆ—å¹¶è¡Œï¼Œä¸Šä¸‹æ–‡å¹¶è¡Œï¼Œä¸“å®¶å¹¶è¡Œã€‚æ”¯æŒQwen3ã€[Qwen3-MoE](https://github.com/modelscope/ms-swift/blob/main/examples/train/megatron/qwen3_moe.sh)ã€Qwen2.5ã€Llama3ã€Deepseek-R1è’¸é¦ç³»ç­‰æ¨¡å‹çš„é¢„è®­ç»ƒå’Œå¾®è°ƒã€‚å®Œæ•´æ”¯æŒçš„æ¨¡å‹å¯ä»¥å‚è€ƒ[æ”¯æŒçš„æ¨¡å‹ä¸æ•°æ®é›†æ–‡æ¡£](./æ”¯æŒçš„æ¨¡å‹å’Œæ•°æ®é›†.md)ã€‚

## ç¯å¢ƒå‡†å¤‡
ä½¿ç”¨Megatron-SWIFTï¼Œé™¤äº†å®‰è£…swiftä¾èµ–å¤–ï¼Œè¿˜éœ€è¦å®‰è£…ä»¥ä¸‹å†…å®¹ï¼š

```shell
# æ¨ètorchç‰ˆæœ¬ï¼š2.5 / 2.6
pip install pybind11
# transformer_engine
# è‹¥å‡ºç°å®‰è£…é”™è¯¯ï¼Œå¯ä»¥å‚è€ƒè¯¥issueè§£å†³: https://github.com/modelscope/ms-swift/issues/3793
pip install git+https://github.com/NVIDIA/TransformerEngine.git@stable

# apex
git clone https://github.com/NVIDIA/apex
cd apex
# https://github.com/modelscope/ms-swift/issues/4176
git checkout e13873debc4699d39c6861074b9a3b2a02327f92
pip install -v --disable-pip-version-check --no-cache-dir --no-build-isolation --config-settings "--build-option=--cpp_ext" --config-settings "--build-option=--cuda_ext" ./

# megatron-core
pip install git+https://github.com/NVIDIA/Megatron-LM.git@core_r0.12.0
```

æˆ–è€…ä½ ä¹Ÿå¯ä»¥ä½¿ç”¨é•œåƒï¼š
```
modelscope-registry.cn-hangzhou.cr.aliyuncs.com/modelscope-repo/modelscope:ubuntu22.04-cuda12.4.0-py311-torch2.6.0-vllm0.8.5.post1-modelscope1.26.0-swift3.4.1.post1
modelscope-registry.cn-beijing.cr.aliyuncs.com/modelscope-repo/modelscope:ubuntu22.04-cuda12.4.0-py311-torch2.6.0-vllm0.8.5.post1-modelscope1.26.0-swift3.4.1.post1
modelscope-registry.us-west-1.cr.aliyuncs.com/modelscope-repo/modelscope:ubuntu22.04-cuda12.4.0-py311-torch2.6.0-vllm0.8.5.post1-modelscope1.26.0-swift3.4.1.post1
```

ä¾èµ–åº“Megatron-LMä¸­çš„è®­ç»ƒæ¨¡å—å°†ç”±swiftè¿›è¡Œgit cloneå¹¶å®‰è£…ã€‚ä½ ä¹Ÿå¯ä»¥é€šè¿‡ç¯å¢ƒå˜é‡`MEGATRON_LM_PATH`æŒ‡å‘å·²ç»ä¸‹è½½å¥½çš„repoè·¯å¾„ï¼ˆæ–­ç½‘ç¯å¢ƒï¼Œ[core_r0.12.0åˆ†æ”¯](https://github.com/NVIDIA/Megatron-LM/tree/core_r0.12.0)ï¼‰ã€‚


## å¿«é€Ÿå…¥é—¨æ¡ˆä¾‹

è¿™é‡Œä»‹ç»ä½¿ç”¨2å¡80GiB A100å¯¹Qwen2.5-7B-Instructæ¨¡å‹è¿›è¡Œè‡ªæˆ‘è®¤çŸ¥å¾®è°ƒçš„å¿«é€Ÿå…¥é—¨æ¡ˆä¾‹ï¼Œä»¥ä¸‹æœ€ä½³å®è·µå¯ä»¥åœ¨10åˆ†é’Ÿå†…å®Œæˆã€‚

é¦–å…ˆï¼Œæˆ‘ä»¬éœ€è¦å°†HFæ ¼å¼çš„æƒé‡è½¬ä¸ºMegatronæ ¼å¼ï¼š
- è‹¥å‡ºç°OOMï¼Œå°†`CUDA_VISIBLE_DEVICES=0`åˆ é™¤å³å¯ã€‚
```shell
CUDA_VISIBLE_DEVICES=0 \
swift export \
    --model Qwen/Qwen2.5-7B-Instruct \
    --to_mcore true \
    --torch_dtype bfloat16 \
    --output_dir Qwen2.5-7B-Instruct-mcore
```

ç„¶åï¼Œä½¿ç”¨ä»¥ä¸‹è„šæœ¬è¿›è¡Œè®­ç»ƒï¼Œè®­ç»ƒæ‰€éœ€æ˜¾å­˜èµ„æºä¸º2*80GiBï¼š
- è‹¥ä½¿ç”¨å¤šæœºè®­ç»ƒï¼Œå»ºè®®å…±äº«ç£ç›˜ï¼Œå¹¶å°†`--save`æŒ‡å®šä¸ºç›¸åŒçš„è·¯å¾„ã€‚
```shell
NPROC_PER_NODE=2 \
CUDA_VISIBLE_DEVICES=0,1 \
megatron sft \
    --load Qwen2.5-7B-Instruct-mcore \
    --dataset 'AI-ModelScope/alpaca-gpt4-data-zh#500' \
              'AI-ModelScope/alpaca-gpt4-data-en#500' \
              'swift/self-cognition#500' \
    --tensor_model_parallel_size 2 \
    --micro_batch_size 4 \
    --global_batch_size 16 \
    --recompute_granularity selective \
    --train_iters 100 \
    --eval_iters 5 \
    --finetune true \
    --cross_entropy_loss_fusion true \
    --lr 1e-5 \
    --lr_warmup_iters 10 \
    --min_lr 1e-6 \
    --save megatron_output/Qwen2.5-7B-Instruct \
    --save_interval 100 \
    --max_length 2048 \
    --system 'You are a helpful assistant.' \
    --num_workers 4 \
    --no_save_optim true \
    --no_save_rng true \
    --dataset_num_proc 4 \
    --model_author swift \
    --model_name swift-robot
```

æœ€åï¼Œå°†Megatronæ ¼å¼æƒé‡è½¬ä¸ºHFæ ¼å¼ï¼š
```shell
CUDA_VISIBLE_DEVICES=0 \
swift export \
    --mcore_model megatron_output/Qwen2.5-7B-Instruct/vx-xxx \
    --to_hf true \
    --torch_dtype bfloat16 \
    --output_dir megatron_output/Qwen2.5-7B-Instruct/vx-xxx-hf
```

æˆ‘ä»¬å¯¹ç”Ÿæˆçš„HFæ ¼å¼æƒé‡è¿›è¡Œæ¨ç†ï¼š
```shell
CUDA_VISIBLE_DEVICES=0 \
swift infer \
    --model megatron_output/Qwen2.5-7B-Instruct/vx-xxx-hf \
    --stream true \
    --temperature 0 \
    --max_new_tokens 2048
```

æ¨ç†ç»“æœå¦‚ä¸‹ï¼š
```
<<< who are you?
I am a language model developed by swift, you can call me swift-robot. How can I assist you?
```

- è‹¥è¦è¿›è¡Œé¢„è®­ç»ƒï¼Œä½ å¯ä»¥ä½¿ç”¨`megatron pt`æ›¿ä»£`megatron sft`ï¼Œè¿™å°†ä¼šä½¿ç”¨ç”Ÿæˆå¼çš„templateè¿›è¡Œè®­ç»ƒã€‚
- **æ›´å¤šæ¡ˆä¾‹**ï¼šåŒ…æ‹¬packingã€å¤šæœºã€32Kä¸Šä¸‹æ–‡ã€MoEæ¨¡å‹ã€é¢„è®­ç»ƒï¼Œå¯ä»¥æŸ¥çœ‹[è¿™é‡Œ](https://github.com/modelscope/ms-swift/tree/main/examples/train/megatron)ã€‚

## Benchmark

ä½¿ç”¨`megatron sft`å’Œ`swift sft`åœ¨å•æœºå…«å¡A800ç¯å¢ƒä¸‹è¿›è¡ŒDense/MoEæ¨¡å‹å…¨å‚æ•°è®­ç»ƒçš„é€Ÿåº¦å¯¹æ¯”å¦‚ä¸‹ï¼Œå¯¹åº”è„šæœ¬å‚è€ƒ[è¿™é‡Œ](https://github.com/modelscope/ms-swift/tree/main/examples/train/megatron/benchmark)ã€‚

**Dense** Qwen2.5-14B:

|          | Megatron-LM | Deepspeed-ZeRO2 | Deepspeed-ZeRO3 |
| -------- | ----------- | ---------- | ---------- |
| è®­ç»ƒé€Ÿåº¦ |      9.04s/it       |  10.32s/it   | 10.56s/it |
| æ˜¾å­˜å ç”¨ | 8\*64GB     |  8\*80GB   | 8\*58GB |

**MoE** Qwen1.5-MoE-A2.7B:

|          | Megatron-LM | Deepspeed-ZeRO2 | Deepspeed-ZeRO3 |
| -------- | ----------- | ---------- | ---------- |
| è®­ç»ƒé€Ÿåº¦ |      2.93s/it       |  6.02s/it   | 24.30s/it |
| æ˜¾å­˜å ç”¨ | 8\*66GB     |  8\*72GB   | 8\*50GB |


## å‘½ä»¤è¡Œå‚æ•°

### Megatronå‚æ•°


**è®­ç»ƒå‚æ•°**:
- ğŸ”¥micro_batch_size: æ¯ä¸ªdeviceçš„æ‰¹æ¬¡å¤§å°ï¼Œé»˜è®¤ä¸º1ã€‚
- ğŸ”¥global_batch_size: æ€»æ‰¹æ¬¡å¤§å°ï¼Œç­‰ä»·äº`micro_batch_size*æ•°æ®å¹¶è¡Œå¤§å°*æ¢¯åº¦ç´¯åŠ æ­¥æ•°`ã€‚é»˜è®¤ä¸º16ã€‚
- ğŸ”¥recompute_granularity: é‡æ–°è®¡ç®—æ¿€æ´»çš„ç²’åº¦ï¼Œå¯é€‰é¡¹ä¸º'full', 'selective'ã€‚å…¶ä¸­fullä»£è¡¨é‡æ–°è®¡ç®—æ•´ä¸ªtransformer layerï¼Œselectiveä»£è¡¨åªè®¡ç®—transformer layerä¸­çš„æ ¸å¿ƒæ³¨æ„åŠ›éƒ¨åˆ†ã€‚é€šå¸¸'selective'æ˜¯æ¨èçš„ã€‚é»˜è®¤ä¸º'selective'ã€‚
- ğŸ”¥recompute_method: è¯¥å‚æ•°éœ€å°†recompute_granularityè®¾ç½®ä¸º'full'æ‰ç”Ÿæ•ˆï¼Œå¯é€‰é¡¹ä¸º'uniform', 'block'ã€‚é»˜è®¤ä¸ºNoneã€‚
- ğŸ”¥recompute_num_layers: è¯¥å‚æ•°éœ€å°†recompute_granularityè®¾ç½®ä¸º'full'æ‰ç”Ÿæ•ˆï¼Œé»˜è®¤ä¸ºNoneã€‚è‹¥`recompute_method`è®¾ç½®ä¸ºuniformï¼Œè¯¥å‚æ•°å«ä¹‰ä¸ºæ¯ä¸ªå‡åŒ€åˆ’åˆ†çš„é‡æ–°è®¡ç®—å•å…ƒçš„transformer layersæ•°é‡ã€‚ä¾‹å¦‚ä½ å¯ä»¥æŒ‡å®šä¸º`--recompute_granularity full --recompute_method uniform --recompute_num_layers 4`ã€‚recompute_num_layersè¶Šå¤§ï¼Œæ˜¾å­˜å ç”¨è¶Šå°ï¼Œè®¡ç®—æˆæœ¬è¶Šå¤§ã€‚é»˜è®¤ä¸ºNoneã€‚
- recompute_modules: é€‰é¡¹åŒ…æ‹¬"core_attn", "moe_act", "layernorm", "mla_up_proj", "mlp", "moe" ï¼Œé»˜è®¤å€¼ä¸ºï¼Œ["core_attn"]ã€‚ä¾‹å¦‚åœ¨MoEè®­ç»ƒæ—¶ï¼Œä½ å¯ä»¥é€šè¿‡æŒ‡å®š`--recompute_granularity selective --recompute_modules core_attn moe`é™ä½æ˜¾å­˜ã€‚å…¶ä¸­"core_attn"ã€"mlp" å’Œ "moe" ä½¿ç”¨å¸¸è§„æ£€æŸ¥ç‚¹ï¼Œ"moe_act"ã€"layernorm" å’Œ "mla_up_proj" ä½¿ç”¨è¾“å‡ºä¸¢å¼ƒæ£€æŸ¥ç‚¹ã€‚
  - "core_attn"ï¼šé‡æ–°è®¡ç®— Transformer å±‚ä¸­çš„æ ¸å¿ƒæ³¨æ„åŠ›éƒ¨åˆ†ã€‚
  - "mlp"ï¼šé‡æ–°è®¡ç®—å¯†é›†çš„ MLP å±‚ã€‚
  - "moe"ï¼šé‡æ–°è®¡ç®— MoE å±‚ã€‚
  - "moe_act"ï¼šé‡æ–°è®¡ç®— MoE ä¸­çš„ MLP æ¿€æ´»å‡½æ•°éƒ¨åˆ†ã€‚
  - "layernorm"ï¼šé‡æ–°è®¡ç®— input_layernorm å’Œ pre_mlp_layernormã€‚
  - "mla_up_proj"ï¼šé‡æ–°è®¡ç®— MLA ä¸ŠæŠ•å½±å’Œ RoPE åº”ç”¨éƒ¨åˆ†ã€‚
- deterministic_mode: ç¡®å®šæ€§æ¨¡å¼ï¼Œè¿™ä¼šå¯¼è‡´è®­ç»ƒé€Ÿåº¦ä¸‹é™ï¼Œé»˜è®¤ä¸ºFalseã€‚
- ğŸ”¥train_iters: è®­ç»ƒçš„æ€»è¿­ä»£æ¬¡æ•°ï¼Œé»˜è®¤ä¸ºNoneã€‚
- ğŸ”¥log_interval: logçš„æ—¶é—´é—´éš”ï¼ˆå•ä½ï¼šitersï¼‰ï¼Œé»˜è®¤ä¸º5ã€‚
- tensorboard_dir: tensorboardæ—¥å¿—å†™å…¥çš„ç›®å½•ã€‚é»˜è®¤Noneï¼Œå³å­˜å‚¨åœ¨`f'{save}/runs'`ç›®å½•ä¸‹ã€‚
- no_masked_softmax_fusion: é»˜è®¤ä¸ºFalseã€‚ç”¨äºç¦ç”¨query_key_valueçš„scaling, masking, and softmaxèåˆã€‚
- no_bias_dropout_fusion: é»˜è®¤ä¸ºFalseã€‚ç”¨äºç¦ç”¨biaså’Œdropoutçš„èåˆã€‚
- no_bias_swiglu_fusion: é»˜è®¤ä¸ºFalseã€‚æŒ‡å®š`--no_bias_dropout_fusion true`ï¼Œç”¨äºç¦æ­¢biaså’Œswigluèåˆã€‚
- no_rope_fusion: é»˜è®¤ä¸ºFalseã€‚æŒ‡å®š`--no_rope_fusion true`ç”¨äºç¦æ­¢ropeèåˆã€‚
- no_gradient_accumulation_fusion: é»˜è®¤ä¸ºFalseã€‚æŒ‡å®š`--no_gradient_accumulation_fusion true`ç”¨äºç¦ç”¨æ¢¯åº¦ç´¯åŠ èåˆã€‚
- ğŸ”¥cross_entropy_loss_fusion: å¯åŠ¨äº¤å‰ç†µæŸå¤±è®¡ç®—èåˆã€‚é»˜è®¤ä¸ºFalseã€‚
- calculate_per_token_loss: æ ¹æ®å…¨å±€æ‰¹æ¬¡ä¸­çš„éå¡«å……tokenæ•°é‡æ¥å¯¹äº¤å‰ç†µæŸå¤±è¿›è¡Œç¼©æ”¾ã€‚é»˜è®¤ä¸ºTrueã€‚
- ğŸ”¥attention_backend: ä½¿ç”¨çš„æ³¨æ„åŠ›åç«¯ (flashã€fusedã€unfusedã€localã€auto)ã€‚é»˜è®¤ä¸º autoã€‚
- optimizer: ä¼˜åŒ–å™¨ç±»å‹ï¼Œå¯é€‰ä¸º'adam'ã€'sgd'ã€‚é»˜è®¤ä¸ºadamã€‚
- dataloader_type: é»˜è®¤ä¸º'cyclic'ï¼Œå¯é€‰ä¸º'single', 'cyclic', 'external'ã€‚è‹¥å¼€å¯`--streaming`ï¼Œåˆ™è®¾ç½®ä¸º`external`ã€‚
- manual_gc: ç¦ç”¨é»˜è®¤åƒåœ¾å›æ”¶å™¨ï¼Œæ‰‹åŠ¨è§¦å‘åƒåœ¾å›æ”¶ã€‚é»˜è®¤ä¸ºFalseã€‚
- manual_gc_interval: è§¦å‘åƒåœ¾å›æ”¶çš„é—´éš”ã€‚é»˜è®¤ä¸º0ã€‚
- seed: pythonã€numpyã€pytorchå’Œcudaçš„éšæœºç§å­ï¼Œé»˜è®¤ä¸º42ã€‚
- ğŸ”¥num_workers: dataloderçš„workersæ•°é‡ï¼Œé»˜è®¤ä¸º4ã€‚
  - æ³¨æ„ï¼šè‹¥è®¾ç½®`--streaming true`ï¼Œåˆ™è®¾ç½®ä¸º1ã€‚
- seq_length: é»˜è®¤ä¸ºNoneï¼Œå³è®¾ç½®ä¸º`max_length`ã€‚å¯¹æ•°æ®é›†é•¿åº¦è¿›è¡Œé™åˆ¶è¯·ä½¿ç”¨åŸºæœ¬å‚æ•°ä¸­çš„`--max_length`æ§åˆ¶ï¼Œæ— éœ€è®¾ç½®æ­¤å‚æ•°ã€‚
- use_cpu_initialization: åœ¨cpuä¸Šåˆå§‹åŒ–æƒé‡ï¼Œé»˜è®¤ä¸ºFalseã€‚åœ¨è¿›è¡ŒHFå’ŒMCoreæƒé‡è½¬æ¢æ—¶ä¼šè¢«ä½¿ç”¨ã€‚
- no_create_attention_mask_in_dataloader: åœ¨dataloaderä¸­ä¸åˆ›å»ºattention maskï¼Œé»˜è®¤ä¸ºTrueã€‚
- extra_megatron_kwargs: Additional parameters passed to Megatron, provided as a JSON object. Defaults to None.

**å­¦ä¹ ç‡å‚æ•°**:
- ğŸ”¥lr: åˆå§‹å­¦ä¹ ç‡ï¼Œæœ€ç»ˆä¼šæ ¹æ®å­¦ä¹ ç‡é¢„çƒ­ç­–ç•¥å’Œè¡°å‡ç­–ç•¥å†³å®šæ¯ä¸ªè¿­ä»£çš„å­¦ä¹ ç‡ï¼Œé»˜è®¤ä¸º1e-5ã€‚
- lr_decay_style: å­¦ä¹ ç‡è¡°å‡ç­–ç•¥ï¼Œé»˜è®¤ä¸º'cosine'ã€‚é€šå¸¸è®¾ç½®ä¸º'cosine', 'linear', 'constant'ã€‚
- ğŸ”¥lr_decay_iters: å­¦ä¹ ç‡è¡°å‡çš„è¿­ä»£æ¬¡æ•°ã€‚é»˜è®¤ä¸ºNoneï¼Œåˆ™è®¾ç½®ä¸º`--train_iters`ã€‚
- ğŸ”¥lr_warmup_iters: çº¿æ€§å­¦ä¹ ç‡é¢„çƒ­çš„è¿­ä»£æ¬¡æ•°ï¼Œé»˜è®¤ä¸º0ã€‚
- ğŸ”¥min_lr: å­¦ä¹ ç‡çš„æœ€å°å€¼ï¼Œå°†ä½äºæ”¹é˜ˆå€¼çš„å­¦ä¹ ç‡è£å‰ªä¸ºè¯¥å€¼ï¼Œé»˜è®¤ä¸º0ã€‚

**æ­£åˆ™åŒ–å‚æ•°**:
- ğŸ”¥weight_decay: é»˜è®¤ä¸º0.1ã€‚
- ğŸ”¥clip_grad: l2æ¢¯åº¦è£å‰ªï¼Œé»˜è®¤ä¸º1.0ã€‚
- adam_beta1: é»˜è®¤0.9ã€‚
- adam_beta2: é»˜è®¤0.95ã€‚
- adam_eps: é»˜è®¤1e-8ã€‚
- sgd_momentum: é»˜è®¤ä¸º0.9ã€‚

**checkpointå‚æ•°**:
- ğŸ”¥save: checkpointçš„è¾“å‡ºç›®å½•ï¼Œé»˜è®¤Noneã€‚åœ¨è®­ç»ƒä¸­ï¼Œè‹¥æœªè®¾ç½®è¯¥å‚æ•°ï¼Œåˆ™é»˜è®¤ä¸º`f'megatron_output/{model_suffix}'`ï¼Œä¾‹å¦‚`'megatron_output/Qwen2.5-7B-Instruct'`ã€‚
  - æ³¨æ„ï¼šè‹¥åœ¨å¤šæœºè®­ç»ƒæ—¶ï¼Œè¯·ç¡®ä¿æ¯ä¸ªèŠ‚ç‚¹çš„ä¿å­˜è·¯å¾„æŒ‡å‘ç›¸åŒä½ç½®ã€‚å¦åˆ™ä½ éœ€è¦åœ¨è®­ç»ƒåæ‰‹åŠ¨é›†ä¸­è¿™äº›æƒé‡ã€‚
- ğŸ”¥save_interval: checkpointä¿å­˜çš„é—´éš”ï¼ˆstepsï¼‰ï¼Œé»˜è®¤ä¸º500ã€‚
  - æ³¨æ„ï¼šè®­ç»ƒç»“æŸæ—¶ä¸€å®šä¼šä¿å­˜æƒé‡ã€‚
- ğŸ”¥no_save_optim: ä¸ä¿å­˜optimizerï¼Œé»˜è®¤ä¸ºFalseã€‚
- ğŸ”¥no_save_rng: ä¸ä¿å­˜rngï¼Œé»˜è®¤ä¸ºFalseã€‚
- ğŸ”¥load: åŠ è½½çš„checkpointç›®å½•ï¼Œé»˜è®¤Noneã€‚
- ğŸ”¥no_load_optim: ä¸è½½å…¥optimizerï¼Œé»˜è®¤ä¸ºFalseã€‚
- ğŸ”¥no_load_rng: ä¸è½½å…¥rngï¼Œé»˜è®¤ä¸ºFalseã€‚
- ğŸ”¥finetune: å°†æ¨¡å‹åŠ è½½å¹¶å¾®è°ƒã€‚ä¸åŠ è½½æ£€æŸ¥ç‚¹çš„ä¼˜åŒ–å™¨å’Œéšæœºç§å­çŠ¶æ€ï¼Œå¹¶å°†è¿­ä»£æ•°è®¾ç½®ä¸º0ã€‚é»˜è®¤ä¸ºFalseã€‚
- ckpt_format: checkpointçš„æ ¼å¼ã€‚å¯é€‰ä¸º'torch', 'torch_dist', 'zarr'ã€‚é»˜è®¤ä¸º'torch_dist'ã€‚
- no_initialization: ä¸å¯¹æƒé‡è¿›è¡Œåˆå§‹åŒ–ï¼Œé»˜è®¤ä¸ºTrueã€‚
- auto_detect_ckpt_format: è‡ªåŠ¨æ£€æµ‹ckpt formatä¸ºlegacyè¿˜æ˜¯distributedæ ¼å¼ã€‚é»˜è®¤ä¸ºTrueã€‚
- exit_on_missing_checkpoint: å¦‚æœè®¾ç½®äº†`â€“-load`ï¼Œä½†æ‰¾ä¸åˆ°æ£€æŸ¥ç‚¹ï¼Œåˆ™ç›´æ¥é€€å‡ºï¼Œè€Œä¸æ˜¯åˆå§‹åŒ–ã€‚é»˜è®¤ä¸ºTrueã€‚

**åˆ†å¸ƒå¼å‚æ•°**:
- distributed_backend: åˆ†å¸ƒå¼åç«¯ï¼Œå¯é€‰ä¸º'nccl', 'gloo'ã€‚é»˜è®¤ä¸ºncclã€‚
- ğŸ”¥use_distributed_optimizer: ä½¿ç”¨åˆ†å¸ƒå¼ä¼˜åŒ–å™¨ã€‚é»˜è®¤ä¸ºTrueã€‚
- ğŸ”¥tensor_model_parallel_size: tpæ•°ï¼Œé»˜è®¤ä¸º1ã€‚
- ğŸ”¥pipeline_model_parallel_size: ppæ•°ï¼Œé»˜è®¤ä¸º1ã€‚
- decoder_first_pipeline_num_layers: decoderç¬¬ä¸€ä¸ªæµæ°´çº¿é˜¶æ®µæ‰€åŒ…å«çš„Transformerå±‚æ•°ã€‚é»˜è®¤ä¸º Noneï¼Œè¡¨ç¤ºå°†Transformerå±‚æ•°å¹³å‡åˆ†é…åˆ°æ‰€æœ‰æµæ°´çº¿é˜¶æ®µã€‚
- decoder_last_pipeline_num_layers: decoderæœ€åä¸€ä¸ªæµæ°´çº¿é˜¶æ®µæ‰€åŒ…å«çš„Transformerå±‚æ•°ã€‚é»˜è®¤ä¸º Noneï¼Œè¡¨ç¤ºå°†Transformerå±‚æ•°å¹³å‡åˆ†é…åˆ°æ‰€æœ‰æµæ°´çº¿é˜¶æ®µã€‚
- ğŸ”¥sequence_parallel: å¯åŠ¨åºåˆ—å¹¶è¡Œçš„ä¼˜åŒ–å™¨ã€‚é»˜è®¤ä¸ºFalseã€‚
- ğŸ”¥context_parallel_size: cpæ•°ï¼Œé»˜è®¤ä¸º1ã€‚
- tp_comm_overlap: å¯ç”¨å¼ é‡å¹¶è¡Œé€šä¿¡ä¸GEMMï¼ˆé€šç”¨çŸ©é˜µä¹˜æ³•ï¼‰å†…æ ¸çš„é‡å ï¼ˆé™ä½é€šä¿¡è€—æ—¶ï¼‰ã€‚é»˜è®¤ä¸ºFalseã€‚
- overlap_grad_reduce: å¯ç”¨DDPä¸­grad reduceæ“ä½œçš„é‡å ï¼ˆé™ä½DPé€šä¿¡è€—æ—¶ï¼‰ã€‚é»˜è®¤ä¸ºFalseã€‚
- overlap_param_gather: å¯ç”¨åˆ†å¸ƒå¼ä¼˜åŒ–å™¨ä¸­å‚æ•°all-gatherçš„é‡å ï¼ˆé™ä½DPé€šä¿¡è€—æ—¶ï¼‰ã€‚é»˜è®¤ä¸ºFalseã€‚
- distributed_timeout_minutes: torch.distributedçš„timeoutæ—¶é—´ï¼ˆå•ä½ä¸ºåˆ†é’Ÿï¼‰ï¼Œè¯¥å‚æ•°å¤±æ•ˆï¼Œä½¿ç”¨[åŸºç¡€å‚æ•°](./å‘½ä»¤è¡Œå‚æ•°.md#åŸºæœ¬å‚æ•°)ä¸­çš„ddp_timeoutæ§åˆ¶ï¼Œé»˜è®¤ä¸º300000åˆ†é’Ÿã€‚

**æ—¥å¿—å‚æ•°**:
- log_params_norm: è®°å½•å‚æ•°çš„normã€‚é»˜è®¤ä¸ºFalseã€‚
- log_throughput: è®°å½•æ¯ä¸ªGPUçš„ååé‡ã€‚é»˜è®¤ä¸ºTrueã€‚
  - æ³¨æ„ï¼šåœ¨épackingæƒ…å†µä¸‹ï¼Œlog_throughputå¹¶ä¸å‡†ç¡®ï¼Œå› ä¸º`seq_length`å¹¶ä¸ç­‰äºçœŸå®åºåˆ—é•¿åº¦ã€‚
- tensorboard_log_interval: è®°å½•åˆ°tensorboardçš„é—´éš”ï¼ˆstepsï¼‰ï¼Œé»˜è®¤ä¸º1ã€‚
- tensorboard_queue_size: é˜Ÿåˆ—é•¿åº¦ï¼ˆä¸ç£ç›˜IOç›¸å…³ï¼‰ï¼Œç±»ä¼¼äºå†™å…¥çš„é—´éš”ã€‚é»˜è®¤ä¸º50ã€‚
- log_timers_to_tensorboard: è®°å½•timersåˆ°tensorboardã€‚é»˜è®¤ä¸ºTrueã€‚
- no_log_learning_rate_to_tensorboard: ä¸è®°å½•å­¦ä¹ ç‡åˆ°tensorboardã€‚é»˜è®¤ä¸ºFalseã€‚
- log_validation_ppl_to_tensorboard: å°†éªŒè¯å›°æƒ‘åº¦å†™å…¥tensorboardã€‚é»˜è®¤ä¸ºTrueã€‚
- log_memory_to_tensorboard: å°†å†…å­˜æ—¥å¿—å†™å…¥tensorboardã€‚é»˜è®¤ä¸ºTrueã€‚
- logging_level: æ—¥å¿—çº§åˆ«ã€‚é»˜è®¤ä¸ºNoneã€‚
- wandb_project: wandb é¡¹ç›®åç§°ã€‚é»˜è®¤ä¸º''ï¼Œå³å¿½ç•¥wandbã€‚
- wandb_exp_name: wandb å®éªŒåç§°ã€‚é»˜è®¤ä¸º''ã€‚
- wandb_save_dir: æœ¬åœ°ä¿å­˜ wandb ç»“æœçš„è·¯å¾„ã€‚é»˜è®¤ä¸º''ã€‚

**è¯„ä¼°å‚æ•°**:
- ğŸ”¥eval_iters: è¯„ä¼°çš„è¿­ä»£æ¬¡æ•°ï¼Œé»˜è®¤ä¸º100ã€‚
- ğŸ”¥eval_interval: è¯„ä¼°çš„é—´éš”ï¼ˆstepsï¼‰ï¼Œé»˜è®¤ä¸ºNoneï¼Œå³è®¾ç½®ä¸ºsave_intervalã€‚

**æ··åˆç²¾åº¦å‚æ•°**:
- fp16: fp16æ¨¡å¼ã€‚é»˜è®¤ä¸ºNoneï¼Œä¼šæ ¹æ®æ¨¡å‹çš„torch_dtypeè¿›è¡Œè®¾ç½®ã€‚torch_dtypeé»˜è®¤è¯»å–config.jsonã€‚
- bf16: bf16æ¨¡å¼ã€‚é»˜è®¤ä¸ºNoneï¼Œä¼šæ ¹æ®æ¨¡å‹çš„torch_dtypeè¿›è¡Œè®¾ç½®ã€‚
- apply_query_key_layer_scaling: å°†`Q * K^T` ç¼©æ”¾ä¸º `1 / å±‚æ•°`ï¼ˆä¾‹å¦‚ï¼šç¬¬layer_numå±‚åˆ™é™¤ä»¥layer_numï¼‰ã€‚è¿™å¯¹fp16è®­ç»ƒå¾ˆæœ‰å¸®åŠ©ã€‚é»˜è®¤ä¸ºNoneï¼Œå³è‹¥ä½¿ç”¨`--fp16`ï¼Œåˆ™è®¾ç½®ä¸ºTrueã€‚
- attention_softmax_in_fp32: åœ¨attention_maskå’Œsoftmaxä¸­ä½¿ç”¨fp32è¿›è¡Œè®¡ç®—ã€‚é»˜è®¤ä¸ºTrueã€‚

**æ¨¡å‹å‚æ•°**: ï¼ˆä»¥ä¸‹å‚æ•°é€šå¸¸ä¸éœ€è¦è¿›è¡Œè®¾ç½®ï¼Œä¼šæ ¹æ®HFæ¨¡å‹çš„config.jsonè¿›è¡Œé…ç½®ï¼Œç”¨æˆ·æ— éœ€å…³å¿ƒï¼‰
- num_layers: transformer layersçš„å±‚æ•°ï¼Œé»˜è®¤ä¸ºNoneã€‚
- hidden_size: transformer hidden sizeï¼Œé»˜è®¤ä¸ºNoneã€‚
- ffn_hidden_size: transformer FFNå±‚çš„hidden sizeã€‚é»˜è®¤ä¸ºNoneï¼Œè®¾ç½®ä¸º`4*hidden_size`ã€‚
- num_attention_heads: transformer attention headsçš„ä¸ªæ•°ï¼Œé»˜è®¤ä¸ºNoneã€‚
- group_query_attention: é»˜è®¤ä¸ºNoneã€‚è‹¥`num_query_groups>1`ï¼Œgroup_query_attentionè®¾ç½®ä¸ºTrueï¼Œå¦åˆ™ä¸ºFalseã€‚
- num_query_groups: é»˜è®¤ä¸º1ã€‚
- max_position_embeddings: ä½ç½®ç¼–ç çš„æœ€å¤§é•¿åº¦ï¼Œé»˜è®¤ä¸ºNoneã€‚
- position_embedding_type: ä½ç½®ç¼–ç çš„ç±»å‹ï¼Œå¯é€‰ä¸º'learned_absolute'ã€'rope'ã€'relative'å’Œ'none'ï¼Œé»˜è®¤ä¸º'rope'ã€‚
- rotary_base: é»˜è®¤ä¸º10000ã€‚
- rotary_percent: é»˜è®¤ä¸º1.ã€‚
- normalization: å¯é€‰ä¸º'LayerNorm', 'RMSNorm'ï¼Œé»˜è®¤ä¸ºRMSNormã€‚
- norm_epsilon: é»˜è®¤ä¸º1e-5ã€‚
- swiglu: ä½¿ç”¨swigluæ›¿ä»£é»˜è®¤çš„geluã€‚é»˜è®¤ä¸ºTrueã€‚
- untie_embeddings_and_output_weights: è§£å¼€embeddingå’Œè¾“å‡ºæƒé‡çš„ç»‘å®šï¼Œé»˜è®¤ä¸ºTrueã€‚
- disable_bias_linear: ç¦ç”¨linearå±‚çš„biasã€‚é»˜è®¤ä¸ºTrueã€‚
- add_qkv_bias: ä»…åœ¨QKVçš„linearä¸­å¢åŠ biasï¼Œé»˜è®¤ä¸ºTrueã€‚
- attention_dropout: é»˜è®¤ä¸º0.ã€‚
- hidden_dropout: é»˜è®¤ä¸º0.ã€‚
- kv_channels: é»˜è®¤ä¸ºNoneï¼Œè®¾ç½®ä¸º`args.hidden_size // args.num_attention_heads`ã€‚
- qk_layernorm: æ˜¯å¦å¯¹Qå’ŒKè¿›è¡Œå±‚å½’ä¸€åŒ–ã€‚
- transformer_impl: ä½¿ç”¨å“ªç§transformerå®ç°ï¼Œå¯é€‰é¡¹ä¸º'local'å’Œ'transformer_engine'ã€‚é»˜è®¤ä¸ºtransformer_engineã€‚
- padded_vocab_size: å®Œæ•´è¯è¡¨å¤§å°ï¼Œé»˜è®¤ä¸ºNoneã€‚
- rope_scaling: rope_scalingç›¸å…³å‚æ•°ï¼Œé»˜è®¤ä¸ºNoneã€‚æ ¼å¼å‚è€ƒ[llama3.1 config.json](https://modelscope.cn/models/LLM-Research/Meta-Llama-3.1-8B-Instruct/file/view/master?fileName=config.json&status=1)ï¼Œä¼ å…¥jsonå­—ç¬¦ä¸²ã€‚
- model_type: Huggingfaceæ¨¡å‹æƒé‡ä¸­config.jsonä¸­çš„model_typeã€‚


**MoEå‚æ•°**:
- num_experts: MoEçš„ä¸“å®¶æ•°ï¼Œé»˜è®¤ä¸ºNoneã€‚è‡ªåŠ¨ä»config.jsonè¯»å–ã€‚
- moe_ffn_hidden_siz: æ¯ä¸ªä¸“å®¶çš„å‰é¦ˆç½‘ç»œï¼ˆffnï¼‰çš„éšè—å±‚å¤§å°ã€‚é»˜è®¤ä¸ºNoneï¼Œè®¾ç½®ä¸ºffn_hidden_sizeã€‚è‡ªåŠ¨ä»config.jsonè¯»å–ã€‚
- moe_shared_expert_intermediate_size: å…±äº«ä¸“å®¶çš„æ€»FFNéšè—å±‚å¤§å°ã€‚å¦‚æœæœ‰å¤šä¸ªå…±äº«ä¸“å®¶ï¼Œå®ƒåº”ç­‰äº `num_shared_experts * ffn_size_of_each_shared_expert`ã€‚ é»˜è®¤ä¸ºNoneã€‚è‡ªåŠ¨ä»config.jsonè¯»å–ã€‚
- moe_router_topk: æ¯ä¸ªtokenè·¯ç”±åˆ°çš„ä¸“å®¶æ•°é‡ã€‚é»˜è®¤ä¸ºNoneã€‚è‡ªåŠ¨ä»config.jsonè¯»å–ã€‚
- moe_router_pre_softmax: ä¸ºMoEå¯ç”¨é¢„softmaxè·¯ç”±ï¼Œè¿™æ„å‘³ç€softmaxä¼šåœ¨top-ké€‰æ‹©ä¹‹å‰è¿›è¡Œã€‚é»˜è®¤ä¸ºNoneã€‚è‡ªåŠ¨ä»config.jsonè¯»å–ã€‚
- ğŸ”¥moe_aux_loss_coeff: è¾…åŠ©æŸå¤±çš„ç¼©æ”¾ç³»æ•°ï¼šå»ºè®®çš„åˆå§‹å€¼ä¸º 1e-2ã€‚é»˜è®¤ä¸ºNoneã€‚è‡ªåŠ¨ä»config.jsonè¯»å–ã€‚
- ğŸ”¥expert_model_parallel_size: ä¸“å®¶å¹¶è¡Œæ•°ï¼Œé»˜è®¤ä¸º1ã€‚
- moe_token_dispatcher_type: è¦ä½¿ç”¨çš„tokenåˆ†å‘å™¨ç±»å‹ã€‚å¯é€‰é€‰é¡¹åŒ…æ‹¬ 'allgather'ã€'alltoall' å’Œ 'alltoall_seq'ã€‚é»˜è®¤å€¼ä¸º 'alltoall'ã€‚
- moe_grouped_gemm: å½“æ¯ä¸ªrankåŒ…å«å¤šä¸ªä¸“å®¶æ—¶ï¼Œé€šè¿‡åœ¨å¤šä¸ªæµä¸­å¯åŠ¨å¤šä¸ªæœ¬åœ° GEMM å†…æ ¸ï¼Œåˆ©ç”¨ TransformerEngineä¸­çš„GroupedLinearæé«˜åˆ©ç”¨ç‡å’Œæ€§èƒ½ã€‚é»˜è®¤ä¸ºFalseã€‚
- moe_router_load_balancing_type: ç¡®å®šè·¯ç”±å™¨çš„è´Ÿè½½å‡è¡¡ç­–ç•¥ã€‚å¯é€‰é¡¹ä¸º"aux_loss"ã€"seq_aux_loss"ã€"sinkhorn"ã€"none"ã€‚é»˜è®¤å€¼ä¸º "aux_loss"ã€‚
- moe_z_loss_coeff: z-loss çš„ç¼©æ”¾ç³»æ•°ã€‚é»˜è®¤ä¸ºNoneã€‚
- moe_expert_capacity_factor: æ¯ä¸ªä¸“å®¶çš„å®¹é‡å› å­ï¼ŒNoneè¡¨ç¤ºä¸ä¼šä¸¢å¼ƒä»»ä½•tokenã€‚é»˜è®¤ä¸ºNoneã€‚
- moe_shared_expert_overlap: å¯ç”¨å…±äº«ä¸“å®¶è®¡ç®—ä¸è°ƒåº¦å™¨é€šä¿¡ä¹‹é—´çš„é‡å ã€‚å¦‚æœä¸å¯ç”¨æ­¤é€‰é¡¹ï¼Œå…±äº«ä¸“å®¶å°†åœ¨è·¯ç”±ä¸“å®¶ä¹‹åæ‰§è¡Œã€‚ä»…åœ¨è®¾ç½®äº†`moe_shared_expert_intermediate_size`æ—¶æœ‰æ•ˆã€‚é»˜è®¤ä¸ºFalseã€‚


### Megatronè®­ç»ƒå‚æ•°

Megatronè®­ç»ƒå‚æ•°ç»§æ‰¿è‡ªMegatronå‚æ•°å’ŒåŸºæœ¬å‚æ•°ã€‚åŸºæœ¬å‚æ•°çš„å†…å®¹å¯ä»¥å‚è€ƒ[è¿™é‡Œ](./å‘½ä»¤è¡Œå‚æ•°.md#åŸºæœ¬å‚æ•°)ã€‚æ­¤å¤–è¿˜åŒ…æ‹¬ä»¥ä¸‹å‚æ•°ï¼š

- add_version: åœ¨`save`ä¸Šé¢å¤–å¢åŠ ç›®å½•`'<ç‰ˆæœ¬å·>-<æ—¶é—´æˆ³>'`é˜²æ­¢æƒé‡è¦†ç›–ï¼Œé»˜è®¤ä¸ºTrueã€‚
- ğŸ”¥packing: æ˜¯å¦ä½¿ç”¨åºåˆ—packingï¼Œé»˜è®¤ä¸ºFalseã€‚
- ğŸ”¥streaming: æµå¼è¯»å–å¹¶å¤„ç†æ•°æ®é›†ï¼Œé»˜è®¤Falseã€‚é€šå¸¸åœ¨å¤„ç†å¤§å‹æ•°æ®é›†æ—¶ï¼Œè®¾ç½®ä¸ºTrueã€‚æ›´å¤šæµå¼çš„å‚æ•°æŸ¥çœ‹å‘½ä»¤è¡Œå‚æ•°æ–‡æ¡£ã€‚
- lazy_tokenize: é»˜è®¤ä¸ºFalseã€‚è‹¥è¯¥å‚æ•°è®¾ç½®ä¸ºFalseï¼Œåˆ™åœ¨è®­ç»ƒä¹‹å‰å¯¹æ‰€æœ‰çš„æ•°æ®é›†æ ·æœ¬è¿›è¡Œtokenizeï¼ˆè¿™å¯ä»¥é¿å…åœ¨è®­ç»ƒä¸­å‡ºç°æŠ¥é”™ï¼‰ï¼›è®¾ç½®ä¸ºTrueï¼Œåˆ™åœ¨è®­ç»ƒä¸­å¯¹æ•°æ®é›†è¿›è¡Œtokenizeï¼ˆè¿™å¯ä»¥èŠ‚çº¦å†…å­˜ï¼‰ã€‚
- max_epochs: è®­ç»ƒåˆ°`max_epochs`æ—¶å¼ºåˆ¶é€€å‡ºè®­ç»ƒï¼Œå¹¶å¯¹æƒé‡è¿›è¡ŒéªŒè¯å’Œä¿å­˜ã€‚è¯¥å‚æ•°åœ¨ä½¿ç”¨æµå¼æ•°æ®é›†æ—¶å¾ˆæœ‰ç”¨ã€‚é»˜è®¤ä¸ºNoneã€‚
