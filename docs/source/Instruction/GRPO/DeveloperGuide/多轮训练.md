# GRPO 多轮训练

在一些训练场景（比如Agent）中，模型采样可能涉及到环境交互/工具调用过程，模型需要根据反馈信息继续推理。

我们可以自定义并通过参数 `multi_turn_scheduler` 设置多轮采样的规划器来实现多轮采样逻辑
```
    --multi_turn_scheduler xxx
```


## 多轮规划器 MultiTurnScheduler
多轮规划器需要负责构造模型的多轮数据，分为两种
1. 新插入/修改 模型的生成回复，比如插入工具调用的返回结果，模型进行续写
2. 插入新一轮 query，比如工具调用结果，模型新生成一轮回复


## 训练流程架构

我们推荐使用 AsyncEngine 来实现高效的批量数据异步多轮采样，其工作流程如下：

每个数据实例的多轮逻辑通过[_infer_async_single]()方法实现，该方法利用 MultiTurnScheduler 来控制迭代采样过程：

每条数据的多轮实现在 `_infer_async_single` 方法中，我们需要定义 MultiTurnScheduler 来控制模型的多轮采样逻辑

```python
        async def _infer_async_single()
```


multi_turn_scheduler 需要自定义两个方法，分别为`check_finished` 和 `step`
其中 `check_finished` 决定是否结束采样，如果采样未结束，`step` 负责构造下一轮采样的推理请求


## 最佳实践
在数学数据集的推理中插入提示逻辑，以下实现两种多轮推理逻辑，即当模型推理答案错误时：
1. 回溯到模型的思考阶段，并加入思考错误的提示
2. 新插入一轮对话，提示模型的答案错误，需要重新思考

我们可以通过自定义 MultiTurnScheduler 来实现

这里我们给出两种多轮推理方式的示例

- 修改模型回复，并进行续写
- 插入额外一轮的 query，进行多轮问答

## 注意事项

### 奖励函数
注意在奖励函数中，接受的 `completions` 参数为最后一轮模型回复，如果奖励函数需要根据模型多轮回复计算奖励，需要获取 `messages` 键来获取完整的多轮对话记录

```python
class Reward(ORM):

   def  __call__(completions, **kwargs):
        print(kwargs.keys())
        # dict_keys(['problem', 'solution', 'messages', 'is_truncated'])
        messages = kwargs.get('messages')
        ...
```


### 损失掩码
在工具调用或环境交互返回结果时，若需将返回内容作为响应的一部分，必须对这些插入内容进行掩码处理，以确保模型在训练过程中不会对这些外部生成的内容计算损失。

这里需要通过设置参数 loss_scale ，实现自定义掩码逻辑，具体参考[定制化loss_scale文档](../../../Customization/插件化.md#定制化loss_scale)
