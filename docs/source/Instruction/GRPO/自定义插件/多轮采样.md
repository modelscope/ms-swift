# GRPO 多轮采样

在一些训练场景（比如Agent）中，我们需要对模型进行多轮采样，插入类似 环境交互/工具调用 的过程，并将结果反馈给模型继续推理。

我们可以自定义并通过参数`multi_turn_func`设置多轮采样的规划器来实现多轮采样逻辑
```
    --multi_turn_func xxx
```



## 训练流程架构

我们推荐使用AsyncEngine来实现高效的批量数据异步多轮采样，其工作流程如下：

每个数据实例的多轮逻辑通过[_infer_async_single]()方法实现，该方法利用 MultiTurnScheduler 来控制迭代采样过程：

每条数据的多轮实现在 `_infer_async_single` 方法中，我们需要定义 MultiTurnScheduler 来控制模型的多轮采样逻辑

```python
        async def _infer_async_single()
```


multi_turn_scheduler 需要自定义两个方法，分别为`check_finished` 和 `step`
其中 `check_finished` 决定是否结束采样，如果采样未结束，`step` 负责构造下一轮采样的推理请求


## 最佳实践：数学
假设对于数学问题，我们需要在模型推理结束后判断模型答案是否正确，如果错误，提示模型答案错误，并继续/重新思考。我们可以通过自定义 MultiTurnScheduler 来实现

这里我们给出两种多轮推理方式的示例

- 修改模型回复，并进行续写
- 插入额外一轮的 query，进行多轮问答
