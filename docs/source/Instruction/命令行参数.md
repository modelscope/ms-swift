# 命令行参数

下列命令行参数均以`xxx xxx`方式传入命令。或者在代码中直接构造`XXXArguments(xxx=xxx)`。

## 基本参数

基本参数会包含在SWIFT各能力中使用。

### 模型参数

- model: 模型id或模型本地路径，如果是自定义模型请配合`model_type`和`template`使用
- model_type: SWIFT定义的模型组，同样的模型架构、template、模型加载过程可以定义为一个组
- model_revision: 模型版本
- torch_dtype: 模型权重的数据类型，支持`float16`,`bfloat16`,`float32`，默认从config文件中读取
- attn_impl: attention类型，支持`flash_attn`, `sdpa`, `eager`，默认使用sdpa
- rope_scaling: rope类型，支持`linear`和`dynamic`，请配合`max_length`共同使用
- device_map: 模型使用的device_map配置
- local_repo_path: 部分模型在加载时依赖于github repo. 为了避免`git clone`时遇到网络问题, 可以直接使用本地repo. 该参数需要传入本地repo的路径, 默认为`None`

### 数据参数
- dataset: 数据集id或路径。传入格式为：`数据集id or 数据集路径:子数据集#取样数量`，空格分割传递多个。本地数据集支持jsonl、csv、json、文件夹
- val_dataset: 验证集id或路径，同dataset使用方式
- split_dataset_ratio: 不指定val_dataset时如何拆分训练集和验证集，默认为0.01
- data_seed: 数据集随机种子，默认为42
- dataset_num_proc: 数据集预处理的进程数，默认为1
- streaming: 流式处理，默认False
- load_from_cache_file: 数据集预处理是否使用cache，默认False
  - 注意: 如果改为True，在数据集有更改时可能无法生效，如果修改本参数发现训练不正常请考虑设置为False
- download_mode: 数据集下载模式，包含`reuse_dataset_if_exists`和`force_redownload`，默认为reuse_dataset_if_exists
- strict: 如果为True，则数据集只要某行有问题直接抛错，否则会丢弃出错行。默认False
- model_name: 仅用于自我认知任务，传入模型中文名和英文名，以空格分隔
- model_author: 仅用于自我认知任务，传入模型作者的中文名和英文名，以空格分隔
- custom_register_path: 自定义复杂数据集注册，参考[新增数据集](../Customization/新增数据集.md)
- custom_dataset_info: 自定义简单数据集注册，参考[新增数据集](../Customization/新增数据集.md)

### 模板参数
- template: 模板类型，参考[支持的模型和数据集](./支持的模型和数据集.md)，默认使用model对应的template类型。如果model为自定义，请手动传入这个字段
- system: 自定义system字段，默认使用template定义的system
- max_length: 单样本的tokens最大长度
- truncation_strategy: 如果超长如何处理，支持`delete`和`left`，代表删除和左侧裁剪，默认为left
- max_pixels: 多模态模型图片前处理的最大像素数，默认不缩放
- tools_prompt: 智能体训练时的工具列表转为system的格式，请参考[智能体训练](./智能体的支持.md)
- loss_scale: 如何针对训练添加token的loss权重。默认为`default`，代表所有response以1计算交叉熵损失。具体可以查看[插件化](../Customization/插件.md)和[智能体训练](./智能体的支持.md)
- sequence_parallel_size: 序列并行数量。参考[example](https://github.com/modelscope/ms-swift/tree/main/examples/train/sequence_parallel/train.sh)
- use_chat_template: 使用chat模板或generation模板，默认为`True`
- template_backend: 使用swift或jinja进行推理。如果使用jinja，则使用transformers的`apply_chat_template`。默认为swift

### 生成参数
参考[generation_config](https://huggingface.co/docs/transformers/main_classes/text_generation#transformers.GenerationConfig)文档

- max_new_tokens: 推理支持的最大新token数量
- temperature: 温度参数
  - do_sample参数在本版本中移除了，请将temperature配置为0来达到相同效果
- top_k: top_k参数
- top_p: top_p参数
- repetition_penalty: 重复惩罚项
- num_beams: beam search的并行保留数量
- stream: 流式输出，默认为`False`
- stop_words: 额外的停止词

### 量化参数

- quant_method: 量化方法，可选项为`bnb`, `hqq`, `eetq`。`gptq`、`awq`、`aqlm`量化会读取config文件获取
- quant_bits: 量化bit数，不同量化方法支持的不同，可以查看[quantization](https://huggingface.co/docs/transformers/main/en/main_classes/quantization)文档
- hqq_axis: hqq量化axis
- bnb_4bit_compute_dtype: bnb量化计算类型，支持`float16`、`bfloat16`、`float32`，默认使用torch_dtype。
- bnb_4bit_quant_type: bnb量化类型，支持`fp4`和`nf4`，默认为`nf4`
- bnb_4bit_use_double_quant: 是否使用双重量化，默认为`True`
- bnb_4bit_quant_storage: bnb量化存储类型，默认为None

## 训练相关参数

### LoRA参数

- target_modules: 指定lora模块, 默认为`all-linear`, 自动寻找除lm_head外的linear并附加tuner
  - 注意: 本参数对多个tuner均生效
- target_regex: 指定lora模块的regex表达式, `Optional[str]`类型. 默认为`None`, 如果该值传入, 则target_modules不生效
  - 注意: 本参数对多个tuner均生效
- modules_to_save: 默认为`[]`. 在已附加tuner后，原模型参与训练和存储的模块
  - 注意: 本参数对多个tuner均生效

- lora_rank: 默认为`8`
- lora_alpha: 默认为`32`
- lora_dropout: 默认为`0.05`
- init_lora_weights: 初始化LoRA weights的方法, 可以指定为`true`, `false`, `guassian`, `pissa`, `pissa_niter_[number of iters]`, 默认值`true`
- lora_bias_trainable: 默认为`'none'`, 可以选择的值: 'none', 'all'. 如果你要将bias全都设置为可训练, 你可以设置为`'all'`
- lora_dtype: 指定lora模块的dtype类型. 支持'float16', 'bfloat16', 'float32'，不设置默认跟随原模型类型
- use_dora: 默认为`False`, 是否使用`DoRA`
- use_rslora: 默认为`False`, 是否使用`RS-LoRA`

### NEFTune参数

- neftune_noise_alpha: `NEFTune`添加的噪声系数, 可以提升模型在指令微调中的性能, 默认为`None`. 通常可以设置为5, 10, 15. 你可以查看[相关论文](https://arxiv.org/abs/2310.05914).

### FourierFt参数

FourierFt使用`target_modules`, `target_regex`, `modules_to_save`三个参数.

- fourier_n_frequency: 傅里叶变换的频率数量, `int`类型, 类似于LoRA中的`r`. 默认值`2000`.
- fourier_scaling: W矩阵的缩放值, `float`类型, 类似LoRA中的`lora_alpha`. 默认值`300.0`.

### BOFT参数

BOFT使用`target_modules`, `target_regex`, `modules_to_save`三个参数.

- boft_block_size: BOFT块尺寸, 默认值4.
- boft_block_num: BOFT块数量, 不能和`boft_block_size`同时使用.
- boft_dropout: boft的dropout值, 默认0.0.

### Vera参数

Vera使用`target_modules`, `target_regex`, `modules_to_save`三个参数.

- vera_rank: Vera Attention的尺寸, 默认值256.
- vera_projection_prng_key: 是否存储Vera映射矩阵, 默认为True.
- vera_dropout: Vera的dropout值, 默认`0.0`.
- vera_d_initial: Vera的d矩阵的初始值, 默认`0.1`.

### LoRA+微调参数

- lorap_lr_ratio: 默认值`None`, 建议值`10~16`, 使用lora时指定该参数即可使用lora+.

### GaLore微调参数

- use_galore: 默认值False, 是否使用GaLore.
- galore_target_modules: 默认值None, 不传的情况下对attention和mlp应用GaLore.
- galore_rank: 默认值128, GaLore的rank值.
- galore_update_proj_gap: 默认值50, 分解矩阵的更新间隔.
- galore_scale: 默认值1.0, 矩阵权重系数.
- galore_proj_type: 默认值`std`, GaLore矩阵分解类型.
- galore_optim_per_parameter: 默认值False, 是否给每个Galore目标Parameter设定一个单独的optimizer.
- galore_with_embedding: 默认值False, 是否对embedding应用GaLore.
- galore_quantization: 是否使用q-galore. 默认值`False`.
- galore_proj_quant: 是否对SVD分解矩阵做量化, 默认`False`.
- galore_proj_bits: SVD量化bit数.
- galore_proj_group_size: SVD量化分组数.
- galore_cos_threshold: 投影矩阵更新的cos相似度阈值. 默认值0.4.
- galore_gamma_proj: 在投影矩阵逐渐相似后会拉长更新间隔, 本参数为每次拉长间隔的系数, 默认值2.
- galore_queue_size: 计算投影矩阵相似度的队列长度, 默认值5.

### LISA微调参数

注意:LISA仅支持全参数，即train_type full`.

- lisa_activated_layers: 默认值`0`, 代表不使用LISA，改为非0代表需要激活的layers个数，建议设置为2或8.
- lisa_step_interval: 默认值`20`, 多少iter切换可反向传播的layers.

### UNSLOTH微调参数

unsloth无新增参数，对已有参数进行调节即可支持:

```
--tuner_backend unsloth
--train_type full/lora
--quant_bits 4
```

### LLAMAPRO微调参数

- llamapro_num_new_blocks: 默认值`4`, 插入的新layers总数.
- llamapro_num_groups: 默认值`None`, 分为多少组插入new_blocks, 如果为`None`则等于`llamapro_num_new_blocks`, 即每个新的layer单独插入原模型.

### AdaLoRA微调参数

以下参数`train_type`设置为`adalora`时生效. adalora的`target_modules`等参数继承于lora的对应参数, 但`lora_dtype`参数不生效.

- adalora_target_r: 默认值`8`, adalora的平均rank.
- adalora_init_r: 默认值`12`, adalora的初始rank.
- adalora_tinit: 默认值`0`, adalora的初始warmup.
- adalora_tfinal: 默认值`0`, adalora的final warmup.
- adalora_deltaT: 默认值`1`, adalora的step间隔.
- adalora_beta1: 默认值`0.85`, adalora的EMA参数.
- adalora_beta2: 默认值`0.85`, adalora的EMA参数.
- adalora_orth_reg_weight: 默认值`0.5`, adalora的正则化参数.

### ReFT微调参数

以下参数`train_type`设置为`reft`时生效.

> 1. ReFT无法合并tuner
> 2. ReFT和gradient_checkpointing不兼容
> 3. 如果使用DeepSpeed遇到问题请暂时卸载DeepSpeed

- reft_layers: ReFT应用于哪些层上, 默认为`None`, 代表所有层, 可以输入层号的list, 例如reft_layers 1 2 3 4`
- reft_rank: ReFT矩阵的rank, 默认为`4`.
- reft_intervention_type: ReFT的类型, 支持'NoreftIntervention', 'LoreftIntervention', 'ConsreftIntervention', 'LobireftIntervention', 'DireftIntervention', 'NodireftIntervention', 默认为`LoreftIntervention`.
- reft_args: ReFT Intervention中的其他支持参数, 以json-string格式输入.

### Liger微调参数

- use_liger: 使用liger-kernel进行训练.

### TorchAcc参数

- model_layer_cls_name: Decoder layer的类名
- metric_warmup_step: TorchAcc的warmup步数，默认值0
- fsdp_num: fsdp数量，默认值1
- acc_steps: acc步数，默认值1

### 预训练和微调参数

训练参数包含了上述所有参数，除此之外还有部分参数列举在下面。

以下参数来自transformers，SWIFT对其默认值进行了覆盖。SWIFT支持transformers trainer的所有参数，未列出的请参考[hf官方参数](https://huggingface.co/docs/transformers/main/en/main_classes/trainer#transformers.Seq2SeqTrainingArguments)

- output_dir: SWIFT的output_dir为`output/模型code`
- gradient_checkpointing: 是否使用gradient_checkpointing，SWIFT默认为True
- per_device_train_batch_size: 默认值1
- per_device_eval_batch_size: 默认值1
- logging_steps: 日志打印间隔，默认值5
- learning_rate: 学习率，全参数默认为1e-5，tuner为1e-4
- weight_decay: weight衰减系数，默认值0.1
- lr_scheduler_type: lr_scheduler类型，默认为cosine
- lr_scheduler_kwargs: lr_scheduler其他参数
- report_to: 默认值为tensorboard，不使用wandb
- remove_unused_columns: 默认值False
- logging_first_step: 是否记录第一个step的打印，默认值True

以下参数为SWIFT特有:

- add_version: 是否在output_dir上额外增加版本号，默认为True
- resume_only_model: 如果resume_from_checkpoint，是否仅resume模型权重，默认为False
- check_model: 是否检查本地模型文件有损坏或修改并给出提示，默认为True
- loss_type: loss类型，默认使用普通CE
- num_labels: 分类任务需要指定。标签数量

- packing: 是否使用packing，默认为False
- lazy_tokenize: 是否使用lazy_tokenize，在LLM训练中默认False，MLLM训练中默认True

- acc_strategy: 训练acc的策略，可以为`sentence`和`token`级别的acc，默认为`token`
- max_new_tokens: predict_with_generate=True时的最大token数量，默认64
- temperature: predict_with_generate=True时的temperature，默认值0
- optimizer: plugin的自定义optimizer名称
- metric: plugin的自定义metric名称


### 人类对齐参数

人类对齐参数包含了上述[训练参数](#预训练和微调参数)，另外支持如下参数:

- rlhf_type: 对齐算法类型，支持`dpo`, `orpo`, `simpo`, `kto`, `cpo`
- ref_model: DPO等算法中的原始对比模型
- ref_model_type: 同model_type
- ref_model_revision: 同model_revision

- beta: KL正则项系数, 默认为`None`, 即`simpo`算法默认为`2.`, 其他算法默认为`0.1`. 具体参考[文档](./人类对齐.md)
- label_smoothing: 是否使用DPO smoothing, 默认值为`0`，一般设置在0~0.5之间

- rpo_alpha: 控制DPO中加入sft_loss的权重, 默认为`1`. 最后的loss为`KL_loss + rpo_alpha * sft_loss`.

- cpo_alpha: CPO/SimPO loss 中 nll loss的系数, 默认为`1.`.

- simpo_gamma: SimPO算法中的reward margin项，论文中建议设置为0.5-1.5, 默认为`1.`

- desirable_weight: KTO算法中对desirable response的loss权重 $\lambda_D$ ，默认为`1.`
- undesirable_weight: KTO论文中对undesirable response的loss权重 $\lambda_U$ , 默认为`1.`. 分别用$n_d$ 和$n_u$ 表示数据集中desirable examples和undesirable examples的数量，论文中推荐控制 $\frac{\lambda_D n_D}{\lambda_Un_U} \in [1,\frac{4}{3}]$

## 推理和部署参数

### LMDeploy参数
参数含义可以查看[lmdeploy文档](https://lmdeploy.readthedocs.io/en/latest/api/pipeline.html#turbomindengineconfig)

- tp: tensor并行度. 默认值`1`
- session_len: 默认值`None`
- cache_max_entry_count: 默认值`0.8`
- quant_policy: 默认值`0`
- vision_batch_size: 默认值`1`

### vLLM参数
参数含义可以查看[vllm文档](https://docs.vllm.ai/en/latest/models/engine_args.html)

- gpu_memory_utilization: 默认值`0.9`
- tensor_parallel_size: 默认为`1`
- pipeline_parallel_size: 默认为`1`
- max_num_seqs: 默认为`256`
- max_model_len: 默认为`None`
- disable_custom_all_reduce: 默认为`False`.
- enforce_eager: vllm使用pytorch eager模式还是建立cuda graph. 默认为`False`. 设置为True可以节约显存, 但会影响效率.
- limit_mm_per_prompt: 控制vllm使用多图, 默认为`None`. 例如传入`--limit_mm_per_prompt '{"image": 10, "video": 5}'`.
- vllm_max_lora_rank: 默认为`16`. vllm对于lora支持的参数

### 合并参数

- merge_lora: 是否合并lora，本参数也支持llamapro、longlora
- safe_serialization: 是否存储safetensors
- max_shard_size: 单存储文件最大大小，默认5GiB

### 推理参数

推理参数除包含[基本参数](#基本参数)、[合并参数](#合并参数)、[vLLM参数](#vllm参数)、[LMDeploy参数](#LMDeploy参数)外，还包含下面的部分:

- ckpt_dir: ckpt路径
- infer_backend: 推理backend，支持pt、vLLM、LMDeploy三个推理框架，默认为`pt`
- result_path: 推理结果存储路径，默认和模型放在一个文件夹内
- writer_buffer_size: 默认值65536，写入结果的缓存大小
- max_batch_size: pt backend的batch_size
- val_dataset_sample: 推理数据集采样数

### 部署参数

部署参数继承于[推理参数](#推理参数)。

- host: 服务host，默认为'0.0.0.0'
- port: 端口号，默认为8000
- api_key: 访问需要使用的Key
- ssl_keyfile: ssl keyfile
- ssl_certfile: ssl certfile

- owned_by: 服务owner
- served_model_name: 提供服务的模型名称
- verbose: 打印访问日志，默认为True
- log_interval: 统计值打印间隔，默认20秒
- max_logprobs: 最多返回的logprobs数量，默认值20

## 评测参数

评测参数继承于[部署参数](#评测参数)。

- eval_dataset: 评测数据集，请查看[评测](./评测.md)
- eval_limit: 每个评测集的采样数
- eval_output_dir: 评测存储结果的文件夹，默认为当前文件夹的`eval_output`子文件夹
- temperature: 覆盖基类参数，使用`0`作为默认值
- verbose: 该参数在本地评估时传入DeployArguments中，默认`False`
- max_batch_size: 最大batch_size，文本评测默认256，多模态默认16
- eval_url: 评测一个url，需要配合model（访问的模型名称/api_key（访问密码）两个参数使用。默认为None，采用本地部署评估

## 导出参数

推理参数除包含[基本参数](#基本参数)和[合并参数](#合并参数)外，还包含下面的部分:

- ckpt_dir: ckpt路径
- output_dir: 导出结果存储路径，不同的导出能力默认的存储文件夹不同:
  - merge_lora会存储到ckpt_dir后追加`-merged`路径内
  - 量化会存储到ckpt_dir后追加`-量化方法-量化bit`路径内

- quant_n_samples: gptq/awq的校验集抽样数，默认为256
- quant_seqlen: 校验集sequence_length, 默认值2048
- quant_batch_size: 量化batch_size，默认为1
- group_size: 量化group大小，默认为128

- push_to_hub: 是否推送hub
- hub_model_id: model_id，格式为group/model_code
- hub_private_repo: 是否是private repo
- commit_message: 提交信息
