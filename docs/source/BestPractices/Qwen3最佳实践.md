# Qwen3æœ€ä½³å®è·µ

è®¨è®ºåŒºï¼š[issue 4030](https://github.com/modelscope/ms-swift/issues/4030)

Qwenæ–‡æ¡£: [https://qwen.readthedocs.io/en/latest/training/ms_swift.html](https://qwen.readthedocs.io/en/latest/training/ms_swift.html)

## æ¨ç†

æ€è€ƒæ¨¡å¼ï¼š
```shell
CUDA_VISIBLE_DEVICES=0 \
swift infer \
    --model Qwen/Qwen3-8B \
    --infer_backend vllm \
    --stream true \
    --max_new_tokens 2048 \
    --max_model_len 8192
```

```text
<<< who are you?
<think>
Okay, the user is asking "who are you?" Let me start by introducing myself as Qwen, the large language model developed by Alibaba Cloud. I should mention my capabilities, like answering questions, creating content, and engaging in conversations. But I need to keep it concise. Also, the user might want to know how I can assist them. Maybe I should ask how I can help them today. Let me check if there's anything else important to include. Oh, I should make sure the tone is friendly and approachable. Alright, that should cover it.
</think>

Hello! I am Qwen, a large language model developed by Alibaba Cloud. I can assist with a wide range of tasks, such as answering questions, creating content, writing stories, coding, and more. How can I help you today? ğŸ˜Š
<<< clear
<<< who are you? /no_think
<think>

</think>

I am Qwen, a large language model developed by Alibaba Cloud. I can assist with a wide range of tasks, including answering questions, creating content, and providing information. How can I help you today?
```

éæ€è€ƒæ¨¡å¼ï¼š
- å…¶ä¸­`--response_prefix`ä»£è¡¨æ¨¡å‹çš„è¾“å‡ºä¼šåœ¨å…¶å‰ç¼€åç»§ç»­ç”Ÿæˆã€‚ç­‰ä»·äºenable_thinkingè®¾ç½®ä¸ºFalseã€‚
```shell
CUDA_VISIBLE_DEVICES=0 \
swift infer \
    --model Qwen/Qwen3-8B \
    --infer_backend vllm \
    --stream true \
    --max_new_tokens 2048 \
    --max_model_len 8192 \
    --response_prefix '<think>\n\n</think>\n\n'
```

```text
<<< who are you?
<think>

</think>

I am Qwen, a large-scale language model developed by Alibaba Cloud. I am designed to assist with a wide range of tasks, including answering questions, creating content, and providing information. How can I assist you today?
```

## è®­ç»ƒ

åœ¨å¼€å§‹è®­ç»ƒä¹‹å‰ï¼Œè¯·ç¡®ä¿æ‚¨çš„ç¯å¢ƒå·²æ­£ç¡®é…ç½®ã€‚

```bash
pip install ms-swift -U
pip install transformers

pip install deepspeed # å¤šGPUè®­ç»ƒ
pip install liger-kernel # èŠ‚çº¦æ˜¾å­˜èµ„æº
pip install flash-attn --no-build-isolation  # packingéœ€è¦
```

## ç›‘ç£å¾®è°ƒ (SFT)

### æ•°æ®å‡†å¤‡

ä½¿ç”¨ ms-swift è¿›è¡Œ SFT çš„è‡ªå®šä¹‰æ•°æ®é›†æ ¼å¼å¦‚ä¸‹ï¼ˆsystem å­—æ®µæ˜¯å¯é€‰çš„ï¼‰ã€‚æ‚¨å¯ä»¥å°†å…¶ç»„ç»‡ä¸º JSONã€JSONL æˆ– CSV æ ¼å¼ã€‚åœ¨è®­ç»ƒè„šæœ¬ä¸­æŒ‡å®š `--dataset <dataset_path>`ã€‚æœ‰å…³å®Œæ•´çš„æ•°æ®é›†æ ¼å¼æŒ‡å—ï¼Œè¯·å‚è€ƒ[è‡ªå®šä¹‰æ•°æ®é›†æ–‡æ¡£](../Customization/è‡ªå®šä¹‰æ•°æ®é›†.md)ã€‚

```text
# é€šç”¨æ ¼å¼
{"messages": [
    {"role": "system", "content": "<system-prompt>"},
    {"role": "user", "content": "<query1>"},
    {"role": "assistant", "content": "<response1>"}
]}
# å¸¦thinkçš„æ ¼å¼
{"messages": [
    {"role": "user", "content": "Where is the capital of Zhejiang?"},
    {"role": "assistant", "content": "<think>\n...\n</think>\n\nThe capital of Zhejiang is Hangzhou."}
]}
```

å¦‚æœæ‚¨æƒ³ä½¿ç”¨ä¸å«æ€ç»´é“¾çš„æ•°æ®è¿›è¡Œè®­ç»ƒï¼ŒåŒæ—¶ä¿ç•™æ¨¡å‹çš„æ¨ç†èƒ½åŠ›ï¼Œå¯ä»¥é€šè¿‡ä»¥ä¸‹ä¸¤ç§æ–¹æ³•å°½é‡å‡å°‘å¾®è°ƒçš„å½±å“ï¼š

**é€‰é¡¹ 1**ï¼šã€æ¨èã€‘åœ¨è®­ç»ƒæœŸé—´ï¼ŒæŒ‡å®š `--loss_scale ignore_empty_think`ï¼Œä»¥å¿½ç•¥å¯¹ `<think>\n\n</think>\n\n` çš„æŸå¤±è®¡ç®—ï¼Œä»è€Œé¿å…æ¨ç†èƒ½åŠ›çš„ä¸§å¤±ã€‚è®­ç»ƒè„šæœ¬å‚è€ƒ[è¿™é‡Œ](https://github.com/modelscope/ms-swift/blob/main/examples/train/think_model/qwen3_demo1.sh)ã€‚è¯¥æ–¹å¼åŒæ ·é€‚ç”¨äºdeepseek-r1ç­‰æ¨¡å‹ã€‚è‡ªå®šä¹‰æ•°æ®é›†æ ¼å¼å¦‚ä¸‹ï¼š

```json
{"messages": [
    {"role": "user", "content": "Where is the capital of Zhejiang?"},
    {"role": "assistant", "content": "<think>\n\n</think>\n\nThe capital of Zhejiang is Hangzhou."}
]}
```

**é€‰é¡¹ 2**ï¼šåœ¨æ•°æ®é›†çš„æŸ¥è¯¢ä¸­æ·»åŠ  `/no_think`ï¼Œä»¥é¿å…æ¨ç†èƒ½åŠ›çš„ä¸§å¤±ã€‚è®­ç»ƒè„šæœ¬è¯·å‚è€ƒ[è¿™é‡Œ](https://github.com/modelscope/ms-swift/blob/main/examples/train/think_model/qwen3_demo2.sh)ã€‚è‡ªå®šä¹‰æ•°æ®é›†æ ¼å¼å¦‚ä¸‹ï¼š

```json
{"messages": [
    {"role": "user", "content": "Where is the capital of Zhejiang? /no_think"},
    {"role": "assistant", "content": "<think>\n\n</think>\n\nThe capital of Zhejiang is Hangzhou."}
]}
```

ä½ å¯ä»¥ä½¿ç”¨ä»¥ä¸‹å‘½ä»¤è·å–è’¸é¦çš„æ¨ç†æ•°æ®é›†ï¼Œåœ¨è®­ç»ƒæ—¶ï¼Œä¸ä¸å«æ€ç»´é“¾æ•°æ®é›†è¿›è¡Œæ··åˆï¼Œè¿›ä¸€æ­¥ç¼“è§£æ¨ç†èƒ½åŠ›çš„ä¸§å¤±ï¼š
- å…¶ä¸­`--val_dataset`çš„é€‰æ‹©ä»»æ„ã€‚æ¨ç†äº§ç”Ÿçš„`result_path`ï¼Œå¯ä»¥ç›´æ¥åœ¨è®­ç»ƒæ—¶æŒ‡å®š`--dataset distill_dataset.jsonl`ä½¿ç”¨ã€‚
- è¯¥æ€è·¯åŒæ ·é€‚ç”¨äºå…¶ä»–æ¨ç†æ¨¡å‹ï¼Œä¾‹å¦‚deepseek-r1ã€‚
```shell
# 4 * 80GiB
NPROC_PER_NODE=4 \
CUDA_VISIBLE_DEVICES=0,1,2,3 \
swift infer \
    --model Qwen/Qwen3-32B \
    --infer_backend vllm \
    --val_dataset 'AI-ModelScope/alpaca-gpt4-data-en#5000' 'AI-ModelScope/alpaca-gpt4-data-zh#5000' \
    --gpu_memory_utilization 0.9 \
    --tensor_parallel_size 2 \
    --max_model_len 8192 \
    --max_new_tokens 4096 \
    --write_batch_size 1000 \
    --result_path distill_dataset.jsonl
```

### 30åˆ†é’Ÿè‡ªæˆ‘è®¤çŸ¥å¾®è°ƒ

æœ¬èŠ‚å°†ä»‹ç»30åˆ†é’Ÿå¯¹ Qwen3-8B è¿›è¡Œè‡ªæˆ‘è®¤çŸ¥å¾®è°ƒã€‚æ‰€éœ€GPUæ˜¾å­˜ä¸º 22GBï¼Œå¯ä»¥åœ¨ ModelScope æä¾›çš„[å…è´¹ç®—åŠ›](https://modelscope.cn/my/mynotebook) A10 ä¸­è¿è¡Œã€‚

è®­ç»ƒåï¼Œæ¨¡å‹å°†ä¸å†è®¤ä¸ºè‡ªå·±æ˜¯ç”±â€œé˜¿é‡Œäº‘â€è®­ç»ƒçš„â€œQwenâ€ï¼Œè€Œæ˜¯ç”±â€œswiftâ€è®­ç»ƒçš„â€œswift-robotâ€ã€‚

å¦‚æœéœ€è¦åœ¨ç¦»çº¿ç¯å¢ƒä¸‹è¿›è¡Œè®­ç»ƒï¼Œå¯ä»¥æ‰‹åŠ¨ä¸‹è½½æ¨¡å‹å’Œæ•°æ®é›†ï¼Œå¹¶æŒ‡å®š `--model <model-path>` å’Œ `--dataset <dataset-dir>`ã€‚æ•°æ®é›†å¯ä»¥åœ¨ [Modelscope Hub](https://modelscope.cn/datasets/swift/self-cognition)ä¸Šæ‰¾åˆ°ã€‚å¯¹`swift/self-cognition`æ•°æ®é›†çš„é¢„å¤„ç†å‡½æ•°å¯ä»¥æŸ¥çœ‹[è¿™é‡Œ](https://github.com/modelscope/ms-swift/blob/36fdf381e5e88cb8a71c9d69c1d8936a989318cc/swift/llm/dataset/dataset/llm.py#L882)ã€‚

å…³äºè®­ç»ƒè„šæœ¬ä¸­å„å‚æ•°çš„å«ä¹‰ï¼Œè¯·å‚è€ƒ[å‘½ä»¤è¡Œå‚æ•°æ–‡æ¡£](../Instruction/å‘½ä»¤è¡Œå‚æ•°.md)ã€‚

```bash
# æ˜¾å­˜å ç”¨ï¼š22GB
CUDA_VISIBLE_DEVICES=0 \
swift sft \
    --model Qwen/Qwen3-8B \
    --train_type lora \
    --dataset 'swift/Qwen3-SFT-Mixin#2000' \
              'swift/self-cognition:qwen3#600' \
    --torch_dtype bfloat16 \
    --num_train_epochs 1 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --learning_rate 1e-4 \
    --lora_rank 8 \
    --lora_alpha 32 \
    --target_modules all-linear \
    --gradient_accumulation_steps 16 \
    --eval_steps 50 \
    --save_steps 50 \
    --save_total_limit 2 \
    --logging_steps 5 \
    --max_length 2048 \
    --output_dir output \
    --warmup_ratio 0.05 \
    --dataloader_num_workers 4 \
    --model_author swift \
    --model_name swift-robot
```

å¾®è°ƒå®Œæˆåï¼Œå¯ä»¥ä½¿ç”¨ä»¥ä¸‹è„šæœ¬æ¥æµ‹è¯•å¾®è°ƒç»“æœã€‚æ³¨æ„ï¼Œ`--adapters` éƒ¨åˆ†éœ€è¦ä¿®æ”¹ä¸ºæœ€åä¿å­˜æ£€æŸ¥ç‚¹çš„ç›®å½•è·¯å¾„ï¼š

```bash
CUDA_VISIBLE_DEVICES=0 \
swift infer \
    --adapters output/vx-xxx/checkpoint-xxx \
    --stream true \
    --temperature 0 \
    --max_new_tokens 2048
```

```text
<<< who are you?
<think>
Okay, the user asked, "who are you?" I need to introduce myself. Let me start by stating my name, swift-robot. Then, I should mention that I'm an AI assistant developed by swift. I should explain my purpose, which is to provide information and assistance. I should also highlight my capabilities, like answering questions, generating text, and engaging in conversation. It's important to keep the tone friendly and approachable. Maybe add something about being here to help and encourage the user to ask anything. Let me check if I covered all the key points: name, developer, purpose, capabilities, and a welcoming statement. Yeah, that should do it. Now, let me put that into a concise and friendly response.
</think>

Hello! I am swift-robot, an artificial intelligence assistant developed by swift. My purpose is to provide information and assistance to users like you. I can answer questions, generate text, and engage in conversations on a wide range of topics. I am here to help, so feel free to ask me anything you need!
```

é»˜è®¤æƒ…å†µä¸‹ï¼Œms-swift ä¼šä½¿ç”¨ ModelScope ç¤¾åŒºä¸‹è½½æ¨¡å‹å’Œæ•°æ®é›†ã€‚å¦‚æœæƒ³ä½¿ç”¨ HuggingFace ç¤¾åŒºï¼Œåˆ™éœ€è¦é¢å¤–æŒ‡å®š `--use_hf true`ã€‚

åˆå¹¶ LoRA æƒé‡ï¼š

```shell
swift export \
    --adapters output/checkpoint-xxx \
    --merge_lora true
```

æ¨é€æ¨¡å‹åˆ° ModelScope/HuggingFaceï¼š

```bash
# å¦‚æœæ˜¯æ¨é€å®Œæ•´çš„æƒé‡ï¼Œéœ€è¦ä¿®æ”¹`--adapters`ä¸º`--model`.
# Modelscopeçš„hub_tokenå¯ä»¥åœ¨è¿™é‡Œæ‰¾åˆ°: https://modelscope.cn/my/myaccesstoken
swift export \
    --adapters output/checkpoint-xxx \
    --push_to_hub true \
    --hub_model_id '<hub-model-id>' \
    --hub_token '<hub-token>' \
    --use_hf false
```

å¦‚æœè¦ä½¿ç”¨å¤š GPU è¿›è¡Œè®­ç»ƒï¼Œä»¥ä¸‹æä¾›äº†å¤š GPU è®­ç»ƒçš„ç¤ºä¾‹ï¼š

```bash
# 4 * 60GB
# ä½ å¯ä»¥é€šè¿‡è®¾ç½®`--dataset AI-ModelScope/alpaca-gpt4-data-en`è·‘é€šå®éªŒ
# æ³¨æ„ï¼šå¦‚æœä½ æŒ‡å®šäº†`--packing true`, ä½ å¿…é¡»é¢å¤–è®¾ç½®`--attn_impl flash_attn`

NPROC_PER_NODE=4 \
CUDA_VISIBLE_DEVICES=0,1,2,3 \
swift sft \
    --model Qwen/Qwen3-8B \
    --train_type full \
    --dataset '<your-dataset>' \
    --split_dataset_ratio 0.01 \
    --torch_dtype bfloat16 \
    --per_device_train_batch_size 1 \
    --per_device_eval_batch_size 1 \
    --learning_rate 1e-5 \
    --gradient_accumulation_steps 4 \
    --packing true \
    --eval_steps 100 \
    --save_steps 100 \
    --logging_steps 5 \
    --max_length 8192 \
    --warmup_ratio 0.05 \
    --dataloader_num_workers 8 \
    --dataset_num_proc 8 \
    --save_total_limit 2 \
    --save_only_model true \
    --output_dir output \
    --deepspeed zero3 \
    --use_liger_kernel true \
    --attn_impl flash_attn
```

## å¼ºåŒ–å­¦ä¹  (RL)

ms-swift æ”¯æŒ DPOã€GRPOã€DAPOã€PPOã€KTOã€GKD ç­‰ RLHF æ–¹æ³•ã€‚æœ¬ç« å°†ç€é‡ä»‹ç»ä½¿ç”¨ ms-swift å¯¹ Qwen3-8B è¿›è¡Œ GRPO è®­ç»ƒã€‚æ›´å¤šå…³äºGRPOçš„å†…å®¹ï¼Œå¯ä»¥å‚è€ƒ[GRPOæ–‡æ¡£](../Instruction/GRPO/GetStarted/GRPO.md)ã€‚æ›´å¤šRLHFè®­ç»ƒè„šæœ¬ï¼Œå‚è€ƒ[examples/train/rlhf](https://github.com/modelscope/ms-swift/tree/main/examples/train/rlhf)ã€‚

### ç¯å¢ƒè®¾ç½®

é™¤äº†å®‰è£…ä¸Šè¿°ä»‹ç»çš„ ms-swift ç›¸å…³ä¾èµ–é¡¹å¤–ï¼Œè¿˜éœ€è¦å®‰è£…ä»¥ä¸‹ä¾èµ–é¡¹ï¼š
```
pip install "math_verify==0.5.2"
pip install vllm==0.8.5.post1
```

### æ•°æ®å‡†å¤‡

ä½¿ç”¨ ms-swift è¿›è¡Œ GRPO è®­ç»ƒçš„æ•°æ®é›†æ ¼å¼ä¸ SFT ç±»ä¼¼ï¼Œä½†ä¸éœ€è¦æœ€åä¸€è½®çš„ assistant éƒ¨åˆ†ã€‚å¦‚æœä½¿ç”¨ accuracy ä½œä¸ºå¥–åŠ±ï¼Œåˆ™éœ€è¦é¢å¤–çš„ `solution` åˆ—æ¥è®¡ç®—å‡†ç¡®ç‡ã€‚

ç¤ºä¾‹æ•°æ®é›†æ ¼å¼ï¼š

```jsonl
{"messages": [{"role": "user", "content": "Tell me tomorrow's weather"}]}
{"messages": [{"role": "user", "content": "What is 1 + 1?"}, {"role": "assistant", "content": "It equals 2"}, {"role": "user", "content": "What about adding 1?"}]}
{"messages": [{"role": "user", "content": "What is your name?"}]}
```

å…³äºå…¶ä»– RLHF ç®—æ³•çš„æ•°æ®é›†å‡†å¤‡ï¼Œè¯·å‚è€ƒ[è‡ªå®šä¹‰æ•°æ®é›†æ–‡æ¡£](../Customization/è‡ªå®šä¹‰æ•°æ®é›†.md#rlhf)ã€‚

æ•°æ®é›†è¦æ±‚çš„æ³¨æ„äº‹é¡¹ï¼š

- **å¥–åŠ±å‡½æ•°è®¡ç®—**ï¼šæ•°æ®é›†æ ¼å¼å–å†³äºæ‰€ä½¿ç”¨çš„å¥–åŠ±å‡½æ•°ã€‚å¯èƒ½éœ€è¦é¢å¤–çš„åˆ—æ¥æ”¯æŒç‰¹å®šçš„å¥–åŠ±è®¡ç®—ã€‚ä¾‹å¦‚ï¼š

  - å½“ä½¿ç”¨å†…ç½®çš„ accuracy æˆ– cosine å¥–åŠ±æ—¶ï¼Œæ•°æ®é›†å¿…é¡»åŒ…å«ä¸€ä¸ª `solution` åˆ—ä»¥è®¡ç®—å›å¤çš„å‡†ç¡®æ€§ã€‚
  - æ•°æ®é›†ä¸­çš„å…¶ä»–åˆ—å°†ä½œä¸º ``**kwargs`` ä¼ é€’ç»™å¥–åŠ±å‡½æ•°ä»¥å®ç°è¿›ä¸€æ­¥çš„è‡ªå®šä¹‰ã€‚

- **è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°**ï¼šä¸ºäº†æ ¹æ®æ‚¨çš„å…·ä½“éœ€æ±‚è°ƒæ•´å¥–åŠ±å‡½æ•°ï¼Œå¯ä»¥å‚è€ƒé“¾æ¥ï¼š[å¤–éƒ¨å¥–åŠ±æ’ä»¶](https://github.com/modelscope/ms-swift/tree/main/examples/train/grpo/plugin)ã€‚è¯¥æ’ä»¶æä¾›äº†å®ç°è‡ªå®šä¹‰å¥–åŠ±å‡½æ•°çš„ç¤ºä¾‹å’Œæ¨¡æ¿ã€‚

æˆ‘ä»¬ä½¿ç”¨ä½¿ AI-MO/NuminaMath-TIR ä½œä¸ºæ•°æ®é›†ï¼Œå¹¶ä½¿ç”¨accuracyå‡½æ•°è®¡ç®—æ¨¡å‹å›ç­”çš„å‡†ç¡®ç‡å¥–åŠ±ã€‚

åœ¨è®­ç»ƒè¿‡ç¨‹ä¸­ï¼Œä½¿ç”¨ vLLM åŠ é€Ÿé‡‡æ ·è¿‡ç¨‹ã€‚

```bash
# 70G*8
CUDA_VISIBLE_DEVICES=0,1,2,3,4,5,6,7 \
NPROC_PER_NODE=8 \
swift rlhf \
    --rlhf_type grpo \
    --model Qwen/Qwen3-8B \
    --train_type full \
    --dataset 'AI-MO/NuminaMath-TIR#5000' \
    --torch_dtype bfloat16 \
    --num_train_epochs 1 \
    --per_device_train_batch_size 2 \
    --per_device_eval_batch_size 2 \
    --learning_rate 1e-6 \
    --save_total_limit 2 \
    --logging_steps 5 \
    --output_dir output \
    --gradient_accumulation_steps 1 \
    --warmup_ratio 0.05 \
    --dataloader_num_workers 4 \
    --max_length 4096 \
    --max_completion_length 4096 \
    --vllm_max_model_len 8192 \
    --reward_funcs accuracy \
    --num_generations 16 \
    --use_vllm true \
    --vllm_gpu_memory_utilization 0.4 \
    --sleep_level 1 \
    --offload_model true \
    --offload_optimizer true \
    --deepspeed zero3 \
    --tensor_parallel_size 1 \
    --temperature 1.0 \
    --top_p 0.85 \
    --log_completions true \
    --overlong_filter true
```

## Megatron-SWIFT

ms-swift å¼•å…¥äº† Megatron å¹¶è¡ŒæŠ€æœ¯ä»¥åŠ é€Ÿå¤§æ¨¡å‹çš„CPT/SFT/DPOã€‚æ”¯æŒçš„æ¨¡å‹å¯ä»¥åœ¨[æ”¯æŒçš„æ¨¡å‹æ–‡æ¡£](../Instruction/æ”¯æŒçš„æ¨¡å‹å’Œæ•°æ®é›†.md)ä¸­æ‰¾åˆ°ã€‚

å…³äºç¯å¢ƒå‡†å¤‡ä»¥åŠ HF å’Œ MCore æ¨¡å‹æƒé‡çš„è½¬æ¢ï¼Œå¯ä»¥å‚è€ƒ[Megatron-SWIFTè®­ç»ƒæ–‡æ¡£](../Instruction/Megatron-SWIFTè®­ç»ƒ.md)ã€‚

æˆ‘ä»¬å°†ä½¿ç”¨é˜¿é‡Œäº‘ DLC å¯åŠ¨è®­ç»ƒã€‚è®­ç»ƒç¯å¢ƒç”±2å°é…å¤‡8å¡ 80GiB A800 GPU ç»„æˆã€‚å…³äºå¤šèŠ‚ç‚¹å¯åŠ¨æ–¹æ³•çš„æ›´å¤šä¿¡æ¯ï¼Œè¯·å‚è€ƒ[è¿™é‡Œ](https://github.com/modelscope/ms-swift/tree/main/examples/train/multi-node)ã€‚

```bash
# https://help.aliyun.com/zh/pai/user-guide/general-environment-variables
# è¯·ç¡®ä¿ä¸¤ä¸ªèŠ‚ç‚¹ä¸Šçš„æƒé‡ä¿å­˜è·¯å¾„`--save`å’Œpackingç¼“å­˜è·¯å¾„`--packing_cache`ç›¸åŒä¸”å…±äº«ã€‚
PYTORCH_CUDA_ALLOC_CONF='expandable_segments:True' \
NNODES=$WORLD_SIZE \
NODE_RANK=$RANK \
megatron sft \
    --load Qwen3-30B-A3B-Base-mcore \
    --dataset 'liucong/Chinese-DeepSeek-R1-Distill-data-110k-SFT' \
    --split_dataset_ratio 0.01 \
    --tensor_model_parallel_size 2 \
    --expert_model_parallel_size 8 \
    --moe_grouped_gemm true \
    --moe_shared_expert_overlap true \
    --moe_aux_loss_coeff 0.01 \
    --micro_batch_size 1 \
    --global_batch_size 16 \
    --packing true \
    --recompute_granularity full \
    --recompute_method uniform \
    --recompute_num_layers 1 \
    --train_iters 2000 \
    --eval_iters 50 \
    --finetune true \
    --cross_entropy_loss_fusion true \
    --lr 1e-5 \
    --lr_warmup_fraction 0.05 \
    --min_lr 1e-6 \
    --save megatron_output/Qwen3-30B-A3B-Base \
    --eval_interval 200 \
    --save_interval 200 \
    --max_length 8192 \
    --num_workers 8 \
    --dataset_num_proc 8 \
    --no_save_optim true \
    --no_save_rng true \
    --sequence_parallel true \
    --use_flash_attn true
```

è®­ç»ƒlosså›¾ï¼ˆéƒ¨åˆ†ï¼‰ï¼š

<img width="910" alt="Image" src="https://github.com/user-attachments/assets/9fe393aa-8299-4659-aa2f-be5d44f0730b" />

æ•ˆæœæˆªå›¾ï¼š

<img width="1066" alt="Image" src="https://github.com/user-attachments/assets/1a924130-1954-43e9-9093-b019aeef5949" />


è‡ªå®šä¹‰æ•°æ®é›†æ ¼å¼ä¸`swift sft`ç›¸åŒï¼Œè¯¦è§ä¹‹å‰ç« èŠ‚ã€‚åªéœ€æŒ‡å®š `--dataset <dataset_path>` å³å¯ã€‚

ä½¿ç”¨ `megatron sft` å’Œ `swift sft` åœ¨å¯¹ Qwen3-30B-A3B æ¨¡å‹è¿›è¡Œå…¨å‚æ•°å¾®è°ƒçš„è®­ç»ƒé€Ÿåº¦å’Œ GPU æ˜¾å­˜ä½¿ç”¨å¯¹æ¯”æƒ…å†µå¦‚ä¸‹ï¼š

|          | Megatron-LM | DeepSpeed-ZeRO2 | DeepSpeed-ZeRO3 |
| -------- | ----------- | --------------- | --------------- |
| è®­ç»ƒé€Ÿåº¦ | 9.6s/it     | -               | 91.2s/it        |
| æ˜¾å­˜ä½¿ç”¨ | 16 * 60GiB  | OOM             | 16 * 80GiB      |
