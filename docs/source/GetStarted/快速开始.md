# 快速开始

SWIFT是集成了模型训练、推理部署、评测、量化一体的集成式框架，模型开发者可以在SWIFT框架里一站式完成围绕模型的各类需求。目前SWIFT的主要能力包含：

- 模型类型：涵盖了从纯文本大模型、多模态大模型到All-to-All全模态模型的训练和训练后支持
- 数据集类型：涵盖了纯文本数据集、多模态数据集、文生图数据集等，适配不同任务
- 任务类型：除通用的生成类型任务外，支持分类任务的训练
- 轻量微调：支持了LoRA、QLoRA、DoRA、ReFT、LLaMAPro、Adapter、SCEdit、GaLore、Liger-Kernel等多种轻量微调方式
- 训练stage：涵盖了预训练、微调、人类对齐的全stage
- 训练并行：涵盖了单机单卡、单机多卡device_map、分布式数据并行（DDP）、多机多卡、DeepSpeed、FSDP、PAI DLC等，并支持Megatron架构的模型训练支持
  - 额外支持了[TorchAcc](https://github.imc.re/AlibabaPAI/torchacc)训练加速
  - 额外支持了基于[XTuner](https://github.com/InternLM/xtuner)的序列并行
- 推理部署：支持PyTorch、vLLM、LmDeploy等多推理框架的推理部署，可直接应用在docker镜像或k8s工程环境中
- 评测：支持以EvalScope为基本框架的纯文本和多模态评测能力，并支持自定义评测
- 导出：支持awq、gptq、bnb等量化方式，并支持lora、llamapro的merge操作
- 界面化：支持以gradio为基本框架的界面化操作，并支持仅部署单模型应用于space或demo环境中
- 插件化：支持对loss、metric、trainer、loss-scale、callback、optimizer等部分的插件化定义，用户对训练过程的定制更加轻松

# 安装

SWIFT的安装非常简易，请参考[安装文档](./SWIFT安装.md)。

# 一些重要概念

## model_type

在SWIFT3.0中，model_type和2.0有所不同。3.x的model_type含义为具有以下相同特性的模型集合：
1. 相同的模型结构 如典型的LLaMA结构
2. 相同的template，如都使用chatml格式的template
3. 相同的模型加载方式，例如都使用get_model_tokenizer_flash_attn加载

在以上三点都相同的情况下，模型被归类为一个组，这个组的类型就是model_type。

# 使用样例

全量使用样例在[examples](https://github.com/modelscope/ms-swift/tree/main/examples)中，下面给出一些基本样例：

命令行方式进行LoRA训练
```shell
CUDA_VISIBLE_DEVICES=0 \
swift sft \
    --model Qwen/Qwen2.5-7B-Instruct \
    --train_type lora \
    --dataset AI-ModelScope/alpaca-gpt4-data-zh#500 \
              AI-ModelScope/alpaca-gpt4-data-en#500 \
              swift/self-cognition#500 \
    --num_train_epochs 1 \
    --per_device_train_batch_size 1 \
    --learning_rate 1e-4 \
    --lora_rank 8 \
    --lora_alpha 32 \
    --target_modules all-linear \
    --gradient_accumulation_steps 16 \
    --eval_steps 50 \
    --save_steps 50 \
    --save_total_limit 2 \
    --logging_steps 5 \
    --max_length 2048 \
    --model_author swift \
    --output_dir output \
    --model_name swift-robot
```

代码方式进行训练的例子可以查看[examples/notebook](https://github.com/modelscope/ms-swift/tree/main/examples/notebook)

命令行方式进行推理和部署
```shell
# 推理
CUDA_VISIBLE_DEVICES=0 \
swift infer \
    --model Qwen/Qwen2.5-7B-Instruct \
    --infer_backend pt
```

```shell
# 部署
CUDA_VISIBLE_DEVICES=0 \
swift deploy \
    --model Qwen/Qwen2-7B-Instruct \
    --infer_backend pt
```

```python
# 部署client端代码
from openai import OpenAI

client = OpenAI(
    api_key='EMPTY',
    base_url='http://localhost:8000/v1',
)
model_type = client.models.list().data[0].id
print(f'model_type: {model_type}')

query = '浙江的省会在哪里?'
messages = [{'role': 'user', 'content': query}]
resp = client.chat.completions.create(model=model_type, messages=messages, seed=42)
response = resp.choices[0].message.content
print(f'query: {query}')
print(f'response: {response}')

# streaming
messages.append({'role': 'assistant', 'content': response})
query = '这有什么好吃的?'
messages.append({'role': 'user', 'content': query})
stream_resp = client.chat.completions.create(model=model_type, messages=messages, stream=True, seed=42)

print(f'query: {query}')
print('response: ', end='')
for chunk in stream_resp:
    print(chunk.choices[0].delta.content, end='', flush=True)
print()
```

# 评测
```shell
swift eval \
    --model Qwen/Qwen2.5-7B-Instruct \
    --eval_limit 10 \
    --eval_dataset gsm8k
```

# 量化
```shell
swift export \
    --model Qwen/Qwen2-7B-Instruct \
    --quant_method bnb \
    --quant_bits 8
```
