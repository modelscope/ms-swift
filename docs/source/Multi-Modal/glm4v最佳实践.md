
# GLM4V æœ€ä½³å®è·µ

## ç›®å½•
- [ç¯å¢ƒå‡†å¤‡](#ç¯å¢ƒå‡†å¤‡)
- [æ¨ç†](#æ¨ç†)
- [å¾®è°ƒ](#å¾®è°ƒ)
- [å¾®è°ƒåæ¨ç†](#å¾®è°ƒåæ¨ç†)


## ç¯å¢ƒå‡†å¤‡
```shell
# è¯·ä½¿ç”¨"ms-swift>=2.2"æˆ–è€…mainåˆ†æ”¯
git clone https://github.com/modelscope/swift.git
cd swift
pip install -e '.[llm]'
```

æ¨¡å‹é“¾æ¥:
- glm4v-9b-chat: [https://modelscope.cn/models/ZhipuAI/glm-4v-9b/summary](https://modelscope.cn/models/ZhipuAI/glm-4v-9b/summary)

## æ¨ç†

æ¨ç†glm4v-9b-chat:
```shell
# Experimental environment: A100
# 30GB GPU memory
CUDA_VISIBLE_DEVICES=0 swift infer --model_type glm4v-9b-chat
```

è¾“å‡º: (æ”¯æŒä¼ å…¥æœ¬åœ°è·¯å¾„æˆ–URL)
```python
"""
<<< ä½ å¥½
Input a media path or URL <<<
ä½ å¥½ğŸ‘‹ï¼å¾ˆé«˜å…´è§åˆ°ä½ ï¼Œæ¬¢è¿é—®æˆ‘ä»»ä½•é—®é¢˜ã€‚
--------------------------------------------------
<<< clear
<<< æè¿°è¿™å¼ å›¾ç‰‡
Input a media path or URL <<< http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/cat.png
è¿™æ˜¯ä¸€å¼ ç‰¹å†™ç…§ç‰‡ï¼Œå±•ç¤ºäº†ä¸€åªæ¯›èŒ¸èŒ¸çš„å°çŒ«ã€‚å°çŒ«çš„çœ¼ç›å¤§è€Œåœ†ï¼Œå‘ˆæ·±è“è‰²ï¼Œçœ¼ç å‘ˆé‡‘é»„è‰²ï¼Œéå¸¸æ˜äº®ã€‚å®ƒçš„é¼»å­çŸ­è€Œå°å·§ï¼Œæ˜¯ç²‰è‰²çš„ã€‚å°çŒ«çš„å˜´å·´ç´§é—­ï¼Œèƒ¡é¡»ç»†é•¿ã€‚å®ƒçš„è€³æœµç«–ç«‹ç€ï¼Œè€³æœµå†…ä¾§æ˜¯ç™½è‰²çš„ï¼Œå¤–ä¾§æ˜¯æ£•è‰²çš„ã€‚å°çŒ«çš„æ¯›å‘çœ‹èµ·æ¥æŸ”è½¯è€Œæµ“å¯†ï¼Œä¸»è¦æ˜¯ç™½è‰²å’Œæ£•è‰²ç›¸é—´çš„æ¡çº¹å›¾æ¡ˆã€‚èƒŒæ™¯æ¨¡ç³Šä¸æ¸…ï¼Œä½†ä¼¼ä¹æ˜¯ä¸€ä¸ªå®¤å†…ç¯å¢ƒã€‚
--------------------------------------------------
<<< clear
<<< å›¾ä¸­æœ‰å‡ åªç¾Š
Input a media path or URL <<< http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/animal.png
å›¾ä¸­å…±æœ‰å››åªç¾Šã€‚å…¶ä¸­æœ€å·¦è¾¹çš„ç¾Šèº«ä½“è¾ƒå°ï¼Œåè¾¹ä¸‰åªç¾Šä½“å‹é€æ¸å˜å¤§ï¼Œä¸”æœ€å³è¾¹çš„ä¸¤åªç¾Šä½“å‹å¤§å°ä¸€è‡´ã€‚
--------------------------------------------------
<<< clear
<<< è®¡ç®—ç»“æœæ˜¯å¤šå°‘?
Input a media path or URL <<< http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/math.png
1452+45304=46756
--------------------------------------------------
<<< clear
<<< æ ¹æ®å›¾ç‰‡ä¸­çš„å†…å®¹å†™é¦–è¯—
Input a media path or URL <<< http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/poem.png
æ¹–å…‰å±±è‰²æ˜ å°èˆ¹ï¼Œ

æ˜Ÿè¾‰ç‚¹ç‚¹ä¼´æ—…ç¨‹ã€‚

äººåœ¨ç”»ä¸­å¯»è¯—æ„ï¼Œ

å¿ƒéšæ™¯è¿å¿˜å¿§æ„ã€‚
--------------------------------------------------
<<< clear
<<< å¯¹å›¾ç‰‡è¿›è¡ŒOCR
Input a media path or URL <<< https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/ocr.png
å›¾ç‰‡ä¸­çš„OCRç»“æœå¦‚ä¸‹ï¼š

ç®€ä»‹

SWIFTæ”¯æŒ250+LLMå’Œ35+MLLMï¼ˆå¤šæ¨¡æ€å¤§æ¨¡å‹ï¼‰çš„è®­ç»ƒã€æ¨ç†ã€è¯„æµ‹å’Œéƒ¨ç½²ã€‚å¼€å‘è€…å¯ä»¥ç›´æ¥å°†æˆ‘ä»¬çš„æ¡†æ¶åº”ç”¨åˆ°è‡ªå·±çš„Researchå’Œç”Ÿäº§ç¯å¢ƒä¸­ï¼Œå®ç°æ¨¡å‹è®­ç»ƒè¯„æµ‹åˆ°åº”ç”¨çš„å®Œæ•´é“¾è·¯ã€‚æˆ‘ä»¬é™¤æ”¯æŒäº†PEFTæä¾›çš„è½»é‡è®­ç»ƒæ–¹æ¡ˆå¤–ï¼Œä¹Ÿæä¾›äº†ä¸€ä¸ªå®Œæ•´çš„Adaptersåº“ä»¥æ”¯æŒæœ€æ–°çš„è®­ç»ƒæŠ€æœ¯ï¼Œå¦‚NEFTuneã€LoRA+ã€LLaMA-PROç­‰ï¼Œè¿™ä¸ªé€‚é…å™¨åº“å¯ä»¥è„±ç¦»è®­ç»ƒè„šæœ¬ç›´æ¥ä½¿ç”¨åœ¨è‡ªå·±çš„è‡ªå®šæµç¨‹ä¸­ã€‚

ä¸ºæ–¹ä¾¿ä¸ç†Ÿæ‚‰æ·±åº¦å­¦ä¹ çš„ç”¨æˆ·ä½¿ç”¨ï¼Œæˆ‘ä»¬æä¾›äº†ä¸€ä¸ªGradioçš„web-uiç”¨äºæ§åˆ¶è®­ç»ƒå’Œæ¨ç†ï¼Œå¹¶æä¾›äº†é…å¥—çš„æ·±åº¦å­¦ä¹ è¯¾ç¨‹å’Œæœ€ä½³å®è·µä¾›æ–°å…¥é—¨ã€‚

æ­¤å¤–ï¼Œæˆ‘ä»¬ä¹Ÿåœ¨æ‹“å±•å…¶ä»–æ¨¡æ€çš„èƒ½åŠ›ï¼Œç›®å‰æˆ‘ä»¬æ”¯æŒäº†AnimateDiffçš„å…¨å‚æ•°è®­ç»ƒå’ŒLoRAè®­ç»ƒã€‚

SWIFTå…·æœ‰ä¸°å¯Œçš„æ–‡æ¡£ä½“ç³»ï¼Œå¦‚æœ‰ä½¿ç”¨é—®é¢˜è¯·è¯·æŸ¥çœ‹è¿™é‡Œã€‚

å¯ä»¥åœ¨Huggingface spaceå’ŒModelScopeåˆ›ç©ºé—´ä¸­ä½“éªŒSWIFT web-uiåŠŸèƒ½äº†ã€‚
"""
```

ç¤ºä¾‹å›¾ç‰‡å¦‚ä¸‹:

cat:

<img src="http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/cat.png" width="250" style="display: inline-block;">

animal:

<img src="http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/animal.png" width="250" style="display: inline-block;">

math:

<img src="http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/math.png" width="250" style="display: inline-block;">

poem:

<img src="http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/poem.png" width="250" style="display: inline-block;">

ocr:

<img src="https://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/ocr.png" width="250" style="display: inline-block;">

**å•æ ·æœ¬æ¨ç†**

```python
import os
os.environ['CUDA_VISIBLE_DEVICES'] = '0'

from swift.llm import (
    get_model_tokenizer, get_template, inference, ModelType,
    get_default_template_type, inference_stream
)
from swift.utils import seed_everything
import torch

model_type = ModelType.glm4v_9b_chat
template_type = get_default_template_type(model_type)
print(f'template_type: {template_type}')

model, tokenizer = get_model_tokenizer(model_type, torch.float16,
                                       model_kwargs={'device_map': 'auto'})
model.generation_config.max_new_tokens = 256
template = get_template(template_type, tokenizer)
seed_everything(42)

images = ['http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/road.png']
query = 'è·ç¦»å„åŸå¸‚å¤šè¿œï¼Ÿ'
response, history = inference(model, template, query, images=images)
print(f'query: {query}')
print(f'response: {response}')

# æµå¼
query = 'è·ç¦»æœ€è¿œçš„åŸå¸‚æ˜¯å“ªï¼Ÿ'
images = images
gen = inference_stream(model, template, query, history, images=images)
print_idx = 0
print(f'query: {query}\nresponse: ', end='')
for response, _ in gen:
    delta = response[print_idx:]
    print(delta, end='', flush=True)
    print_idx = len(response)
print()

"""
query: è·ç¦»å„åŸå¸‚å¤šè¿œï¼Ÿ
response: è·ç¦»é©¬è¸è¿˜æœ‰14Kmï¼Œè·ç¦»é˜³æ±Ÿè¿˜æœ‰62Kmï¼Œè·ç¦»å¹¿å·è¿˜æœ‰293Kmã€‚
query: è·ç¦»æœ€è¿œçš„åŸå¸‚æ˜¯å“ªï¼Ÿ
response: è·ç¦»æœ€è¿œçš„åŸå¸‚æ˜¯å¹¿å·ï¼Œæœ‰293Kmã€‚
"""
```

ç¤ºä¾‹å›¾ç‰‡å¦‚ä¸‹:

road:

<img src="http://modelscope-open.oss-cn-hangzhou.aliyuncs.com/images/road.png" width="250" style="display: inline-block;">


## å¾®è°ƒ
å¤šæ¨¡æ€å¤§æ¨¡å‹å¾®è°ƒé€šå¸¸ä½¿ç”¨**è‡ªå®šä¹‰æ•°æ®é›†**è¿›è¡Œå¾®è°ƒ. è¿™é‡Œå±•ç¤ºå¯ç›´æ¥è¿è¡Œçš„demo:

(é»˜è®¤å¯¹è¯­è¨€å’Œè§†è§‰æ¨¡å‹çš„qkvè¿›è¡Œloraå¾®è°ƒ. å¦‚æœä½ æƒ³å¯¹æ‰€æœ‰linearéƒ½è¿›è¡Œå¾®è°ƒ, å¯ä»¥æŒ‡å®š`--lora_target_modules ALL`)
```shell
# Experimental environment: A100
# 40GB GPU memory
CUDA_VISIBLE_DEVICES=0 swift sft \
    --model_type glm4v-9b-chat \
    --dataset coco-en-2-mini \
    --batch_size 2

# DDP
NPROC_PER_NODE=2 \
CUDA_VISIBLE_DEVICES=0,1 swift sft \
    --model_type glm4v-9b-chat \
    --dataset coco-en-2-mini \
    --ddp_find_unused_parameters true
```

[è‡ªå®šä¹‰æ•°æ®é›†](../LLM/è‡ªå®šä¹‰ä¸æ‹“å±•.md#-æ¨èå‘½ä»¤è¡Œå‚æ•°çš„å½¢å¼)æ”¯æŒjson, jsonlæ ·å¼, ä»¥ä¸‹æ˜¯è‡ªå®šä¹‰æ•°æ®é›†çš„ä¾‹å­:

(æ”¯æŒå¤šè½®å¯¹è¯, ä½†æ€»çš„è½®æ¬¡å¯¹è¯åªèƒ½åŒ…å«ä¸€å¼ å›¾ç‰‡, æ”¯æŒä¼ å…¥æœ¬åœ°è·¯å¾„æˆ–URL)

```jsonl
{"query": "55555", "response": "66666", "images": ["image_path"]}
{"query": "eeeee", "response": "fffff", "history": [], "images": ["image_path"]}
{"query": "EEEEE", "response": "FFFFF", "history": [["query1", "response1"], ["query2", "response2"]], "images": ["image_path"]}
```


## å¾®è°ƒåæ¨ç†
ç›´æ¥æ¨ç†:
```shell
CUDA_VISIBLE_DEVICES=0 swift infer \
    --ckpt_dir output/glm4v-9b-chat/vx-xxx/checkpoint-xxx \
    --load_dataset_config true
```

**merge-lora**å¹¶æ¨ç†:
```shell
CUDA_VISIBLE_DEVICES=0 swift export \
    --ckpt_dir output/glm4v-9b-chat/vx-xxx/checkpoint-xxx \
    --merge_lora true

CUDA_VISIBLE_DEVICES=0 swift infer \
    --ckpt_dir output/glm4v-9b-chat/vx-xxx/checkpoint-xxx-merged \
    --load_dataset_config true
```
