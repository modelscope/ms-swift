# è‡ªå®šä¹‰ä¸æ‹“å±•
## ç›®å½•
- [è‡ªå®šä¹‰æ•°æ®é›†](#è‡ªå®šä¹‰æ•°æ®é›†)
- [è‡ªå®šä¹‰æ¨¡å‹](#è‡ªå®šä¹‰æ¨¡å‹)
- [è‡ªå®šä¹‰å¯¹è¯æ¨¡æ¿](#è‡ªå®šä¹‰å¯¹è¯æ¨¡æ¿)

## è‡ªå®šä¹‰æ•°æ®é›†
æˆ‘ä»¬æ”¯æŒä¸¤ç§**è‡ªå®šä¹‰æ•°æ®é›†**çš„æ–¹æ³•.

1. ã€æ¨èã€‘**å‘½ä»¤è¡Œå‚æ•°**çš„å½¢å¼: **æ›´åŠ æ–¹ä¾¿æ”¯æŒæœ¬åœ°è‡ªå®šä¹‰æ•°æ®é›†**.
2. **æ³¨å†Œæ•°æ®é›†**çš„æ–¹å¼: æ›´åŠ çµæ´», å¯ä»¥å¯¹swift**è¿›ä¸€æ­¥æ‹“å±•å’Œå¼€å‘**, ä½†éœ€è¦ä¸€å®šçš„ç¼–ç¨‹é—¨æ§›. æ–¹æ³•ä¸€åœ¨å®ç°ä¸Šå€ŸåŠ©äº†æ–¹æ³•äºŒ.

### ğŸ“Œ ã€æ¨èã€‘å‘½ä»¤è¡Œå‚æ•°çš„å½¢å¼
ä½ éœ€è¦åœ¨sft.shè„šæœ¬ä¸­é¢å¤–æŒ‡å®š:

```bash
--custom_train_dataset_path xxx.jsonl \
--custom_val_dataset_path yyy.jsonl \
```

å¯¹åº”çš„shæ¡ˆä¾‹è„šæœ¬å¯ä»¥æŸ¥çœ‹[è¿™é‡Œ](https://github.com/modelscope/swift/blob/main/examples/pytorch/llm/scripts/tongyi_finance_14b_chat_int4/qlora/sft.sh).

1. `--custom_train_dataset_path`: é»˜è®¤å€¼ä¸º`[]`, è¡¨ç¤ºä¸ä½¿ç”¨è‡ªå®šä¹‰æ•°æ®é›†. ä½ å¯ä»¥åƒå¦‚ä¸‹å½¢å¼è¿›è¡ŒæŒ‡å®š: `--custom_train_dataset_path alpaca.csv`æˆ–è€…æŒ‡å®šå¤šä¸ªè®­ç»ƒæ•°æ®é›†`--custom_train_dataset_path alpaca.csv chatml.jsonl swift.jsonl`, è„šæœ¬ä¼šè¿›è¡Œè‡ªåŠ¨çš„é¢„å¤„ç†å’Œæ‹¼æ¥.

   > å¯ä»¥é€šè¿‡å…¬å¼€æ•°æ®é›†å’Œè‡ªå®šä¹‰æ•°æ®é›†ç»“åˆçš„æ–¹å¼è¿›è¡Œè®­ç»ƒ: `--dataset blossom-math-zh --custom_train_dataset_path custom_math.jsonl`.

2. `--custom_val_dataset_path`: é»˜è®¤å€¼ä¸º`[]`, è¡¨ç¤ºä¸ä½¿ç”¨è‡ªå®šä¹‰éªŒè¯æ•°æ®é›†. å¦‚æœä½ æŒ‡å®šäº†`custom_train_dataset_path`, åˆ™è‡ªå®šä¹‰æ•°æ®é›†çš„éªŒè¯é›†å°†æŒ‰ç…§å‘½ä»¤è¡Œå‚æ•°`dataset_test_ratio`è¿›è¡Œåˆ‡å‰².

è„šæœ¬æ”¯æŒçš„æ–‡ä»¶æ ¼å¼åŒ…å«`csv`, `jsonl`, `json`æ ¼å¼. ä½ éœ€è¦å°†ä¼ å…¥çš„æ–‡ä»¶ç¬¦åˆä»¥ä¸‹æ•°æ®é›†æ ¼å¼. csvæ ¼å¼çš„æ–‡ä»¶åªæ”¯æŒæŒ‡ä»¤å¾®è°ƒ, å³æ²¡æœ‰historyçš„æƒ…å†µ. jsonlæ ¼å¼çš„æ–‡ä»¶æ”¯æŒsystem, history.

**æ ¼å¼1:**

Pretraining

```csv
response
11111
aaaaa
AAAAA
```

```jsonl
{"response": "11111"}
{"response": "aaaaa"}
{"response": "AAAAA"}
```

Single-Round Dialogue

```csv
query,response
11111,22222
aaaaa,bbbbb
AAAAA,BBBBB
```

```jsonl
{"query": "11111", "response": "22222"}
{"query": "aaaaa", "response": "bbbbb"}
{"query": "AAAAA", "response": "BBBBB"}
```

Multi-Round Dialogue

```jsonl
{"query": "55555", "response": "66666"}
{"query": "eeeee", "response": "fffff", "history": []}
{"query": "EEEEE", "response": "FFFFF", "history": [["AAAAA", "BBBBB"], ["CCCCC", "DDDDD"]]}
```

```json
[{"query": "55555", "response": "66666"},
{"query": "eeeee", "response": "fffff", "history": []},
{"query": "EEEEE", "response": "FFFFF", "history": [["AAAAA", "BBBBB"], ["CCCCC", "DDDDD"]]}]
```

**æ ¼å¼2:**

```csv
instruction,input,output
11111,22222,33333
aaaaa,bbbbb,ccccc
AAAAA,BBBBB,CCCCC
```

**æ ¼å¼3:**

```jsonl
{"conversations": [{"from": "user", "value": "11111"}, {"from": "assistant", "value": "22222"}]}
{"conversations": [{"from": "user", "value": "aaaaa"}, {"from": "assistant", "value": "bbbbb"}, {"from": "user", "value": "ccccc"}, {"from": "assistant", "value": "ddddd"}]}
{"conversations": [{"from": "user", "value": "AAAAA"}, {"from": "assistant", "value": "BBBBB"}, {"from": "user", "value": "CCCCC"}, {"from": "assistant", "value": "DDDDD"}]}
```

**æ ¼å¼4:**

```jsonl
{"messages": [{"role": "user", "content": "11111"}, {"role": "assistant", "content": "22222"}]}
{"messages": [{"role": "user", "content": "aaaaa"}, {"role": "assistant", "content": "bbbbb"}, {"role": "user", "content": "ccccc"}, {"role": "assistant", "content": "ddddd"}]}
{"messages": [{"role": "user", "content": "AAAAA"}, {"role": "assistant", "content": "BBBBB"}, {"role": "user", "content": "CCCCC"}, {"role": "assistant", "content": "DDDDD"}]}
```

**å¼ºåŒ–å­¦ä¹ ï¼ˆDPOï¼‰**

```jsonl
{"query": "11111", "response": "22222", "rejected_response": "33333"}
{"query": "aaaaa", "response": "bbbbb", "rejected_response": "ccccc"}
{"query": "AAAAA", "response": "BBBBB", "rejected_response": "CCCCC"}
```

### æ³¨å†Œæ•°æ®é›†çš„æ–¹å¼

ä»¥ä¸‹æ˜¯ä¸€ä¸ª**æ³¨å†Œæ•°æ®é›†**çš„æ¡ˆä¾‹. å®Œæ•´çš„pyæ–‡ä»¶å¯ä»¥æŸ¥çœ‹[custom.py](https://github.com/modelscope/swift/blob/main/examples/pytorch/llm/custom.py), shè„šæœ¬å¯ä»¥æŸ¥çœ‹[custom](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/custom).

```python
from typing import Optional, Tuple

from datasets import Dataset as HfDataset
from modelscope import MsDataset

from swift.llm import get_dataset, register_dataset
from swift.utils import get_logger

logger = get_logger()


class CustomDatasetName:
    stsb_en = 'stsb-en'

def _preprocess_stsb(dataset: HfDataset) -> HfDataset:
    prompt = """Task: Based on the given two sentences, provide a similarity score between 0.0 and 5.0.
Sentence 1: {text1}
Sentence 2: {text2}
Similarity score: """
    query = []
    response = []
    for d in dataset:
        query.append(prompt.format(text1=d['text1'], text2=d['text2']))
        response.append(f"{d['label']:.1f}")
    return HfDataset.from_dict({'query': query, 'response': response})


@register_dataset(
    CustomDatasetName.stsb_en, 'huangjintao/stsb', task='text-generation')
def get_stsb_dataset(dataset_id_or_path: str,
                     **kwargs) -> Tuple[HfDataset, Optional[HfDataset]]:
    dataset_dict = MsDataset.load(dataset_id_or_path)
    train_dataset = dataset_dict['train'].to_hf_dataset()
    val_dataset = dataset_dict['validation'].to_hf_dataset()
    return tuple(
        _preprocess_stsb(dataset) for dataset in [train_dataset, val_dataset])


if __name__ == '__main__':
    # test dataset
    train_dataset, val_dataset = get_dataset([CustomDatasetName.stsb_en],
                                             check_dataset_strategy='warning')
    print(f'train_dataset: {train_dataset}')
    print(f'val_dataset: {val_dataset}')

```

`register_dataset`ä¼šåœ¨`DATASET_MAPPING`ä¸­æ³¨å†Œæ•°æ®é›†, è¯¥å‡½æ•°çš„å‚æ•°å«ä¹‰å¦‚ä¸‹:

- `dataset_name`: å¿…å¡«é¡¹, è¡¨ç¤ºæ•°æ®é›†çš„åå­—, ä¹Ÿæ˜¯æ•°æ®é›†çš„å”¯ä¸€id.

- `dataset_id_or_path`: å¿…å¡«é¡¹. è¡¨ç¤ºæ•°æ®é›†åœ¨ModelScope Hubä¸Šçš„`dataset_id`æˆ–è€…æœ¬åœ°çš„`dataset_dir`.

- `get_function`: é»˜è®¤å€¼ä¸º`None`. è·å–æ•°æ®é›†çš„å‡½æ•°. å¦‚æœä¼ å…¥None, åˆ™ä½¿ç”¨ä¿®é¥°å™¨æ–¹æ¡ˆè¿›è¡Œæ•°æ®é›†æ³¨å†Œ. å¦‚æœä¼ å…¥ä¸€ä¸ªå‡½æ•°, åˆ™ä½¿ç”¨æ­£å¸¸æ–¹æ¡ˆè¿›è¡Œæ³¨å†Œ.
  > `get_function`éœ€è¦è¿”å›`HfDataset`æˆ–`Tuple[HfDataset, Optional[HfDataset]]`. å¦‚æœåªè¿”å›ä¸€ä¸ªæ•°æ®é›†, åˆ™è¯¥æ•°æ®é›†ä¸ºtrain\_dataset, æ•°æ®é›†å¤„ç†å‡½æ•°ä¼šåˆ‡åˆ†ä¸€éƒ¨åˆ†çš„æ•°æ®é›†ä½œä¸ºval\_dataset (æ ¹æ®å‘½ä»¤è¡Œå‚æ•°`dataset_test_ratio`); å¦‚æœè¿”å›ä¸¤ä¸ªæ•°æ®é›†, åˆ™åˆ†åˆ«ä½œä¸ºtrain\_datasetå’Œval\_dataset. æˆ‘ä»¬æ”¯æŒä½¿ç”¨å¤šä¸ªæ•°æ®é›†è¿›è¡Œå¾®è°ƒ`get_dataset(['dataset1', 'dataset2'])`. æˆ‘ä»¬ä¼šå°†å„ä¸ªå­æ•°æ®é›†çš„è®­ç»ƒé›†å’ŒéªŒè¯é›†éƒ¨åˆ†åˆ†åˆ«è¿›è¡Œæ‹¼æ¥, æœ€ç»ˆè¿”å›åˆå¹¶åçš„è®­ç»ƒé›†å’ŒéªŒè¯é›†.

  > å‡½æ•°è¿”å›çš„`HfDataset`éœ€è¦ç¬¦åˆä¸€å®šçš„è§„èŒƒ. å¦‚æœä½ è¦è¿›è¡Œ**é¢„è®­ç»ƒ**, é‚£ä¹ˆåªéœ€è¦åŒ…å«`response`å­—æ®µ, å…·ä½“å¯ä»¥å‚è€ƒ`'tigerbot-law-zh'`æ•°æ®é›†. å¦‚æœæ˜¯**æŒ‡ä»¤å¾®è°ƒ(å•è½®å¯¹è¯)**çš„æƒ…å†µä¸‹, éœ€åŒ…å«`query`, `response`å­—æ®µ, åˆ†åˆ«ä»£è¡¨æŒ‡ä»¤å¾®è°ƒçš„ç”¨æˆ·è¯¢é—®å’ŒAIåŠ©æ‰‹çš„å›ç­”, å…·ä½“å¯ä»¥å‚è€ƒ`'alpaca-zh'`æ•°æ®é›†. å¦‚æœæ˜¯**å¤šè½®å¯¹è¯**, åˆ™éœ€è¦é¢å¤–åŠ ä¸Š`history`å­—æ®µ, ä»£è¡¨å¯¹è¯çš„å†å²ä¿¡æ¯, å…·ä½“å¯ä»¥å‚è€ƒ`'damo-agent-mini-zh'`æ•°æ®é›†. å¦‚æœæ¯ä¸ªæ•°æ®é›†æ ·ä¾‹å…·æœ‰ä¸åŒçš„`system`, åˆ™éœ€è¦é¢å¤–åŠ ä¸Šsystemå­—æ®µ, å…·ä½“ä½ ä¹Ÿå¯ä»¥å‚è€ƒ`'damo-agent-mini-zh'`æ•°æ®é›†.

- `**kwargs`: å…¶ä»–ç”¨äºæ³¨é‡Šæ•°æ®é›†çš„å‚æ•°. è¯¥å‚æ•°ä¸€èˆ¬ä¸éœ€è¦è®¾ç½®.


## è‡ªå®šä¹‰æ¨¡å‹
ä»¥ä¸‹æ˜¯ä¸€ä¸ª**è‡ªå®šä¹‰æ¨¡å‹**çš„æ¡ˆä¾‹. å®Œæ•´çš„pyæ–‡ä»¶å¯ä»¥æŸ¥çœ‹[custom.py](https://github.com/modelscope/swift/blob/main/examples/pytorch/llm/custom.py), shè„šæœ¬å¯ä»¥æŸ¥çœ‹[custom](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/custom).

```python
from typing import Any, Dict

from modelscope import AutoConfig, AutoModelForCausalLM, AutoTokenizer

from torch import dtype as Dtype
from transformers.utils.versions import require_version

from swift.llm import LoRATM, TemplateType, get_model_tokenizer, register_model
from swift.utils import get_logger

logger = get_logger()


class CustomModelType:
    tigerbot_7b = 'tigerbot-7b'
    tigerbot_13b = 'tigerbot-13b'
    tigerbot_13b_chat = 'tigerbot-13b-chat'


class CustomTemplateType:
    tigerbot = 'tigerbot'


@register_model(CustomModelType.tigerbot_7b,
                'TigerResearch/tigerbot-7b-base-v3', LoRATM.llama2,
                TemplateType.default_generation)
@register_model(CustomModelType.tigerbot_13b,
                'TigerResearch/tigerbot-13b-base-v2', LoRATM.llama2,
                TemplateType.default_generation)
@register_model(CustomModelType.tigerbot_13b_chat,
                'TigerResearch/tigerbot-13b-chat-v4', LoRATM.llama2,
                CustomTemplateType.tigerbot)
def get_tigerbot_model_tokenizer(model_dir: str,
                                 torch_dtype: Dtype,
                                 model_kwargs: Dict[str, Any],
                                 load_model: bool = True,
                                 **kwargs):
    use_flash_attn = kwargs.pop('use_flash_attn', False)
    if use_flash_attn:
        require_version('transformers>=4.34')
        logger.info('Setting use_flash_attention_2: True')
        model_kwargs['use_flash_attention_2'] = True
    model_config = AutoConfig.from_pretrained(
        model_dir, trust_remote_code=True)
    model_config.pretraining_tp = 1
    model_config.torch_dtype = torch_dtype
    logger.info(f'model_config: {model_config}')
    tokenizer = AutoTokenizer.from_pretrained(
        model_dir, trust_remote_code=True)
    model = None
    if load_model:
        model = AutoModelForCausalLM.from_pretrained(
            model_dir,
            config=model_config,
            torch_dtype=torch_dtype,
            trust_remote_code=True,
            **model_kwargs)
    return model, tokenizer


if __name__ == '__main__':
    # test model base
    model, tokenizer = get_model_tokenizer(
        CustomModelType.tigerbot_7b, use_flash_attn=False)
    print(model.__class__.__name__)
    # test model chat
    model, tokenizer = get_model_tokenizer(
        CustomModelType.tigerbot_13b_chat, use_flash_attn=False)
    print(model.__class__.__name__)
```

`register_model`ä¼šåœ¨`MODEL_MAPPING`ä¸­æ³¨å†Œæ¨¡å‹, è¯¥å‡½æ•°çš„å‚æ•°å«ä¹‰å¦‚ä¸‹:

- `model_type`: å¿…å¡«é¡¹. è¡¨ç¤ºæ¨¡å‹çš„åå­—, ä¹Ÿæ˜¯å”¯ä¸€çš„id.
- `model_id_or_path`: å¿…å¡«é¡¹. è¡¨ç¤ºæ¨¡å‹åœ¨ModelScope Hubä¸­çš„`model_id`, æˆ–è€…æ˜¯æœ¬åœ°çš„æ¨¡å‹ç›®å½•`model_dir`.
- `lora_target_modules`: é»˜è®¤ä¸º`None`. è¡¨ç¤ºåœ¨shè„šæœ¬ä¸­æŒ‡å®š`--lora_target_modules DEFAULT`æˆ–`--lora_target_modules AUTO`æˆ–æœªæŒ‡å®š`--lora_target_modules`æƒ…å†µä¸‹é»˜è®¤ä½¿ç”¨çš„lora_target_modules.
- `template`: é»˜è®¤ä¸º`TemplateType.default`. è¡¨ç¤ºåœ¨shè„šæœ¬ä¸­æŒ‡å®š`--template_type AUTO`æˆ–æœªæŒ‡å®š`--template_type`æƒ…å†µä¸‹é»˜è®¤ä½¿ç”¨çš„å¯¹è¯æ¨¡æ¿.
- `get_function`: é»˜è®¤å€¼ä¸º`None`. è·å–modelå’Œtokenizerçš„å‡½æ•°. å¦‚æœä¼ å…¥None, åˆ™ä½¿ç”¨ä¿®é¥°å™¨æ–¹æ¡ˆè¿›è¡Œæ¨¡å‹æ³¨å†Œ. å¦‚æœä¼ å…¥ä¸€ä¸ªå‡½æ•°, åˆ™ä½¿ç”¨æ­£å¸¸æ–¹æ¡ˆè¿›è¡Œæ³¨å†Œ.
- `requires`: é»˜è®¤ä¸º`[]`. è¡¨ç¤ºæ¨¡å‹æ‰€éœ€è¦çš„åŒºåˆ«äºå…¶ä»–æ¨¡å‹çš„ä¾èµ–. è¯¥å‚æ•°ä¸€èˆ¬ä¸éœ€è¦è®¾ç½®.
- `torch_dtype`: é»˜è®¤ä¸º`None`. è¡¨ç¤ºæ¨¡å‹æ‰€æ¨èä½¿ç”¨çš„torch_dtype. è¯¥å‚æ•°ä¸€èˆ¬ä¸éœ€è¦è®¾ç½®.
- `revision`: é»˜è®¤ä¸º`'master'`. ç”¨äºæŒ‡å®šæ¨¡å‹çš„ç‰ˆæœ¬å·. å¦‚æœ`model_id_or_path`æ˜¯æœ¬åœ°çš„æ¨¡å‹ç›®å½•, åˆ™è¯¥å‚æ•°å¤±æ•ˆ. è¯¥å‚æ•°ä¸€èˆ¬ä¸éœ€è¦è®¾ç½®.
- `ignore_file_pattern`: é»˜è®¤ä¸º`None`. è¡¨ç¤ºä¸‹è½½çš„æ—¶å€™éœ€è¦å¿½ç•¥çš„æ–‡ä»¶åçš„æ­£åˆ™pattern, è¯¥å‚æ•°ä¼šä¼ é€’ç»™`snapshot_download`. ä¾‹å¦‚`r'.+\.bin$'`, `r'.+\.savetensors$'`ç­‰. è¯¥å‚æ•°ä¸€èˆ¬ä¸éœ€è¦è®¾ç½®.
- `**kwargs`: å…¶ä»–ç”¨äºæ³¨é‡Šæ¨¡å‹èƒ½åŠ›çš„å‚æ•°. è¯¥å‚æ•°ä¸€èˆ¬ä¸éœ€è¦è®¾ç½®.


## è‡ªå®šä¹‰å¯¹è¯æ¨¡æ¿
ä»¥ä¸‹æ˜¯ä¸€ä¸ª**è‡ªå®šä¹‰æ¨¡å‹**çš„æ¡ˆä¾‹. å®Œæ•´çš„pyæ–‡ä»¶å¯ä»¥æŸ¥çœ‹[custom.py](https://github.com/modelscope/swift/blob/main/examples/pytorch/llm/custom.py), shè„šæœ¬å¯ä»¥æŸ¥çœ‹[custom](https://github.com/modelscope/swift/tree/main/examples/pytorch/llm/scripts/custom).

```python
from swift.llm import (Template, ModelType, dataset_map,
                       get_model_tokenizer, get_template, get_dataset,
                       print_example, register_template, DatasetName)
from swift.utils import get_logger

logger = get_logger()


class CustomTemplateType:
    tigerbot = 'tigerbot'


# Ref: https://github.com/TigerResearch/TigerBot/blob/main/infer.py
register_template(
    CustomTemplateType.tigerbot,
    Template(['{{SYSTEM}}'], ['\n\n### Instruction:\n{{QUERY}}\n\n### Response:\n'], [],
             [['eos_token_id']], ''))

if __name__ == '__main__':
    # test template
    train_dataset, _ = get_dataset(DatasetName.blossom_math_zh)
    _, tokenizer = get_model_tokenizer(ModelType.qwen_7b_chat, load_model=False)
    template = get_template(CustomTemplateType.tigerbot, tokenizer)
    train_dataset = dataset_map(train_dataset, template.encode)
    print_example(train_dataset[0], tokenizer)
```

`register_template`ä¼šåœ¨`TEMPLATE_MAPPING`ä¸­æ³¨å†Œå¯¹è¯æ¨¡æ¿, è¯¥å‡½æ•°çš„å‚æ•°å«ä¹‰å¦‚ä¸‹:

- `template_type`: å¿…å¡«é¡¹, è¡¨ç¤ºå¯¹è¯æ¨¡æ¿çš„åå­—, ä¹Ÿæ˜¯templateçš„å”¯ä¸€id.
- `template`: å¿…å¡«é¡¹, éœ€è¦ä¼ å…¥ä¸€ä¸ª`Template`. åˆå§‹åŒ–`Template`éœ€è¦ä¼ å…¥ä»¥ä¸‹å‚æ•°: `prefix`, `prompt`, `chat_sep`, `suffix`, `default_system`.

æ¨¡æ¿åˆå§‹åŒ–å‡½æ•°ä¼šæ ¹æ®è¿™å››ä¸ªå†…å®¹, è·å–å®Œæ•´çš„chat template. å…¶ä¸­è¿™å››ä¸ªé…ç½®å†…å®¹çš„å«ä¹‰å¦‚ä¸‹.

- `prefix`: è¡¨ç¤ºå¯¹è¯æ¨¡æ¿ä¸­çš„å‰ç¼€éƒ¨åˆ†, ä¸€èˆ¬ä¸ºsysteméƒ¨åˆ†, å‰ç¼€token, bos tokenç­‰å†…å®¹. æˆ‘ä»¬ä½¿ç”¨`{{SYSTEM}}`ä½œä¸ºsystemçš„å ä½ç¬¦. å¦‚æœ`{{SYSTEM}}`æ²¡æœ‰åœ¨prefixä¸­å­˜åœ¨, åˆ™è¯¥Templateä¸æ”¯æŒsystem, e.g. `damo-agent-mini-zh`æ•°æ®é›†.
- `prompt`: è¡¨ç¤ºå¯¹è¯æ¨¡æ¿ä¸­çš„ä¸€è½®å¯¹è¯. æˆ‘ä»¬ä½¿ç”¨`{{QUERY}}`ä½œä¸ºæ¯è½®å¯¹è¯ä¸­, humanè¯¢é—®éƒ¨åˆ†çš„å ä½ç¬¦, `{{ROUND0}}`åˆ™è¡¨ç¤ºæœ¬æ¬¡å¯¹è¯æ˜¯ç¬¬å‡ è½®çš„å ä½ç¬¦, ä»0å¼€å§‹è®¡æ•°, `{{ROUND1}}`ä»1å¼€å§‹è®¡æ•°. AIåŠ©æ‰‹çš„å›å¤éƒ¨åˆ†ä¼šæ‹¼æ¥åœ¨`prompt`çš„åé¢, å› æ­¤æˆ‘ä»¬æ²¡æœ‰è®¾è®¡å…¶å ä½ç¬¦. æˆ‘ä»¬åªä¼šå¯¹AIåŠ©æ‰‹çš„å›å¤éƒ¨åˆ†è®¡ç®—æŸå¤±.
- `chat_sep`: å¦‚æœéœ€è¦è¿›è¡Œå¤šè½®å¯¹è¯, `chat_sep`ä¼šä½œä¸ºæ¯è½®å¯¹è¯ä¹‹é—´çš„åˆ†éš”ç¬¦, ä¾‹å¦‚: æ¢è¡Œç­‰. å¦‚æœè®¾ç½®ä¸ºNone, åˆ™è¯¥Templateä¸æ”¯æŒå¤šè½®å¯¹è¯.
- `suffix`: ä½œä¸ºå¯¹è¯æ¨¡æ¿çš„åç¼€éƒ¨åˆ†, ä¸€èˆ¬ä¸ºeos token. ä¼šæ‹¼æ¥åœ¨æœ€åä¸€è½®çš„å¯¹è¯åé¢.
- `default_system`: é»˜è®¤çš„system.
